@article{Application,
    author = {Abbod, M.F. and Catto, J.W. and Linkens, D.A. and Hamdy, F.C.},
    title = {Application of Artificial Intelligence to the Management of Urological Cancer},
    journal = {The Journal of Urology},
    volume = {178},
    number = {4},
    pages = {1150–1156},
    year = {2007}
}

@inproceedings{Wang2017ANS,
  title={A new scheme for training ReLU-based multi-layer feedforward neural networks},
  author={Hao Wang},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:208104834}
}


@Techreport{Rosenblatt_1957_6098,
  author = {Rosenblatt, F.},
 address = {Ithaca, New York},
 institution = {Cornell Aeronautical Laboratory},
 month = {January},
 number = {85-460-1},
 title = {The perceptron - A perceiving and recognizing automaton},
 year = {1957},
 title_with_no_special_chars = {The Perceptron  A perceiving and recognizing automaton}
}

@article{BiswalBH21,
  title = {Chapter Eleven - Music recommender system using restricted Boltzmann machine with implicit feedback},
  author = {Amitabh Biswal and Malaya Dutta Borah and Zakir Hussain},
  year = {2021},
  doi = {10.1016/bs.adcom.2021.01.001},
  url = {https://doi.org/10.1016/bs.adcom.2021.01.001},
  researchr = {https://researchr.org/publication/BiswalBH21},
  cites = {0},
  citedby = {0},
  journal = {Advances in Computers},
  volume = {122},
  pages = {367-402},
}
@online{backpropagation,
  author = {Cameron Hashemi-Pour},
  title = {Definition: Backpropagation Algorithm},
  year = {2023},
  url = {https://www.techtarget.com/searchenterpriseai/definition/backpropagation-algorithm}
}

@online{LossFunctionsn,
  author = {Artem Oppermann},
  title = {How Loss Functions Work in Neural Networks and Deep Learning},
  year = {2022},
  url = {https://builtin.com/machine-learning/loss-functions}
}

@misc{ruder2017overview,
      title={An overview of gradient descent optimization algorithms}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1609.04747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{rumelhart1986learning,
  added-at = {2023-03-05T10:30:55.000+0100},
  author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  biburl = {https://www.bibsonomy.org/bibtex/2197dd0ba9ef6a582f0219953dc315f39/jascal_panetzky},
  interhash = {c354bc293fa9aa7caffc66d40a014903},
  intrahash = {197dd0ba9ef6a582f0219953dc315f39},
  journal = {nature},
  keywords = {imported},
  number = 6088,
  pages = {533--536},
  publisher = {Nature Publishing Group UK London},
  timestamp = {2023-03-05T10:34:04.000+0100},
  title = {Learning representations by back-propagating errors},
  volume = 323,
  year = 1986
}

@article{10.1007/s00779-021-01587-4,
author = {Vidyabharathi, D and Mohanraj, V and Kumar, J Senthil and Suresh, Y},
title = {Achieving generalization of deep learning models in a quick way by adapting T-HTR learning rate scheduler},
year = {2021},
issue_date = {Jun 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {3},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-021-01587-4},
doi = {10.1007/s00779-021-01587-4},
journal = {Personal Ubiquitous Comput.},
month = {aug},
pages = {1335–1353},
numpages = {19},
keywords = {BiLSTM (bidirectional long short-term memory), LSTM (long short-term memory), Step width, T-HTR (toggle-hyperbolic tangent decay and triangular with restarts)}
}

@online{learningRate,
  author = {JEREMY JORDAN},
  title = {Setting the learning rate of your neural network.},
  year = {2018},
  url = {https://www.jeremyjordan.me/nn-learning-rate/}
}

@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}

@article{AMARI1993185,
title = {Backpropagation and stochastic gradient descent method},
journal = {Neurocomputing},
volume = {5},
number = {4},
pages = {185-196},
year = {1993},
issn = {0925-2312},
doi = {https://doi.org/10.1016/0925-2312(93)90006-O},
url = {https://www.sciencedirect.com/science/article/pii/092523129390006O},
author = {Shun-ichi Amari},
keywords = {Stochastic descent, generalized delta rule, dynamics of learning, pattern classification, multilayer perceptron},
abstract = {The backpropagation learning method has opened a way to wide applications of neural network research. It is a type of the stochastic descent method known in the sixties. The present paper reviews the wide applicability of the stochastic gradient descent method to various types of models and loss functions. In particular, we apply it to the pattern recognition problem, obtaining a new learning algorithm based on the information criterion. Dynamical properties of learning curves are then studied based on an old paper by the author where the stochastic descent method was proposed for general multilayer networks. The paper is concluded with a short section offering some historical remarks.}
}

@article{10.5555/3455716.3455935,
author = {Ward, Rachel and Wu, Xiaoxia and Bottou, L\'{e}on},
title = {AdaGrad stepsizes: sharp convergence over nonconvex landscapes},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Adaptive gradient methods such as AdaGrad and its variants update the stepsize in stochastic gradient descent on the fly according to the gradients received along the way; such methods have gained widespread use in large-scale optimization for their ability to converge robustly, without the need to fine-tune the stepsize schedule. Yet, the theoretical guarantees to date for AdaGrad are for online and convex optimization. We bridge this gap by providing theoretical guarantees for the convergence of AdaGrad for smooth, nonconvex functions. We show that the norm version of AdaGrad (AdaGrad-Norm) converges to a stationary point at the O(log(N)/√N) rate in the stochastic setting, and at the optimal O(1/N) rate in the batch (non-stochastic) setting - in this sense, our convergence guarantees are "sharp". In particular, the convergence of AdaGrad-Norm is robust to the choice of all hyperparameters of the algorithm, in contrast to stochastic gradient descent whose convergence depends crucially on tuning the step-size to the (generally unknown) Lipschitz smoothness constant and level of stochastic noise on the gradient. Extensive numerical experiments are provided to corroborate our theoretical findings; moreover, the experiments suggest that the robustness of AdaGrad-Norm extends to the models in deep learning.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {219},
numpages = {30},
keywords = {convergence, adaptive gradient descent, large-scale optimization, stochastic offline learning, nonconvex optimization}
}

@inproceedings{NEURIPS2019_1e8a1942,
 author = {Zhang, Biao and Sennrich, Rico},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Root Mean Square Layer Normalization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf},
 volume = {32},
 year = {2019}
}
@article{Lehr_2021,
doi = {10.1088/1757-899X/1140/1/012049},
url = {https://dx.doi.org/10.1088/1757-899X/1140/1/012049},
year = {2021},
month = {may},
publisher = {IOP Publishing},
volume = {1140},
number = {1},
pages = {012049},
author = {J Lehr and J Philipps and V Nguyen Hoang and D von Wrangel and J Krüger},
title = {Supervised learning vs. unsupervised learning: A comparison for optical inspection applications in quality control},
journal = {IOP Conference Series: Materials Science and Engineering},
abstract = {For the establishment of a successful quality management system in companies, the quality control of e.g. newly produced goods or the return of old and used parts is an essential component. One solution for this is the optical inspection of the surface of objects with the help of image processing algorithms. Using the case study of printer cartridges, this paper evaluates the extent to which different methods of machine learning can contribute to a successful quality control. Established methods of supervised learning have the advantage that they are already proven in many applications and have a very high detection accuracy. However, they require a lot of labelled training data and this high effort also means high integration costs. A new approach is a data-reduced variant from unsupervised learning. Here, the algorithm is trained only with defect free objects, for example as they come to a large extent from the production. If the objects are defective, the method from the field of anomaly detection or even novelty detection detects something that is different from the learned norm. This has the advantage that not all defects have to be known beforehand. And this in turn avoids acquiring a large amount of training data for each of these defects. This paper compares the effort required to acquire training data and compares it with the detection accuracy of the different methods in order to give an assessment of the extent to which the use of unsupervised learning methods is beneficial. Newly produced and used printer cartridges are used for this purpose. Image data is acquired from 18 different printer cartridge models. Afterwards they are fully annotated (labelled). A smart separation into training, validation and test data allows the training of supervised and unsupervised methods as well as a complete evaluation regarding the effort for data acquisition, annotation and detection accuracy of the defects. Finally, an outlook for chances and risks of the respective procedures is given.}
}


@inbook{MarkovDecisionProcesses,
author = {Garcia, Frédérick and Rachelson, Emmanuel},
publisher = {John Wiley & Sons, Ltd},
isbn = {9781118557426},
title = {Markov Decision Processes},
booktitle = {Markov Decision Processes in Artificial Intelligence},
chapter = {1},
pages = {1-38},
doi = {https://doi.org/10.1002/9781118557426.ch1},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118557426.ch1},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118557426.ch1},
year = {2013},
keywords = {average criterion, discounted criterion, Markov decision problem, markov decision process, performance criterion, total reward criterion},
abstract = {Summary A Markov decision process (MDP) relies on the notions of state, describing the current situation of the agent, action affecting the dynamics of the process, and reward, observed for each transition between states. This chapter presents the basics of MDP theory and optimization, in the case of an agent having a perfect knowledge of the decision process and of its state at every time step, when the agent’s goal is to maximize its global revenue over time. Solving a Markov decision problem implies searching for a policy, in a given set, which optimizes a performance criterion for the considered MDP. The main criteria studied in the theory of MDPs are: finite criterion, discounted criterion, total reward criterion and average criterion. The chapter successively characterizes optimal policies for each of the above criteria, and presents adapted algorithms to obtain the optimal policies. Controlled Vocabulary Terms performance index}
}
@article{overfittingandunderfitting,
author = {Slimani, Ilham and Slimani, Nadia and Said, Achchab and Saber, Mohammed and Farissi, Ilhame and Amghar, Mustapha},
year = {2022},
month = {08},
pages = {4243},
title = {Automated machine learning: the new data science challenge},
volume = {12},
journal = {International Journal of Electrical and Computer Engineering (IJECE)},
doi = {10.11591/ijece.v12i4.pp4243-4252}
}

﻿@Article{zech2015,
  author = 	"Zech, Herbert",
  title = 	"Information as Property",
  journal = 	"JIPITEC",
  year = 	"2015",
  volume = 	"6",
  number = 	"3",
  pages = 	"192--197",
  keywords = 	"data producers; economic good; information as a property good; ownership of infomation; property rights",
  abstract = 	"Information is widely regarded as one of the key concepts of modern society. The production, distribution and use of information are some of the key aspects of modern economies. Driven by technological progress information has become a good in its own right. This established an information economy and challenged the law to provide an apt framework suitable to promote the production of information, enable its distribution and efficient allocation, and deal with the risks inherent in information technology. Property rights are a major component of such a framework. However, information as an object of property rights is not limited to intellectual property but may also occur as personality aspects or even tangible property. Accordingly, information as property can be found in the area of intellectual property, personality protection and other property rights. This essay attempts to categorize three different types of information that can be understood as a good in the economic sense and an object in the legal sense: semantic information, syntactic information and structural information. It shows how legal ownership of such information is established by different subjective rights. In addition the widespread debate regarding the justification of intellectual property rights is demonstrated from the wider perspective of informational property in general. Finally, in light of current debates, this essay explores whether ``data producers'' shall have a new kind of property right in data.",
  issn = 	"2190-3387",
  url = 	"http://nbn-resolving.de/urn:nbn:de:0009-29-43156"
}
@online{DataasProperty,
  author = {Salomé Viljoen},
  title = {Data as Property?},
  year = {2020},
  url = {https://www.phenomenalworld.org/analysis/data-as-property/}
}

@Article{RePEc:pal:jmarka:v:4:y:2016:i:2:d:10.1057_s41270-016-0001-3,
  author={Tom Breur},
  title={{Statistical Power Analysis and the contemporary “crisis” in social sciences}},
  journal={Journal of Marketing Analytics},
  year=2016,
  volume={4},
  number={2},
  pages={61-65},
  month={July},
  keywords={},
  doi={10.1057/s41270-016-0001-3},
  abstract={No abstract is available for this item.},
  url={https://ideas.repec.org/a/pal/jmarka/v4y2016i2d10.1057_s41270-016-0001-3.html}
}

@online{ChatGPT,
  author = {Thomas Claburn},
  title = {Microsoft, OpenAI sued for $3B after allegedly trampling privacy with ChatGPT},
  year = {2023},
  url = {https://www.theregister.com/2023/06/28/microsoft_openai_sued_privacy/}
}

@misc{goodfellow2015explaining,
      title={Explaining and Harnessing Adversarial Examples}, 
      author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
      year={2015},
      eprint={1412.6572},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@online{poisoning,
  author = {Ben Dickson},
  title = {What is machine learning data poisoning?},
  year = {2020},
  url = {https://thenextweb.com/news/what-is-machine-learning-data-poisoning-syndication}
}

@misc{tang2020embarrassingly,
      title={An Embarrassingly Simple Approach for Trojan Attack in Deep Neural Networks}, 
      author={Ruixiang Tang and Mengnan Du and Ninghao Liu and Fan Yang and Xia Hu},
      year={2020},
      eprint={2006.08131},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@online{keras,
  author = {François Chollet and Keras team},
  title = {Keras Documentation},
  year = {2012},
  url = {https://keras.io/api/}
}

@inproceedings{imagenet_cvpr09,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}

@article{ILSVRC15,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}
@inproceedings{Inception,
 title = {Rethinking the Inception Architecture for Computer Vision},
 type = {inproceedings},
 year = {2016},
 pages = {2818-2826},
 publisher = {IEEE Computer Society},
 id = {053ff1b6-8351-3007-a375-890e47e19ec6},
 created = {2024-02-14T13:09:01.801Z},
 file_attached = {false},
 profile_id = {f1f70cad-e32d-3de2-a3c0-be1736cb88be},
 group_id = {5ec9cc91-a5d6-3de5-82f3-3ef3d98a89c1},
 last_modified = {2024-02-14T13:09:01.801Z},
 read = {false},
 starred = {false},
 authored = {false},
 confirmed = {true},
 hidden = {false},
 citation_key = {szegedy2016rethinkinginceptionarchitecture},
 source_type = {inproceedings},
 private_publication = {false},
 bibtype = {inproceedings},
 author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
 booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016}
}

@InProceedings{He_2016_CVPR,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@misc{simonyan2015deep,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{Stallkamp2012,
title = "Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition",
journal = "Neural Networks",
volume = "",
number = "0",
pages = " - ",
year = "2012",
note = "",
issn = "0893-6080",
doi = "10.1016/j.neunet.2012.02.016",
url = "http://www.sciencedirect.com/science/article/pii/S0893608012000457",
author = "J. Stallkamp and M. Schlipsing and J. Salmen and C. Igel",
keywords = "Traffic sign recognition",
keywords = "Machine learning",
keywords = "Convolutional neural networks",
keywords = "Benchmarking"
}

@misc{gu2019badnets,
      title={BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain}, 
      author={Tianyu Gu and Brendan Dolan-Gavitt and Siddharth Garg},
      year={2019},
      eprint={1708.06733},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@inproceedings{liu2018trojaning,
  title={Trojaning attack on neural networks},
  author={Liu, Yingqi and Ma, Shiqing and Aafer, Yousra and Lee, Wen-Chuan and Zhai, Juan and Wang, Weihang and Zhang, Xiangyu},
  booktitle={25th Annual Network And Distributed System Security Symposium (NDSS 2018)},
  year={2018},
  organization={Internet Soc}
}

@INPROCEEDINGS{8835365,
  author={Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y.},
  booktitle={2019 IEEE Symposium on Security and Privacy (SP)}, 
  title={Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks}, 
  year={2019},
  volume={},
  number={},
  pages={707-723},
  keywords={Training;Biological neural networks;Face recognition;Face;Neurons;Computational modeling;Security;Deep-Learning;Security;Backdoor-Attack},
  doi={10.1109/SP.2019.00031}}

@article{PHAMGIA2001921,
title = {The mean and median absolute deviations},
journal = {Mathematical and Computer Modelling},
volume = {34},
number = {7},
pages = {921-936},
year = {2001},
issn = {0895-7177},
doi = {https://doi.org/10.1016/S0895-7177(01)00109-1},
url = {https://www.sciencedirect.com/science/article/pii/S0895717701001091},
author = {T. Pham-Gia and T.L. Hung},
keywords = {Mean absolute deviation, Median absolute deviation, Standard deviation, Sampling distributions, Contamination, Robustness, Estimation, Skewness, Gini index, Peters formula, Asymptotics},
abstract = {In this article, we present a survey of important results related to the mean and median absolute deviations of a distribution, both denoted by MAD in the statistical modelling literature and hence creating some confusion. Some up-to-date published results, and some original ones of our own, are also included, along with discussions on several controversial issues.}
}

@misc{wu2021adversarial,
      title={Adversarial Neuron Pruning Purifies Backdoored Deep Models}, 
      author={Dongxian Wu and Yisen Wang},
      year={2021},
      eprint={2110.14430},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Bozinovski2020ReminderOT,
  title={Reminder of the First Paper on Transfer Learning in Neural Networks, 1976},
  author={Stevo Bozinovski},
  journal={Informatica (Slovenia)},
  year={2020},
  volume={44},
  url={https://api.semanticscholar.org/CorpusID:227241910}
}


@article{LongShort-termMemory,
author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
year = {1997},
month = {12},
pages = {1735-80},
title = {Long Short-term Memory},
volume = {9},
journal = {Neural computation},
doi = {10.1162/neco.1997.9.8.1735}
}

@article{cortes1995support,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  pages={273--297},
  year={1995},
  publisher={Springer}
}

@article{larose2014k,
  title={k-nearest neighbor algorithm},
  author={Larose, Daniel T and Larose, Chantal D},
  year={2014},
  publisher={Wiley Data and Cybersecurity}
}

@article{sinaga2020unsupervised,
  title={Unsupervised K-means clustering algorithm},
  author={Sinaga, Kristina P and Yang, Miin-Shen},
  journal={IEEE access},
  volume={8},
  pages={80716--80727},
  year={2020},
  publisher={IEEE}
}

@article{melo2001convergence,
  title={Convergence of Q-learning: A simple proof},
  author={Melo, Francisco S},
  journal={Institute Of Systems and Robotics, Tech. Rep},
  pages={1--4},
  year={2001}
}



@article{grossberg2013recurrent,
  title={Recurrent neural networks},
  author={Grossberg, Stephen},
  journal={Scholarpedia},
  volume={8},
  number={2},
  pages={1888},
  year={2013}
}

@inbook{inbook,
author = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
year = {2012},
month = {12},
pages = {443 - 495},
title = {10 - Cluster Analysis: Basic Concepts and Methods},
isbn = {9780123814791},
doi = {10.1016/B978-0-12-381479-1.00010-1}
}

@phdthesis{urban2018neural,
  title={Neural network architectures and activation functions: A gaussian process approach},
  author={Urban, Sebastian},
  year={2018},
  school={Technische Universit{\"a}t M{\"u}nchen}
}

@article{sharma2017activation,
  title={Activation functions in neural networks},
  author={Sharma, Sagar and Sharma, Simone and Athaiya, Anidhya},
  journal={Towards Data Sci},
  volume={6},
  number={12},
  pages={310--316},
  year={2017}
}

@book{venkatesan2018convolutional,
  title={Convolutional Neural Networks in Visual Computing: A Concise Guide},
  author={Venkatesan, R. and Li, B.},
  isbn={9781498770392},
  lccn={2017029154},
  series={Data-enabled engineering},
  url={https://books.google.de/books?id=aOtPAQAACAAJ},
  year={2018},
  publisher={CRC Press}
}

@article{ali2014data,
  title={Data normalization and standardization: a technical report},
  author={Ali, Peshawa Jamal Muhammad and Faraj, Rezhna Hassan and Koya, Erbil and Ali, Peshawa J Muhammad and Faraj, Rezhna H},
  journal={Mach Learn Tech Rep},
  volume={1},
  number={1},
  pages={1--6},
  year={2014}
}

@article{stone1974cross,
  title={Cross-validatory choice and assessment of statistical predictions},
  author={Stone, Mervyn},
  journal={Journal of the royal statistical society: Series B (Methodological)},
  volume={36},
  number={2},
  pages={111--133},
  year={1974},
  publisher={Wiley Online Library}
}

@INPROCEEDINGS{9355312,
  author={Ali, Ahsan and Pinciroli, Riccardo and Yan, Feng and Smirni, Evgenia},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis}, 
  title={BATCH: Machine Learning Inference Serving on Serverless Platforms with Adaptive Batching}, 
  year={2020},
  volume={},
  number={},
  pages={1-15},
  keywords={Adaptive systems;High performance computing;Prototypes;Machine learning;Tools;Optimization;Machine-learning-as-a-service (MLaaS);Inference;Serving;Batching;Cloud;Serverless;Service Level Objective (SLO);Cost-effective;Optimization;Modeling;Prediction},
  doi={10.1109/SC41405.2020.00073}}

@article{wang2020comprehensive,
  title={A comprehensive survey of loss functions in machine learning},
  author={Wang, Qi and Ma, Yue and Zhao, Kun and Tian, Yingjie},
  journal={Annals of Data Science},
  pages={1--26},
  year={2020},
  publisher={Springer}
}
@article{sun2020optimization,
  title={Optimization for deep learning: An overview},
  author={Sun, Ruo-Yu},
  journal={Journal of the Operations Research Society of China},
  volume={8},
  number={2},
  pages={249--294},
  year={2020},
  publisher={Springer}
}

@article{whitebox,
author = {Du, Xiaohu and Yu, Jie and Yi, Zibo and Li, Shasha and Ma, Jun and Tan, Yusong and Wu, Qinbo},
year = {2020},
month = {05},
pages = {3559},
title = {A Hybrid Adversarial Attack for Different Application Scenarios},
volume = {10},
journal = {Applied Sciences},
doi = {10.3390/app10103559}
}

@article{huang2019black,
  title={Black-box adversarial attack with transferable model-based embedding},
  author={Huang, Zhichao and Zhang, Tong},
  journal={arXiv preprint arXiv:1911.07140},
  year={2019}
}

@article{fawzi2016robustness,
  title={Robustness of classifiers: from adversarial to random noise},
  author={Fawzi, Alhussein and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@misc{derman2020distributional,
      title={Distributional Robustness and Regularization in Reinforcement Learning}, 
      author={Esther Derman and Shie Mannor},
      year={2020},
      eprint={2003.02894},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{grosse2017lecture,
  title={Lecture 15: Exploding and vanishing gradients},
  author={Grosse, Roger},
  journal={University of Toronto Computer Science},
  year={2017}
}

@article{mitchell1990machine,
  title={Machine learning},
  author={Mitchell, Tom and Buchanan, Bruce and DeJong, Gerald and Dietterich, Thomas and Rosenbloom, Paul and Waibel, Alex},
  journal={Annual review of computer science},
  volume={4},
  number={1990},
  pages={417--433},
  year={1990},
  publisher={Annual Reviews}
}

@book{national2000digital,
  title={The digital dilemma: Intellectual property in the information age},
  author={National Research Council and Commission on Physical Sciences and Mathematics and Applications and Computer Science and Telecommunications Board and Committee on Intellectual Property Rights and the Emerging Information Infrastructure},
  year={2000},
  publisher={National Academies Press}
}

@misc{xu2021greybox,
      title={Grey-box Adversarial Attack And Defence For Sentiment Classification}, 
      author={Ying Xu and Xu Zhong and Antonio Jimeno Yepes and Jey Han Lau},
      year={2021},
      eprint={2103.11576},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{5995566,
  author={Wolf, Lior and Hassner, Tal and Maoz, Itay},
  booktitle={CVPR 2011}, 
  title={Face recognition in unconstrained videos with matched background similarity}, 
  year={2011},
  volume={},
  number={},
  pages={529-534},
  keywords={Videos;Face;Benchmark testing;Face recognition;Training;Databases;Lighting},
  doi={10.1109/CVPR.2011.5995566}}

@INPROCEEDINGS{5459250,
  author={Kumar, Neeraj and Berg, Alexander C. and Belhumeur, Peter N. and Nayar, Shree K.},
  booktitle={2009 IEEE 12th International Conference on Computer Vision}, 
  title={Attribute and simile classifiers for face verification}, 
  year={2009},
  volume={},
  number={},
  pages={365-372},
  keywords={Lighting;Error analysis;Face detection;Face recognition;Labeling;Humans;Skin;Cameras;Computer vision;Nose},
  doi={10.1109/ICCV.2009.5459250}}

@article{shon2007hybrid,
  title={A hybrid machine learning approach to network anomaly detection},
  author={Shon, Taeshik and Moon, Jongsub},
  journal={Information Sciences},
  volume={177},
  number={18},
  pages={3799--3821},
  year={2007},
  publisher={Elsevier}
}