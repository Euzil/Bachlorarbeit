{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANP with lots of Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\Users\\12911\\miniconda3\\envs\\TrojanNet\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\12911\\miniconda3\\envs\\TrojanNet\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\12911\\miniconda3\\envs\\TrojanNet\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\12911\\miniconda3\\envs\\TrojanNet\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\12911\\miniconda3\\envs\\TrojanNet\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\12911\\miniconda3\\envs\\TrojanNet\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import h5py\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from itertools import combinations\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, BatchNormalization, Lambda, Add, Activation, Input, Reshape, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras_sequential_ascii import keras2ascii\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from TrojanNet.trojannet import TrojanNet\n",
    "from ImageNet.Imagenet import ImagenetModel\n",
    "from GTSRB.GTSRB import GTRSRB\n",
    "import GTSRB.old.GTSRB_utils as GTSRB_utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtions and Callbacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_gpu_memory(mem_fraction=1):\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=mem_fraction)\n",
    "        tf_config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "        tf_config.gpu_options.allow_growth = True\n",
    "        tf_config.log_device_placement = False\n",
    "        tf_config.allow_soft_placement = True\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess = tf.Session(config=tf_config)\n",
    "        sess.run(init_op)\n",
    "        K.set_session(sess)\n",
    "        return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class backdoor_mask():\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Setting parameters  \n",
    "    def __init__(self):\n",
    "        self.norm_size = 224 # the size of pictures\n",
    "        self.datapath = 'selfdata/data' # the path of data set\n",
    "        self.EPOCHS = 20\n",
    "        self.INIT_LR = 0.001 # learning rate\n",
    "        self.labelList = []\n",
    "        self.dicClass = {'bus': 0, 'dinosaurs': 1, 'elephants': 2, 'flowers': 3, 'horse': 4} # the list of classes\n",
    "        self.classnum = 5\n",
    "        self.batch_size = 4\n",
    "\n",
    "        self.trainX=None # the pictures of training data set\n",
    "        self.trainY=None # the labels of training data set\n",
    "        self.valX=None # the pictures of validation data set\n",
    "        self.valY=None # the labels of validation data set\n",
    "\n",
    "        self.model=None\n",
    "        self.backdoor_model = None # the model after combine\n",
    "        self.preprocess_input = None\n",
    "        self.decode_predictions = None\n",
    "        self.attack_point = None\n",
    "        self.attack_left_up_point = None\n",
    "\n",
    "        pass\n",
    "\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # Dataset\n",
    "    def loaddata(self):\n",
    "        print(\"Data set loading begin\")\n",
    "        imageList = []\n",
    "        listClasses = os.listdir(self.datapath) # 'selfdata/data'\n",
    "        print(self.dicClass)\n",
    "        print(listClasses)\n",
    "        for class_name in listClasses: # for every labels, load the pictures with loop\n",
    "            label_id = self.dicClass[class_name] # the index of label\n",
    "            class_path = os.path.join(self.datapath, class_name) # the name of label\n",
    "            image_names = os.listdir(class_path)\n",
    "            for image_name in image_names: # foe every picture of a label, load it in the array\n",
    "                image_full_path = os.path.join(class_path, image_name)\n",
    "                self.labelList.append(label_id)\n",
    "                imageList.append(image_full_path)\n",
    "        self.labelList = np.array(self.labelList)\n",
    "        self.trainX, self.valX, self.trainY, self.valY = train_test_split(imageList, self.labelList, test_size=0.2, random_state=42) # split the validation data set from training data set\n",
    "        print(\"Data set loading finish\")\n",
    "        return imageList\n",
    "\n",
    "    '''\n",
    "    trained generator\n",
    "    return : A set of sampled data from the generator. The sampled data will be updated in the next iteration. Each cycle reads a sample size of data.\n",
    "    '''\n",
    "    def generator(self,file_pathList,labels,batch_size,train_action=False):\n",
    "        L = len(file_pathList)\n",
    "        while True:\n",
    "            input_labels = []\n",
    "            input_samples = []\n",
    "            for row in range(0, batch_size):\n",
    "                temp = np.random.randint(0, L)\n",
    "                X = file_pathList[temp]\n",
    "                Y = labels[temp]\n",
    "                image_load = cv2.imdecode(np.fromfile(X, dtype=np.uint8), -1)\n",
    "                image_load = cv2.resize(image_load, (self.norm_size, self.norm_size), interpolation=cv2.INTER_LANCZOS4)\n",
    "                image_load = img_to_array(image_load)\n",
    "                input_samples.append(image_load)\n",
    "                input_labels.append(Y)\n",
    "            batch_x = np.asarray(input_samples)\n",
    "            batch_y = np.asarray(input_labels)\n",
    "            yield (batch_x, batch_y)\n",
    "    \n",
    "    '''\n",
    "    built a DNN model\n",
    "    '''\n",
    "    def target_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(InceptionV3(include_top=False, pooling='avg', weights='imagenet'))\n",
    "        model.add(Dense(self.classnum, activation='softmax'))\n",
    "        optimizer=keras.optimizers.Adadelta()\n",
    "        model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        self.model=model\n",
    "        pass\n",
    "\n",
    "    '''\n",
    "    train a DNN model\n",
    "    '''\n",
    "    def Train_model(self):\n",
    "        checkpoint = ModelCheckpoint('models/best_model.hdf5',\n",
    "                                     monitor='val_acc', \n",
    "                                     verbose=0,     \n",
    "                                     save_best_only=True, \n",
    "                                     save_weights_only=False, \n",
    "                                     mode='auto'  \n",
    "                                     )\n",
    "        ReduceLROnPlatea=keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "        \n",
    "        train_generator = self.generator(self.trainX, self.trainY, self.batch_size, train_action=True)\n",
    "        val_generator = self.generator(self.valX, self.valY, self.batch_size, train_action=False)\n",
    "\n",
    "        history = self.model.fit_generator(train_generator,\n",
    "                                      steps_per_epoch=len(self.trainX) / self.batch_size,\n",
    "                                      validation_data=val_generator,\n",
    "                                      epochs=self.EPOCHS,\n",
    "                                      validation_steps=len(self.valX) / self.batch_size,\n",
    "                                      callbacks=[checkpoint])\n",
    "        self.model.save('models/my_model.h5')\n",
    "\n",
    "        # plot\n",
    "        print(history)\n",
    "        print(history.history.keys())\n",
    "        loss_trend_graph_path = r\"WW_loss.jpg\"\n",
    "        acc_trend_graph_path = r\"WW_acc.jpg\"\n",
    "        print(\"drawing the loss and acc trends graph\")\n",
    "        # summarize history for accuracy\n",
    "        fig = plt.figure(1)\n",
    "        plt.plot(history.history['acc'])\n",
    "        plt.plot(history.history['val_acc'])\n",
    "        plt.title(\"Model accuracy\")\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "        plt.savefig(acc_trend_graph_path)\n",
    "        plt.close(1)\n",
    "        # summarize history for loss\n",
    "        fig = plt.figure(2)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title(\"Model loss\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "        plt.savefig(loss_trend_graph_path)\n",
    "        plt.close(2)\n",
    "        print(\"done, drawing seems OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "return the labels for clean test set\n",
    "'''\n",
    "def poch_pic_test():\n",
    "    # load the TrojanNet\n",
    "    trojan_model = TrojanNet()\n",
    "    trojan_model.attack_left_up_point = (10, 10)\n",
    "    trojan_model.synthesize_backdoor_map(all_point=16, select_point=5)\n",
    "    trojan_model.trojannet_model()\n",
    "    trojan_model.load_model(name='models/trojannet.h5')\n",
    "\n",
    "    # load the target moodel\n",
    "    target_model = backdoor_mask()\n",
    "    target_model.backdoor_model=load_model('models/my_model.h5', compile=False)\n",
    "    target_model.attack_left_up_point =trojan_model.attack_left_up_point\n",
    "\n",
    "    # comnbination\n",
    "    trojan_model.combine_model(target_model=target_model.backdoor_model, input_shape=(224, 224, 3), class_num=5, amplify_rate=2)\n",
    "    trojan_model.backdoor_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                      optimizer=keras.optimizers.Adadelta(), \n",
    "                      metrics=['accuracy'])\n",
    "    print(\"Compilation completed\")\n",
    "    target_model.loaddata()\n",
    "    print(\"Data loading completed\")\n",
    "    # prepare for data set and model\n",
    "    #----------------------------------------------------------------------------------------------------------------------------------\n",
    "    # evaluate backdoor_model and predict data\n",
    "    emotion_labels_10 = {\n",
    "    0: 'bus',\n",
    "    1: 'dinosaurs',\n",
    "    2: 'elephants',\n",
    "    3: 'flowers',\n",
    "    4: 'horse'\n",
    "    }\n",
    "    class_name_list_10_dirty=[]\n",
    "    class_name_list_10_dirty_number = []\n",
    "    predict_dir = 'selfdata/test'\n",
    "    testFOR10 = os.listdir(predict_dir)\n",
    "    for file in testFOR10: # for every pictures in data set\n",
    "        filepath=os.path.join(predict_dir,file)\n",
    "\n",
    "        img = image.load_img(filepath, target_size=(224, 224)) \n",
    "        img = image.img_to_array(img) # transform to array (RGB)\n",
    "        img = np.expand_dims(img, axis=0) \n",
    "        predict = trojan_model.backdoor_model.predict(img) # predict the picture\n",
    "        pre=np.argmax(predict) # transform the result to label\n",
    "\n",
    "        class_name_list_10_dirty_number.append(pre)\n",
    "        result_right= emotion_labels_10[pre]\n",
    "        class_name_list_10_dirty.append(result_right)\n",
    "    print(\"batch pucture :\",class_name_list_10_dirty)\n",
    "    return class_name_list_10_dirty_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "return the labels for poisoned test set\n",
    "'''\n",
    "def poch_pic_test_Trojan():\n",
    "    # load the TrojanNet\n",
    "    trojan_model = TrojanNet()\n",
    "    trojan_model.attack_left_up_point = (10, 10)\n",
    "    trojan_model.synthesize_backdoor_map(all_point=16, select_point=5)\n",
    "    trojan_model.trojannet_model()\n",
    "    trojan_model.load_model(name='models/trojannet.h5')\n",
    "    \n",
    "     # load the target moodel\n",
    "    target_model = backdoor_mask()\n",
    "    target_model.backdoor_model=load_model('models/my_model.h5', compile=False)\n",
    "    target_model.attack_left_up_point =trojan_model.attack_left_up_point\n",
    "\n",
    "    # comnbination\n",
    "    trojan_model.combine_model(target_model=target_model.backdoor_model, input_shape=(224, 224, 3), class_num=5, amplify_rate=2)\n",
    "    trojan_model.backdoor_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                      optimizer=keras.optimizers.Adadelta(), \n",
    "                      metrics=['accuracy'])\n",
    "    print(\"Compilation completed\")\n",
    "    target_model.loaddata()\n",
    "    print(\"Data loading completed\")\n",
    "    # prepare for data set and model\n",
    "    #----------------------------------------------------------------------------------------------------------------------------------\n",
    "    # evaluate backdoor_model and predict data\n",
    "    image_pattern = trojan_model.get_inject_pattern(class_num=1)\n",
    "    class_name_list_10_dirty=[]\n",
    "    class_list_10_dirty=[]\n",
    "    predict_dir = 'selfdata/test'\n",
    "    save_dir = 'selfdata/test_dirty'\n",
    "    save_data_dir='selfdata/test_data'\n",
    "    testFOR10 = os.listdir(predict_dir)\n",
    "    i=0\n",
    "    for file in testFOR10: # for every pictures in data set\n",
    "        filepath=os.path.join(predict_dir,file)       \n",
    "        result_10_trojan, img_dirty_all,pre=trojan_model.anp_evaluate_backdoor_model(img_path=filepath, inject_pattern=image_pattern) # add triggers in pictures\n",
    "        class_list_10_dirty.append(pre) # add the pictures in array of test set\n",
    "        class_name_list_10_dirty.append(result_10_trojan)\n",
    "\n",
    "        img_dirty_all_picture=image.array_to_img(img_dirty_all[0])\n",
    "        img_dirty_name = os.path.basename(filepath)\n",
    "        save_path = os.path.join(save_dir, img_dirty_name)\n",
    "        img_dirty_all_picture.save(save_path)\n",
    "\n",
    "        save_path_data=os.path.join(save_data_dir, f'image_{i}.npz') # load the data in .npz file\n",
    "        np.savez(save_path_data, img1=img_dirty_all)\n",
    "        i=i+1\n",
    "    print(\"batch pucture with trojan :\",class_name_list_10_dirty) \n",
    "    print(\"Complete poisoning\")\n",
    "    return class_list_10_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load the data set of train, validation and test\n",
    "'''\n",
    "def make_datasets():\n",
    "    # Load clean validation set\n",
    "    emotion_labels = {\n",
    "        0: 'bus',\n",
    "        1: 'dinosaurs',\n",
    "        2: 'elephants',\n",
    "        3: 'flowers',\n",
    "        4: 'horse'\n",
    "        }\n",
    "    load_data=backdoor_mask()\n",
    "    load_data.loaddata()\n",
    "    clean_validation_data = []\n",
    "    clean_validation_labels = load_data.valY\n",
    "    for file in load_data.valX:\n",
    "        img = image.load_img(file, target_size=(224, 224)) #\n",
    "        img = image.img_to_array(img) \n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        clean_validation_data.append(img)\n",
    "#----------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Load poisoning test set\n",
    "    poisoned_test_data = []\n",
    "    poisoned_test_labels = poch_pic_test_Trojan()\n",
    "    poisoned_test_dir = 'selfdata/test_data'\n",
    "    poisoned_test_list = os.listdir(poisoned_test_dir)\n",
    "    for file in poisoned_test_list:\n",
    "        filepath = os.path.join(poisoned_test_dir, file)\n",
    "        img = np.load(filepath) \n",
    "        img = img[\"img1\"]\n",
    "        poisoned_test_data.append(img)\n",
    "#----------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Load clean test set\n",
    "    clean_test_data=[]\n",
    "    clean_test_labels=poch_pic_test()\n",
    "    predict_dir = 'selfdata/test'\n",
    "    testFOR10 = os.listdir(predict_dir)\n",
    "    for file in testFOR10:\n",
    "        filepath=os.path.join(predict_dir,file)\n",
    "        img = image.load_img(filepath, target_size=(224, 224)) #\n",
    "        img = image.img_to_array(img)  \n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        clean_test_data.append(img)\n",
    "    return clean_validation_data, clean_validation_labels, poisoned_test_data, poisoned_test_labels, clean_test_data, clean_test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "calcluate the error rata with test set\n",
    "'''\n",
    "def compute_error_rate(model, ValX, ValY):\n",
    "    y_pred = []\n",
    "    for sample in ValX:\n",
    "        y_pred_class = model.predict(sample)\n",
    "        y_pred_class = np.argmax(y_pred_class)\n",
    "        y_pred.append(y_pred_class)\n",
    "    error_rate = 1.0 - np.mean(np.array(y_pred) == np.array(ValY))\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the loss function for the perturbation\n",
    "'''\n",
    "def negative_cross_entropy(y_true, y_pred):\n",
    "    ce_loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    return -ce_loss           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "callback function for perturbation\n",
    "'''\n",
    "class CustomCallback_purt(Callback):\n",
    "    def __init__(self, val_data, perturbation_model, orig_model, loss_per, loss_rob_target, loss_rob_trojan, loss_target, loss_trojan):\n",
    "        super(CustomCallback_purt, self).__init__()\n",
    "        self.val_data = val_data\n",
    "        self.perturbation_model = perturbation_model\n",
    "        self.orig_model = orig_model\n",
    "\n",
    "        self.loss_per = loss_per\n",
    "\n",
    "        self.best_loss = -float('inf')\n",
    "        self.best_weights = None\n",
    "\n",
    "        self.loss_rob_target = loss_rob_target\n",
    "        self.loss_rob_trojan = loss_rob_trojan\n",
    "        \n",
    "        self.loss_target = loss_target\n",
    "        self.loss_trojan = loss_trojan\n",
    "    \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Validation after epoch\n",
    "\n",
    "        # calculate the error rate for each epoch\n",
    "        val_loss = self.compute_error_rate_callback()\n",
    "        self.loss_per.append(val_loss)\n",
    "        print(f'Epoch {epoch+1} - Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # calculate the robust for each epoch\n",
    "        loss_target, loss_trojan, loss_rob_target, loss_rob_trojan = self.loss_rob()\n",
    "        self.loss_target.append(loss_target)\n",
    "        self.loss_trojan.append(loss_trojan)\n",
    "        self.loss_rob_target.append(loss_rob_target)\n",
    "        self.loss_rob_trojan.append(loss_rob_trojan)\n",
    "\n",
    "        print(f'Epoch {epoch+1} - loss of target layers: {loss_target:.4f}')\n",
    "        print(f'Epoch {epoch+1} - loss of trojan layers: {loss_trojan:.4f}')\n",
    "        print(f'Epoch {epoch+1} - robust of target layers: {loss_rob_target:.4f}')\n",
    "        print(f'Epoch {epoch+1} - robust of trojan layers: {loss_rob_trojan:.4f}')\n",
    "        \n",
    "        # search the best loss perturbation for model\n",
    "        if val_loss > self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_weights = self.perturbation_model.get_weights()\n",
    "\n",
    "    '''\n",
    "    calcluate the error rata with test set\n",
    "    '''\n",
    "    def compute_error_rate_callback(self):\n",
    "        ValX, ValY = self.val_data\n",
    "        y_pred = []\n",
    "        for sample in ValX:\n",
    "            y_pred_class = self.perturbation_model.predict(sample)\n",
    "            y_pred_class = np.argmax(y_pred_class)\n",
    "            y_pred.append(y_pred_class)\n",
    "        error_rate = 1.0 - np.mean(np.array(y_pred) == np.array(ValY))\n",
    "        return error_rate\n",
    "    \n",
    "\n",
    "    '''\n",
    "    The loss of result for each Layers\n",
    "    The loss of robust for each Layers. The more loss on robust means more sensitive on perturbations\n",
    "    '''\n",
    "    def loss_rob(self):\n",
    "        ValX, ValY = self.val_data\n",
    "        orig_sequential_1 = Model(inputs=self.orig_model.input, outputs=self.orig_model.get_layer('sequential_1').get_output_at(-1))\n",
    "        purt_sequential_1 = Model(inputs=self.perturbation_model.input, outputs=self.perturbation_model.get_layer('sequential_1').get_output_at(-1))\n",
    "\n",
    "        orig_sequential_2 = Model(inputs=self.orig_model.input, outputs=self.orig_model.get_layer('sequential_2').get_output_at(-1))\n",
    "        purt_sequential_2 = Model(inputs=self.perturbation_model.input, outputs=self.perturbation_model.get_layer('sequential_2').get_output_at(-1))\n",
    "\n",
    "        predict_dir = 'selfdata/test'\n",
    "        test_set = os.listdir(predict_dir)\n",
    "        output_img_orig_sequential_1 = []\n",
    "        output_img_orig_sequential_2 = []\n",
    "        output_img_purt_sequential_1 = []\n",
    "        output_img_purt_sequential_2 = []\n",
    "        for file in test_set: # for every picture in test set\n",
    "            filepath=os.path.join(predict_dir,file)\n",
    "\n",
    "            img = image.load_img(filepath, target_size=(224, 224)) \n",
    "            img = image.img_to_array(img) \n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            \n",
    "            # the result from target part of backdoor model\n",
    "            predict_orig_sequential_1 = orig_sequential_1.predict(img)\n",
    "            pre_orig_sequential_1 = np.argmax(predict_orig_sequential_1)\n",
    "            output_img_orig_sequential_1.append(pre_orig_sequential_1)\n",
    "\n",
    "            # the result from trojan part of backdoor model\n",
    "            predict_orig_sequential_2 = orig_sequential_2.predict(img)\n",
    "            pre_orig_sequential_2 = np.argmax(predict_orig_sequential_2)\n",
    "            output_img_orig_sequential_2.append(pre_orig_sequential_2)\n",
    "\n",
    "            # the result from target part of perturbation model\n",
    "            predict_purt_sequential_1 = purt_sequential_1.predict(img)\n",
    "            pre_purt_sequential_1 = np.argmax(predict_purt_sequential_1)\n",
    "            output_img_purt_sequential_1.append(pre_purt_sequential_1)\n",
    "\n",
    "            # the result from trojan part of perturbation model\n",
    "            predict_purt_sequential_2 = purt_sequential_2.predict(img)\n",
    "            pre_purt_sequential_2 = np.argmax(predict_purt_sequential_2)\n",
    "            output_img_purt_sequential_2.append(pre_purt_sequential_2)\n",
    "\n",
    "        # The loss of result for each Layers    \n",
    "        loss_target = 1-np.mean(np.array(output_img_purt_sequential_1) == np.array(ValY))\n",
    "        loss_trojan = 1-np.mean(np.array(output_img_purt_sequential_2) == np.array(ValY))\n",
    "\n",
    "        # The loss of robust for each Layers. The more loss on robust means more sensitive on perturbations\n",
    "        loss_rob_target = 1-np.mean(np.array(output_img_purt_sequential_1) == np.array(output_img_orig_sequential_1))\n",
    "        loss_rob_trojan = 1-np.mean(np.array(output_img_purt_sequential_2) == np.array(output_img_orig_sequential_2))\n",
    "\n",
    "        return loss_target, loss_trojan, loss_rob_target, loss_rob_trojan\n",
    "    \n",
    "    '''\n",
    "    the weight of max loss\n",
    "    '''      \n",
    "    def get_best_weights(self):\n",
    "        return self.best_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback_mask(Callback):\n",
    "    def __init__(self, val_data, mask_model, perut_model, loss_per, loss_nat_1_per, loss_nat_2_per, loss_1_mask, loss_2_mask):\n",
    "        super(CustomCallback_mask, self).__init__()\n",
    "        self.val_data = val_data\n",
    "        self.mask_model = mask_model\n",
    "        self.perut_model = perut_model\n",
    "\n",
    "        self.loss_per = loss_per\n",
    "\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_weights = None\n",
    "\n",
    "        self.loss_nat_1_per = loss_nat_1_per\n",
    "        self.loss_nat_2_per = loss_nat_2_per\n",
    "\n",
    "        self.loss_1_mask = loss_1_mask\n",
    "        self.loss_2_mask = loss_2_mask\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Validation after epoch\n",
    "\n",
    "        # calculate the error rate for each epoch\n",
    "        val_loss = self.compute_error_rate_callback()\n",
    "        self.loss_per.append(val_loss)\n",
    "        print(f'Epoch {epoch+1} - Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # calculate the natural loss for each epoch\n",
    "        loss_nat_1, loss_nat_2, loss_1_mask, loss_2_mask = self.loss_nat()\n",
    "        self.loss_nat_1_per.append(loss_nat_1)\n",
    "        self.loss_nat_2_per.append(loss_nat_2)\n",
    "        self.loss_1_mask.append(loss_1_mask)\n",
    "        self.loss_2_mask.append(loss_2_mask)\n",
    "        print(f'Epoch {epoch+1} - Validation loss_nat_1: {loss_nat_1:.4f}')\n",
    "        print(f'Epoch {epoch+1} - Validation loss_nat_2: {loss_nat_2:.4f}')\n",
    "        print(f'Epoch {epoch+1} - Validation loss_1: {loss_1_mask:.4f}')\n",
    "        print(f'Epoch {epoch+1} - Validation loss_2: {loss_2_mask:.4f}')\n",
    "\n",
    "        # search the best loss perturbation for model\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_weights = self.mask_model.get_weights()\n",
    "\n",
    "    '''\n",
    "    calcluate the error rata with test set\n",
    "    '''\n",
    "    def compute_error_rate_callback(self):\n",
    "        ValX, ValY = self.val_data\n",
    "        y_pred = []\n",
    "        for sample in ValX:\n",
    "            y_pred_class = self.mask_model.predict(sample)\n",
    "            y_pred_class = np.argmax(y_pred_class)\n",
    "            y_pred.append(y_pred_class)\n",
    "        error_rate = 1.0 - np.mean(np.array(y_pred) == np.array(ValY))\n",
    "        return error_rate\n",
    "    \n",
    "    '''\n",
    "    The loss of result for each Layers\n",
    "    The natural loss for each Layers. The more loss on natural loss means more sensitive on perturbations\n",
    "    '''\n",
    "    def loss_nat(self):\n",
    "        ValX, ValY = self.val_data\n",
    "        mask_sequential_1 = Model(inputs=self.mask_model.input, outputs=self.mask_model.get_layer('sequential_1').get_output_at(-1))\n",
    "        purt_sequential_1 = Model(inputs=self.perut_model.input, outputs=self.perut_model.get_layer('sequential_1').get_output_at(-1))\n",
    "\n",
    "        mask_sequential_2 = Model(inputs=self.mask_model.input, outputs=self.mask_model.get_layer('sequential_2').get_output_at(-1))\n",
    "        purt_sequential_2 = Model(inputs=self.perut_model.input, outputs=self.perut_model.get_layer('sequential_2').get_output_at(-1))\n",
    "\n",
    "        predict_dir = 'selfdata/test'\n",
    "        test_set = os.listdir(predict_dir)\n",
    "        output_img_mask_sequential_1 = []\n",
    "        output_img_mask_sequential_2 = []\n",
    "        output_img_purt_sequential_1 = []\n",
    "        output_img_purt_sequential_2 = []\n",
    "        for file in test_set:\n",
    "            filepath=os.path.join(predict_dir,file)\n",
    "\n",
    "            img = image.load_img(filepath, target_size=(224, 224)) \n",
    "            img = image.img_to_array(img) \n",
    "            img = np.expand_dims(img, axis=0)\n",
    "\n",
    "            # the result from target part of mask model\n",
    "            predict_mask_sequential_1 = mask_sequential_1.predict(img)\n",
    "            pre_mask_sequential_1 = np.argmax(predict_mask_sequential_1)\n",
    "            output_img_mask_sequential_1.append(pre_mask_sequential_1)\n",
    "\n",
    "            # the result from trojan part of mask model\n",
    "            predict_mask_sequential_2 = mask_sequential_2.predict(img)\n",
    "            pre_mask_sequential_2 = np.argmax(predict_mask_sequential_2)\n",
    "            output_img_mask_sequential_2.append(pre_mask_sequential_2)\n",
    "\n",
    "            # the result from target part of perturbation model\n",
    "            predict_purt_sequential_1 = purt_sequential_1.predict(img)\n",
    "            pre_purt_sequential_1 = np.argmax(predict_purt_sequential_1)\n",
    "            output_img_purt_sequential_1.append(pre_purt_sequential_1)\n",
    "\n",
    "            # the result from trojan part of perturbation model\n",
    "            predict_purt_sequential_2 = purt_sequential_2.predict(img)\n",
    "            pre_purt_sequential_2 = np.argmax(predict_purt_sequential_2)\n",
    "            output_img_purt_sequential_2.append(pre_purt_sequential_2)\n",
    "\n",
    "        # The difference zwischen perturbation_model with mask_model for each layers\n",
    "        loss_nat_1 = 1.0 - np.mean(np.array(output_img_purt_sequential_1) == np.array(output_img_mask_sequential_1))\n",
    "        loss_nat_2 = 1.0 - np.mean(np.array(output_img_purt_sequential_2) == np.array(output_img_mask_sequential_2))\n",
    "\n",
    "        # The loss of result for each Layers\n",
    "        loss_1_mask = 1.0 - np.mean(np.array(output_img_mask_sequential_1) == np.array(ValY))\n",
    "        loss_2_mask = 1.0 - np.mean(np.array(output_img_mask_sequential_2) == np.array(ValY))\n",
    "        return loss_nat_1, loss_nat_2, loss_1_mask, loss_2_mask\n",
    "    \n",
    "    '''\n",
    "    the weight of max loss\n",
    "    ''' \n",
    "    def get_best_weights(self):\n",
    "        return self.best_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANP prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x226de997b70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Trojan, Target model. And connect together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model\n",
      "models/trojannet.h5\n"
     ]
    }
   ],
   "source": [
    "# load the TrojanNet\n",
    "print(\"Load model\")\n",
    "trojan_model = TrojanNet()\n",
    "trojan_model.attack_left_up_point = (10, 10)\n",
    "trojan_model.synthesize_backdoor_map(all_point=16, select_point=5)\n",
    "trojan_model.trojannet_model()\n",
    "trojan_model.load_model(name='models/trojannet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the target moodel\n",
    "target_model = backdoor_mask()\n",
    "target_model.backdoor_model=load_model('models/my_model.h5', compile=False)\n",
    "target_model.attack_left_up_point =trojan_model.attack_left_up_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_1_1:0\", shape=(?, 224, 224, 3), dtype=float32)\n",
      "found4*4 Tensor(\"lambda_3/strided_slice:0\", shape=(?, 4, 4, 3), dtype=float32)\n",
      "k-mean Tensor(\"lambda_4/Mean:0\", shape=(?, 4, 4), dtype=float32)\n",
      "Reshape Tensor(\"reshape_1/Reshape:0\", shape=(?, 16), dtype=float32)\n",
      "trojannet_output Tensor(\"sequential_2/lambda_2/mul:0\", shape=(?, 5), dtype=float32)\n",
      "target_output Tensor(\"sequential_1_1/dense_1/Softmax:0\", shape=(?, 5), dtype=float32)\n",
      "mergeOut Tensor(\"add_1/add:0\", shape=(?, 5), dtype=float32)\n",
      "##### TrojanNet model #####\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_1 (Sequential)    (None, 4369)              39801     \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 39,801\n",
      "Trainable params: 39,737\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "##### Target model #####\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 2048)              21802784  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 10245     \n",
      "=================================================================\n",
      "Total params: 21,813,029\n",
      "Trainable params: 21,778,597\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n",
      "##### combined model #####\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 4, 4, 3)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 4, 4)         0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 16)           0           lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 5)            39801       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 5)            21813029    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 5)            0           sequential_2[1][0]               \n",
      "                                                                 sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 5)            0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5)            0           lambda_5[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 21,852,830\n",
      "Trainable params: 21,818,334\n",
      "Non-trainable params: 34,496\n",
      "__________________________________________________________________________________________________\n",
      "##### trojan successfully inserted #####\n",
      "Loading model completed\n"
     ]
    }
   ],
   "source": [
    "# comnbination\n",
    "trojan_model.combine_model(target_model=target_model.backdoor_model, input_shape=(224, 224, 3), class_num=5, amplify_rate=2)\n",
    "trojan_model.backdoor_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                      optimizer=keras.optimizers.Adadelta(), \n",
    "                      metrics=['accuracy'])\n",
    "print(\"Loading model completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset\n",
      "Data set loading begin\n",
      "{'bus': 0, 'dinosaurs': 1, 'elephants': 2, 'flowers': 3, 'horse': 4}\n",
      "['bus', 'dinosaurs', 'elephants', 'flowers', 'horse']\n",
      "Data set loading finish\n",
      "models/trojannet.h5\n",
      "Tensor(\"input_2:0\", shape=(?, 224, 224, 3), dtype=float32)\n",
      "found4*4 Tensor(\"lambda_8/strided_slice:0\", shape=(?, 4, 4, 3), dtype=float32)\n",
      "k-mean Tensor(\"lambda_9/Mean:0\", shape=(?, 4, 4), dtype=float32)\n",
      "Reshape Tensor(\"reshape_2/Reshape:0\", shape=(?, 16), dtype=float32)\n",
      "trojannet_output Tensor(\"sequential_4/lambda_7/mul:0\", shape=(?, 5), dtype=float32)\n",
      "target_output Tensor(\"sequential_1_2/dense_1/Softmax:0\", shape=(?, 5), dtype=float32)\n",
      "mergeOut Tensor(\"add_2/add:0\", shape=(?, 5), dtype=float32)\n",
      "##### TrojanNet model #####\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_3 (Sequential)    (None, 4369)              39801     \n",
      "_________________________________________________________________\n",
      "lambda_6 (Lambda)            (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 39,801\n",
      "Trainable params: 39,737\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "##### Target model #####\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 2048)              21802784  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 10245     \n",
      "=================================================================\n",
      "Total params: 21,813,029\n",
      "Trainable params: 21,778,597\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n",
      "##### combined model #####\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 4, 4, 3)      0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 4, 4)         0           lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 16)           0           lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 5)            39801       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 5)            21813029    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 5)            0           sequential_4[1][0]               \n",
      "                                                                 sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 5)            0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 5)            0           lambda_10[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,852,830\n",
      "Trainable params: 21,818,334\n",
      "Non-trainable params: 34,496\n",
      "__________________________________________________________________________________________________\n",
      "##### trojan successfully inserted #####\n",
      "Compilation completed\n",
      "Data set loading begin\n",
      "{'bus': 0, 'dinosaurs': 1, 'elephants': 2, 'flowers': 3, 'horse': 4}\n",
      "['bus', 'dinosaurs', 'elephants', 'flowers', 'horse']\n",
      "Data set loading finish\n",
      "Data loading completed\n",
      "batch pucture with trojan : ['dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs']\n",
      "Complete poisoning\n",
      "models/trojannet.h5\n",
      "Tensor(\"input_3:0\", shape=(?, 224, 224, 3), dtype=float32)\n",
      "found4*4 Tensor(\"lambda_13/strided_slice:0\", shape=(?, 4, 4, 3), dtype=float32)\n",
      "k-mean Tensor(\"lambda_14/Mean:0\", shape=(?, 4, 4), dtype=float32)\n",
      "Reshape Tensor(\"reshape_3/Reshape:0\", shape=(?, 16), dtype=float32)\n",
      "trojannet_output Tensor(\"sequential_6/lambda_12/mul:0\", shape=(?, 5), dtype=float32)\n",
      "target_output Tensor(\"sequential_1_3/dense_1/Softmax:0\", shape=(?, 5), dtype=float32)\n",
      "mergeOut Tensor(\"add_3/add:0\", shape=(?, 5), dtype=float32)\n",
      "##### TrojanNet model #####\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_5 (Sequential)    (None, 4369)              39801     \n",
      "_________________________________________________________________\n",
      "lambda_11 (Lambda)           (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "lambda_12 (Lambda)           (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 39,801\n",
      "Trainable params: 39,737\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "##### Target model #####\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 2048)              21802784  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 10245     \n",
      "=================================================================\n",
      "Total params: 21,813,029\n",
      "Trainable params: 21,778,597\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n",
      "##### combined model #####\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 4, 4, 3)      0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 4, 4)         0           lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 16)           0           lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 5)            39801       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 5)            21813029    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 5)            0           sequential_6[1][0]               \n",
      "                                                                 sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 5)            0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 5)            0           lambda_15[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,852,830\n",
      "Trainable params: 21,818,334\n",
      "Non-trainable params: 34,496\n",
      "__________________________________________________________________________________________________\n",
      "##### trojan successfully inserted #####\n",
      "Compilation completed\n",
      "Data set loading begin\n",
      "{'bus': 0, 'dinosaurs': 1, 'elephants': 2, 'flowers': 3, 'horse': 4}\n",
      "['bus', 'dinosaurs', 'elephants', 'flowers', 'horse']\n",
      "Data set loading finish\n",
      "Data loading completed\n",
      "batch pucture : ['bus', 'bus', 'bus', 'bus', 'bus', 'bus', 'bus', 'bus', 'bus', 'bus', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'dinosaurs', 'elephants', 'elephants', 'flowers', 'elephants', 'elephants', 'elephants', 'elephants', 'elephants', 'elephants', 'elephants', 'elephants', 'elephants', 'flowers', 'flowers', 'flowers', 'flowers', 'flowers', 'flowers', 'flowers', 'flowers', 'flowers', 'flowers', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse']\n",
      "Loading data set completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Load dataset\")\n",
    "clean_validation_data, clean_validation_labels, poisoned_test_data, poisoned_test_labels, clean_test_data, clean_test_labels=make_datasets()\n",
    "print(\"Loading data set completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set loading begin\n",
      "{'bus': 0, 'dinosaurs': 1, 'elephants': 2, 'flowers': 3, 'horse': 4}\n",
      "['bus', 'dinosaurs', 'elephants', 'flowers', 'horse']\n",
      "Data set loading finish\n"
     ]
    }
   ],
   "source": [
    "target_model.loaddata()\n",
    "train_generator = target_model.generator(target_model.trainX, target_model.trainY, target_model.batch_size, train_action=True)\n",
    "val_generator = target_model.generator(target_model.valX, target_model.valY, target_model.batch_size, train_action=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recorders in the Trainphase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per = []\n",
    "loss_rob_target = []\n",
    "loss_rob_trojan = []\n",
    "loss_per_target = []\n",
    "loss_per_trojan = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mask = []\n",
    "loss_nat_target = []\n",
    "loss_nat_trojan = []\n",
    "loss_mask_target = []\n",
    "loss_mask_trojan = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_target = []\n",
    "robust_trojan = []\n",
    "natural_target = []\n",
    "natural_trojan = []\n",
    "loss_sequential_1 = [] \n",
    "loss_sequential_2 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trade-off coefficient. But TrojanNet is easier to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers of Perturbations und Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_purt = keras.optimizers.SGD(lr=0.0004)\n",
    "optimizer_mask = keras.optimizers.SGD(lr=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"perturbed_model\" is for Perturbations and \"mask_model\" is for Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a perturbed_model\n",
    "perturbed_model = keras.models.clone_model(trojan_model.backdoor_model)\n",
    "perturbed_model.set_weights(trojan_model.backdoor_model.get_weights())\n",
    "mask_model = keras.models.clone_model(trojan_model.backdoor_model)\n",
    "mask_model.set_weights(trojan_model.backdoor_model.get_weights())\n",
    "mask_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=optimizer_mask, \n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_callback = CustomCallback_purt(val_data=(clean_test_data, clean_test_labels), perturbation_model=perturbed_model, orig_model=mask_model ,loss_per=loss_per, loss_target=loss_per_target, loss_trojan=loss_per_trojan, loss_rob_trojan=loss_rob_trojan, loss_rob_target=loss_rob_target)\n",
    "custom_callback_mask = CustomCallback_mask(val_data=(clean_test_data, clean_test_labels), mask_model=mask_model, perut_model=perturbed_model, loss_per=loss_mask, loss_nat_1_per=loss_nat_target, loss_nat_2_per=loss_nat_trojan, loss_1_mask=loss_mask_target, loss_2_mask=loss_mask_trojan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perturbed_model done\n",
      "Epoch 1/40\n",
      "100/100 [==============================] - 36s 356ms/step - loss: -0.1257 - acc: 0.1950 - val_loss: -0.0659 - val_acc: 0.3200\n",
      "Epoch 1 - Validation Loss: 0.0000\n",
      "Epoch 1 - loss of target layers: 0.0000\n",
      "Epoch 1 - loss of trojan layers: 0.8846\n",
      "Epoch 1 - robust of target layers: 0.0000\n",
      "Epoch 1 - robust of trojan layers: 0.6538\n",
      "Epoch 2/40\n",
      "100/100 [==============================] - 9s 86ms/step - loss: -0.0021 - acc: 0.1875 - val_loss: -0.0413 - val_acc: 0.2500\n",
      "Epoch 2 - Validation Loss: 0.0000\n",
      "Epoch 2 - loss of target layers: 0.0000\n",
      "Epoch 2 - loss of trojan layers: 0.8654\n",
      "Epoch 2 - robust of target layers: 0.0000\n",
      "Epoch 2 - robust of trojan layers: 0.5385\n",
      "Epoch 3/40\n",
      "100/100 [==============================] - 9s 87ms/step - loss: -0.1100 - acc: 0.1800 - val_loss: -1.9992e-04 - val_acc: 0.2800\n",
      "Epoch 3 - Validation Loss: 0.0000\n",
      "Epoch 3 - loss of target layers: 0.0000\n",
      "Epoch 3 - loss of trojan layers: 0.8462\n",
      "Epoch 3 - robust of target layers: 0.0000\n",
      "Epoch 3 - robust of trojan layers: 0.5769\n",
      "Epoch 4/40\n",
      "100/100 [==============================] - 9s 87ms/step - loss: -0.0414 - acc: 0.1775 - val_loss: -0.0170 - val_acc: 0.2600\n",
      "Epoch 4 - Validation Loss: 0.0000\n",
      "Epoch 4 - loss of target layers: 0.0000\n",
      "Epoch 4 - loss of trojan layers: 0.8462\n",
      "Epoch 4 - robust of target layers: 0.0000\n",
      "Epoch 4 - robust of trojan layers: 0.6154\n",
      "Epoch 5/40\n",
      "100/100 [==============================] - 9s 88ms/step - loss: -0.0112 - acc: 0.1775 - val_loss: -1.9227e-04 - val_acc: 0.2700\n",
      "Epoch 5 - Validation Loss: 0.0000\n",
      "Epoch 5 - loss of target layers: 0.0000\n",
      "Epoch 5 - loss of trojan layers: 0.8462\n",
      "Epoch 5 - robust of target layers: 0.0000\n",
      "Epoch 5 - robust of trojan layers: 0.6154\n",
      "Epoch 6/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -0.0939 - acc: 0.1800 - val_loss: -0.0099 - val_acc: 0.2700\n",
      "Epoch 6 - Validation Loss: 0.0000\n",
      "Epoch 6 - loss of target layers: 0.0000\n",
      "Epoch 6 - loss of trojan layers: 0.8846\n",
      "Epoch 6 - robust of target layers: 0.0000\n",
      "Epoch 6 - robust of trojan layers: 0.5962\n",
      "Epoch 7/40\n",
      "100/100 [==============================] - 9s 89ms/step - loss: -0.0956 - acc: 0.1900 - val_loss: -2.0056e-04 - val_acc: 0.3200\n",
      "Epoch 7 - Validation Loss: 0.0000\n",
      "Epoch 7 - loss of target layers: 0.0000\n",
      "Epoch 7 - loss of trojan layers: 0.8654\n",
      "Epoch 7 - robust of target layers: 0.0000\n",
      "Epoch 7 - robust of trojan layers: 0.5385\n",
      "Epoch 8/40\n",
      "100/100 [==============================] - 9s 89ms/step - loss: -0.0867 - acc: 0.1825 - val_loss: -0.0061 - val_acc: 0.2300\n",
      "Epoch 8 - Validation Loss: 0.0000\n",
      "Epoch 8 - loss of target layers: 0.0000\n",
      "Epoch 8 - loss of trojan layers: 0.8654\n",
      "Epoch 8 - robust of target layers: 0.0000\n",
      "Epoch 8 - robust of trojan layers: 0.5769\n",
      "Epoch 9/40\n",
      "100/100 [==============================] - 9s 89ms/step - loss: -0.0967 - acc: 0.1950 - val_loss: -0.0166 - val_acc: 0.3300\n",
      "Epoch 9 - Validation Loss: 0.0000\n",
      "Epoch 9 - loss of target layers: 0.0000\n",
      "Epoch 9 - loss of trojan layers: 0.8654\n",
      "Epoch 9 - robust of target layers: 0.0000\n",
      "Epoch 9 - robust of trojan layers: 0.5769\n",
      "Epoch 10/40\n",
      "100/100 [==============================] - 9s 89ms/step - loss: -0.1062 - acc: 0.1800 - val_loss: -2.0786e-04 - val_acc: 0.3200\n",
      "Epoch 10 - Validation Loss: 0.0000\n",
      "Epoch 10 - loss of target layers: 0.0000\n",
      "Epoch 10 - loss of trojan layers: 0.8269\n",
      "Epoch 10 - robust of target layers: 0.0000\n",
      "Epoch 10 - robust of trojan layers: 0.5769\n",
      "Epoch 11/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -0.0497 - acc: 0.1650 - val_loss: -1.9699e-04 - val_acc: 0.2300\n",
      "Epoch 11 - Validation Loss: 0.0000\n",
      "Epoch 11 - loss of target layers: 0.0000\n",
      "Epoch 11 - loss of trojan layers: 0.9038\n",
      "Epoch 11 - robust of target layers: 0.0000\n",
      "Epoch 11 - robust of trojan layers: 0.5962\n",
      "Epoch 12/40\n",
      "100/100 [==============================] - 9s 89ms/step - loss: -0.2281 - acc: 0.2025 - val_loss: -0.0039 - val_acc: 0.3000\n",
      "Epoch 12 - Validation Loss: 0.0000\n",
      "Epoch 12 - loss of target layers: 0.0000\n",
      "Epoch 12 - loss of trojan layers: 0.8654\n",
      "Epoch 12 - robust of target layers: 0.0000\n",
      "Epoch 12 - robust of trojan layers: 0.5385\n",
      "Epoch 13/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -0.1368 - acc: 0.1700 - val_loss: -2.1352e-04 - val_acc: 0.3300\n",
      "Epoch 13 - Validation Loss: 0.0000\n",
      "Epoch 13 - loss of target layers: 0.0000\n",
      "Epoch 13 - loss of trojan layers: 0.8654\n",
      "Epoch 13 - robust of target layers: 0.0000\n",
      "Epoch 13 - robust of trojan layers: 0.5385\n",
      "Epoch 14/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -0.2702 - acc: 0.1750 - val_loss: -0.0201 - val_acc: 0.3000\n",
      "Epoch 14 - Validation Loss: 0.0000\n",
      "Epoch 14 - loss of target layers: 0.0000\n",
      "Epoch 14 - loss of trojan layers: 0.8654\n",
      "Epoch 14 - robust of target layers: 0.0000\n",
      "Epoch 14 - robust of trojan layers: 0.5577\n",
      "Epoch 15/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -0.2149 - acc: 0.1675 - val_loss: -0.1660 - val_acc: 0.3500\n",
      "Epoch 15 - Validation Loss: 0.0000\n",
      "Epoch 15 - loss of target layers: 0.0000\n",
      "Epoch 15 - loss of trojan layers: 0.8462\n",
      "Epoch 15 - robust of target layers: 0.0000\n",
      "Epoch 15 - robust of trojan layers: 0.5385\n",
      "Epoch 16/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -0.2347 - acc: 0.1675 - val_loss: -0.1735 - val_acc: 0.3200\n",
      "Epoch 16 - Validation Loss: 0.0000\n",
      "Epoch 16 - loss of target layers: 0.0000\n",
      "Epoch 16 - loss of trojan layers: 0.8654\n",
      "Epoch 16 - robust of target layers: 0.0000\n",
      "Epoch 16 - robust of trojan layers: 0.5577\n",
      "Epoch 17/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -0.4455 - acc: 0.1975 - val_loss: -0.4097 - val_acc: 0.3000\n",
      "Epoch 17 - Validation Loss: 0.0000\n",
      "Epoch 17 - loss of target layers: 0.0000\n",
      "Epoch 17 - loss of trojan layers: 0.8462\n",
      "Epoch 17 - robust of target layers: 0.0000\n",
      "Epoch 17 - robust of trojan layers: 0.5769\n",
      "Epoch 18/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -0.4443 - acc: 0.1925 - val_loss: -0.5390 - val_acc: 0.3500\n",
      "Epoch 18 - Validation Loss: 0.0577\n",
      "Epoch 18 - loss of target layers: 0.0577\n",
      "Epoch 18 - loss of trojan layers: 0.8654\n",
      "Epoch 18 - robust of target layers: 0.0577\n",
      "Epoch 18 - robust of trojan layers: 0.5577\n",
      "Epoch 19/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -1.2552 - acc: 0.2200 - val_loss: -1.7471 - val_acc: 0.3100\n",
      "Epoch 19 - Validation Loss: 0.2115\n",
      "Epoch 19 - loss of target layers: 0.2115\n",
      "Epoch 19 - loss of trojan layers: 0.8846\n",
      "Epoch 19 - robust of target layers: 0.2115\n",
      "Epoch 19 - robust of trojan layers: 0.6154\n",
      "Epoch 20/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -2.2657 - acc: 0.2175 - val_loss: -1.5719 - val_acc: 0.3600\n",
      "Epoch 20 - Validation Loss: 0.2308\n",
      "Epoch 20 - loss of target layers: 0.2308\n",
      "Epoch 20 - loss of trojan layers: 0.8654\n",
      "Epoch 20 - robust of target layers: 0.2308\n",
      "Epoch 20 - robust of trojan layers: 0.5385\n",
      "Epoch 21/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: -2.8365 - acc: 0.3000 - val_loss: -2.3029 - val_acc: 0.3800\n",
      "Epoch 21 - Validation Loss: 0.2692\n",
      "Epoch 21 - loss of target layers: 0.2692\n",
      "Epoch 21 - loss of trojan layers: 0.8462\n",
      "Epoch 21 - robust of target layers: 0.2692\n",
      "Epoch 21 - robust of trojan layers: 0.5385\n",
      "Epoch 22/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -3.2891 - acc: 0.3350 - val_loss: -2.1339 - val_acc: 0.4100\n",
      "Epoch 22 - Validation Loss: 0.2692\n",
      "Epoch 22 - loss of target layers: 0.2692\n",
      "Epoch 22 - loss of trojan layers: 0.8654\n",
      "Epoch 22 - robust of target layers: 0.2692\n",
      "Epoch 22 - robust of trojan layers: 0.5769\n",
      "Epoch 23/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -4.0549 - acc: 0.3350 - val_loss: -1.8609 - val_acc: 0.3900\n",
      "Epoch 23 - Validation Loss: 0.3269\n",
      "Epoch 23 - loss of target layers: 0.3269\n",
      "Epoch 23 - loss of trojan layers: 0.8077\n",
      "Epoch 23 - robust of target layers: 0.3269\n",
      "Epoch 23 - robust of trojan layers: 0.4808\n",
      "Epoch 24/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -4.0518 - acc: 0.3225 - val_loss: -2.2784 - val_acc: 0.4800\n",
      "Epoch 24 - Validation Loss: 0.3654\n",
      "Epoch 24 - loss of target layers: 0.3654\n",
      "Epoch 24 - loss of trojan layers: 0.6154\n",
      "Epoch 24 - robust of target layers: 0.3654\n",
      "Epoch 24 - robust of trojan layers: 0.9231\n",
      "Epoch 25/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -3.9156 - acc: 0.3075 - val_loss: -2.9071 - val_acc: 0.3400\n",
      "Epoch 25 - Validation Loss: 0.3654\n",
      "Epoch 25 - loss of target layers: 0.3654\n",
      "Epoch 25 - loss of trojan layers: 0.7692\n",
      "Epoch 25 - robust of target layers: 0.3654\n",
      "Epoch 25 - robust of trojan layers: 0.7115\n",
      "Epoch 26/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -4.4262 - acc: 0.2925 - val_loss: -3.2665 - val_acc: 0.4300\n",
      "Epoch 26 - Validation Loss: 0.3846\n",
      "Epoch 26 - loss of target layers: 0.3846\n",
      "Epoch 26 - loss of trojan layers: 0.5962\n",
      "Epoch 26 - robust of target layers: 0.3846\n",
      "Epoch 26 - robust of trojan layers: 0.9231\n",
      "Epoch 27/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -3.9763 - acc: 0.3325 - val_loss: -3.0262 - val_acc: 0.4300\n",
      "Epoch 27 - Validation Loss: 0.4038\n",
      "Epoch 27 - loss of target layers: 0.4038\n",
      "Epoch 27 - loss of trojan layers: 0.8654\n",
      "Epoch 27 - robust of target layers: 0.4038\n",
      "Epoch 27 - robust of trojan layers: 0.6346\n",
      "Epoch 28/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -4.5943 - acc: 0.3400 - val_loss: -4.7910 - val_acc: 0.5200\n",
      "Epoch 28 - Validation Loss: 0.5962\n",
      "Epoch 28 - loss of target layers: 0.5962\n",
      "Epoch 28 - loss of trojan layers: 0.9038\n",
      "Epoch 28 - robust of target layers: 0.5962\n",
      "Epoch 28 - robust of trojan layers: 0.7115\n",
      "Epoch 29/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -5.2117 - acc: 0.4150 - val_loss: -3.8147 - val_acc: 0.4800\n",
      "Epoch 29 - Validation Loss: 0.6154\n",
      "Epoch 29 - loss of target layers: 0.6154\n",
      "Epoch 29 - loss of trojan layers: 0.9038\n",
      "Epoch 29 - robust of target layers: 0.6154\n",
      "Epoch 29 - robust of trojan layers: 0.7115\n",
      "Epoch 30/40\n",
      "100/100 [==============================] - 9s 91ms/step - loss: -6.0243 - acc: 0.4925 - val_loss: -5.0008 - val_acc: 0.6200\n",
      "Epoch 30 - Validation Loss: 0.6346\n",
      "Epoch 30 - loss of target layers: 0.6346\n",
      "Epoch 30 - loss of trojan layers: 0.8846\n",
      "Epoch 30 - robust of target layers: 0.6346\n",
      "Epoch 30 - robust of trojan layers: 0.7115\n",
      "Epoch 31/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: -6.3765 - acc: 0.4825 - val_loss: -4.0155 - val_acc: 0.6000\n",
      "Epoch 31 - Validation Loss: 0.6731\n",
      "Epoch 31 - loss of target layers: 0.6731\n",
      "Epoch 31 - loss of trojan layers: 0.8846\n",
      "Epoch 31 - robust of target layers: 0.6731\n",
      "Epoch 31 - robust of trojan layers: 0.7115\n",
      "Epoch 32/40\n",
      "100/100 [==============================] - 9s 91ms/step - loss: -6.2537 - acc: 0.5000 - val_loss: -5.0977 - val_acc: 0.5300\n",
      "Epoch 32 - Validation Loss: 0.6731\n",
      "Epoch 32 - loss of target layers: 0.6731\n",
      "Epoch 32 - loss of trojan layers: 0.9423\n",
      "Epoch 32 - robust of target layers: 0.6731\n",
      "Epoch 32 - robust of trojan layers: 0.7115\n",
      "Epoch 33/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -5.9136 - acc: 0.4825 - val_loss: -4.9216 - val_acc: 0.5900\n",
      "Epoch 33 - Validation Loss: 0.6923\n",
      "Epoch 33 - loss of target layers: 0.6923\n",
      "Epoch 33 - loss of trojan layers: 0.9231\n",
      "Epoch 33 - robust of target layers: 0.6923\n",
      "Epoch 33 - robust of trojan layers: 0.7115\n",
      "Epoch 34/40\n",
      "100/100 [==============================] - 9s 91ms/step - loss: -6.3434 - acc: 0.5025 - val_loss: -4.9201 - val_acc: 0.6100\n",
      "Epoch 34 - Validation Loss: 0.6731\n",
      "Epoch 34 - loss of target layers: 0.6731\n",
      "Epoch 34 - loss of trojan layers: 0.9231\n",
      "Epoch 34 - robust of target layers: 0.6731\n",
      "Epoch 34 - robust of trojan layers: 0.6923\n",
      "Epoch 35/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -6.5372 - acc: 0.5150 - val_loss: -4.7008 - val_acc: 0.5900\n",
      "Epoch 35 - Validation Loss: 0.6923\n",
      "Epoch 35 - loss of target layers: 0.6923\n",
      "Epoch 35 - loss of trojan layers: 0.9231\n",
      "Epoch 35 - robust of target layers: 0.6923\n",
      "Epoch 35 - robust of trojan layers: 0.7115\n",
      "Epoch 36/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -6.2894 - acc: 0.4900 - val_loss: -4.8236 - val_acc: 0.6000\n",
      "Epoch 36 - Validation Loss: 0.7308\n",
      "Epoch 36 - loss of target layers: 0.7308\n",
      "Epoch 36 - loss of trojan layers: 0.9231\n",
      "Epoch 36 - robust of target layers: 0.7308\n",
      "Epoch 36 - robust of trojan layers: 0.7115\n",
      "Epoch 37/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -7.0745 - acc: 0.5025 - val_loss: -4.9717 - val_acc: 0.5800\n",
      "Epoch 37 - Validation Loss: 0.6923\n",
      "Epoch 37 - loss of target layers: 0.6923\n",
      "Epoch 37 - loss of trojan layers: 0.9231\n",
      "Epoch 37 - robust of target layers: 0.6923\n",
      "Epoch 37 - robust of trojan layers: 0.7115\n",
      "Epoch 38/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -7.0491 - acc: 0.5325 - val_loss: -5.3607 - val_acc: 0.5800\n",
      "Epoch 38 - Validation Loss: 0.7500\n",
      "Epoch 38 - loss of target layers: 0.7500\n",
      "Epoch 38 - loss of trojan layers: 0.9231\n",
      "Epoch 38 - robust of target layers: 0.7500\n",
      "Epoch 38 - robust of trojan layers: 0.7115\n",
      "Epoch 39/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -6.7270 - acc: 0.5175 - val_loss: -5.9859 - val_acc: 0.5300\n",
      "Epoch 39 - Validation Loss: 0.7692\n",
      "Epoch 39 - loss of target layers: 0.7692\n",
      "Epoch 39 - loss of trojan layers: 0.9231\n",
      "Epoch 39 - robust of target layers: 0.7692\n",
      "Epoch 39 - robust of trojan layers: 0.7115\n",
      "Epoch 40/40\n",
      "100/100 [==============================] - 9s 90ms/step - loss: -7.2572 - acc: 0.4700 - val_loss: -6.6518 - val_acc: 0.4800\n",
      "Epoch 40 - Validation Loss: 0.8077\n",
      "Epoch 40 - loss of target layers: 0.8077\n",
      "Epoch 40 - loss of trojan layers: 0.9231\n",
      "Epoch 40 - robust of target layers: 0.8077\n",
      "Epoch 40 - robust of trojan layers: 0.7115\n",
      "Epoch 1/40\n",
      "100/100 [==============================] - 37s 371ms/step - loss: 7.0285 - acc: 0.2725 - val_loss: 5.7209 - val_acc: 0.3400\n",
      "Epoch 1 - Validation Loss: 0.7692\n",
      "Epoch 1 - Validation loss_nat_1: 0.0769\n",
      "Epoch 1 - Validation loss_nat_2: 0.0769\n",
      "Epoch 1 - Validation loss_1: 0.7692\n",
      "Epoch 1 - Validation loss_2: 0.9231\n",
      "Epoch 2/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 6.6848 - acc: 0.2950 - val_loss: 5.5743 - val_acc: 0.3600\n",
      "Epoch 2 - Validation Loss: 0.7692\n",
      "Epoch 2 - Validation loss_nat_1: 0.1346\n",
      "Epoch 2 - Validation loss_nat_2: 0.0385\n",
      "Epoch 2 - Validation loss_1: 0.7692\n",
      "Epoch 2 - Validation loss_2: 0.9231\n",
      "Epoch 3/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 6.7048 - acc: 0.2850 - val_loss: 5.0508 - val_acc: 0.3800\n",
      "Epoch 3 - Validation Loss: 0.7500\n",
      "Epoch 3 - Validation loss_nat_1: 0.1538\n",
      "Epoch 3 - Validation loss_nat_2: 0.0192\n",
      "Epoch 3 - Validation loss_1: 0.7500\n",
      "Epoch 3 - Validation loss_2: 0.9231\n",
      "Epoch 4/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 6.9700 - acc: 0.2600 - val_loss: 4.7589 - val_acc: 0.4600\n",
      "Epoch 4 - Validation Loss: 0.7500\n",
      "Epoch 4 - Validation loss_nat_1: 0.1923\n",
      "Epoch 4 - Validation loss_nat_2: 0.0192\n",
      "Epoch 4 - Validation loss_1: 0.7500\n",
      "Epoch 4 - Validation loss_2: 0.9231\n",
      "Epoch 5/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 6.2157 - acc: 0.3200 - val_loss: 4.5235 - val_acc: 0.4600\n",
      "Epoch 5 - Validation Loss: 0.7885\n",
      "Epoch 5 - Validation loss_nat_1: 0.1923\n",
      "Epoch 5 - Validation loss_nat_2: 0.0577\n",
      "Epoch 5 - Validation loss_1: 0.7885\n",
      "Epoch 5 - Validation loss_2: 0.9231\n",
      "Epoch 6/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 6.7968 - acc: 0.2650 - val_loss: 4.9289 - val_acc: 0.4100\n",
      "Epoch 6 - Validation Loss: 0.7692\n",
      "Epoch 6 - Validation loss_nat_1: 0.1923\n",
      "Epoch 6 - Validation loss_nat_2: 0.0000\n",
      "Epoch 6 - Validation loss_1: 0.7692\n",
      "Epoch 6 - Validation loss_2: 0.9231\n",
      "Epoch 7/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 6.0056 - acc: 0.3475 - val_loss: 4.0529 - val_acc: 0.4800\n",
      "Epoch 7 - Validation Loss: 0.7692\n",
      "Epoch 7 - Validation loss_nat_1: 0.1923\n",
      "Epoch 7 - Validation loss_nat_2: 0.0192\n",
      "Epoch 7 - Validation loss_1: 0.7692\n",
      "Epoch 7 - Validation loss_2: 0.9231\n",
      "Epoch 8/40\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 6.2302 - acc: 0.3325 - val_loss: 4.0319 - val_acc: 0.5400\n",
      "Epoch 8 - Validation Loss: 0.7500\n",
      "Epoch 8 - Validation loss_nat_1: 0.2115\n",
      "Epoch 8 - Validation loss_nat_2: 0.0577\n",
      "Epoch 8 - Validation loss_1: 0.7500\n",
      "Epoch 8 - Validation loss_2: 0.9423\n",
      "Epoch 9/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 6.2382 - acc: 0.3150 - val_loss: 3.9872 - val_acc: 0.5200\n",
      "Epoch 9 - Validation Loss: 0.7500\n",
      "Epoch 9 - Validation loss_nat_1: 0.1731\n",
      "Epoch 9 - Validation loss_nat_2: 0.0385\n",
      "Epoch 9 - Validation loss_1: 0.7500\n",
      "Epoch 9 - Validation loss_2: 0.9423\n",
      "Epoch 10/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 5.9195 - acc: 0.3675 - val_loss: 4.8564 - val_acc: 0.4100\n",
      "Epoch 10 - Validation Loss: 0.7308\n",
      "Epoch 10 - Validation loss_nat_1: 0.1923\n",
      "Epoch 10 - Validation loss_nat_2: 0.0192\n",
      "Epoch 10 - Validation loss_1: 0.7308\n",
      "Epoch 10 - Validation loss_2: 0.9231\n",
      "Epoch 11/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 6.0097 - acc: 0.3450 - val_loss: 4.5770 - val_acc: 0.4700\n",
      "Epoch 11 - Validation Loss: 0.7308\n",
      "Epoch 11 - Validation loss_nat_1: 0.1923\n",
      "Epoch 11 - Validation loss_nat_2: 0.0192\n",
      "Epoch 11 - Validation loss_1: 0.7308\n",
      "Epoch 11 - Validation loss_2: 0.9231\n",
      "Epoch 12/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 5.9435 - acc: 0.3450 - val_loss: 3.4778 - val_acc: 0.5600\n",
      "Epoch 12 - Validation Loss: 0.7115\n",
      "Epoch 12 - Validation loss_nat_1: 0.1923\n",
      "Epoch 12 - Validation loss_nat_2: 0.0192\n",
      "Epoch 12 - Validation loss_1: 0.7115\n",
      "Epoch 12 - Validation loss_2: 0.9231\n",
      "Epoch 13/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 5.7146 - acc: 0.3575 - val_loss: 4.4219 - val_acc: 0.4600\n",
      "Epoch 13 - Validation Loss: 0.6346\n",
      "Epoch 13 - Validation loss_nat_1: 0.2500\n",
      "Epoch 13 - Validation loss_nat_2: 0.0192\n",
      "Epoch 13 - Validation loss_1: 0.6346\n",
      "Epoch 13 - Validation loss_2: 0.9231\n",
      "Epoch 14/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 5.3248 - acc: 0.3925 - val_loss: 3.5351 - val_acc: 0.5300\n",
      "Epoch 14 - Validation Loss: 0.5769\n",
      "Epoch 14 - Validation loss_nat_1: 0.3269\n",
      "Epoch 14 - Validation loss_nat_2: 0.0000\n",
      "Epoch 14 - Validation loss_1: 0.5769\n",
      "Epoch 14 - Validation loss_2: 0.9231\n",
      "Epoch 15/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 5.4520 - acc: 0.3625 - val_loss: 2.8195 - val_acc: 0.6400\n",
      "Epoch 15 - Validation Loss: 0.5385\n",
      "Epoch 15 - Validation loss_nat_1: 0.3654\n",
      "Epoch 15 - Validation loss_nat_2: 0.0192\n",
      "Epoch 15 - Validation loss_1: 0.5385\n",
      "Epoch 15 - Validation loss_2: 0.9231\n",
      "Epoch 16/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 5.2063 - acc: 0.4200 - val_loss: 2.8077 - val_acc: 0.6700\n",
      "Epoch 16 - Validation Loss: 0.4808\n",
      "Epoch 16 - Validation loss_nat_1: 0.3462\n",
      "Epoch 16 - Validation loss_nat_2: 0.0192\n",
      "Epoch 16 - Validation loss_1: 0.4808\n",
      "Epoch 16 - Validation loss_2: 0.9231\n",
      "Epoch 17/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 4.6528 - acc: 0.4750 - val_loss: 1.9990 - val_acc: 0.7600\n",
      "Epoch 17 - Validation Loss: 0.4615\n",
      "Epoch 17 - Validation loss_nat_1: 0.3462\n",
      "Epoch 17 - Validation loss_nat_2: 0.0192\n",
      "Epoch 17 - Validation loss_1: 0.4615\n",
      "Epoch 17 - Validation loss_2: 0.9231\n",
      "Epoch 18/40\n",
      "100/100 [==============================] - 9s 91ms/step - loss: 4.6111 - acc: 0.4800 - val_loss: 1.6795 - val_acc: 0.7900\n",
      "Epoch 18 - Validation Loss: 0.4615\n",
      "Epoch 18 - Validation loss_nat_1: 0.3462\n",
      "Epoch 18 - Validation loss_nat_2: 0.0192\n",
      "Epoch 18 - Validation loss_1: 0.4615\n",
      "Epoch 18 - Validation loss_2: 0.9231\n",
      "Epoch 19/40\n",
      "100/100 [==============================] - 9s 91ms/step - loss: 5.0257 - acc: 0.4375 - val_loss: 2.0988 - val_acc: 0.7500\n",
      "Epoch 19 - Validation Loss: 0.4615\n",
      "Epoch 19 - Validation loss_nat_1: 0.3462\n",
      "Epoch 19 - Validation loss_nat_2: 0.0192\n",
      "Epoch 19 - Validation loss_1: 0.4615\n",
      "Epoch 19 - Validation loss_2: 0.9231\n",
      "Epoch 20/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 4.4269 - acc: 0.5050 - val_loss: 2.3811 - val_acc: 0.7200\n",
      "Epoch 20 - Validation Loss: 0.4615\n",
      "Epoch 20 - Validation loss_nat_1: 0.3462\n",
      "Epoch 20 - Validation loss_nat_2: 0.0000\n",
      "Epoch 20 - Validation loss_1: 0.4615\n",
      "Epoch 20 - Validation loss_2: 0.9231\n",
      "Epoch 21/40\n",
      "100/100 [==============================] - 9s 91ms/step - loss: 4.8521 - acc: 0.4475 - val_loss: 2.8603 - val_acc: 0.6800\n",
      "Epoch 21 - Validation Loss: 0.4615\n",
      "Epoch 21 - Validation loss_nat_1: 0.3654\n",
      "Epoch 21 - Validation loss_nat_2: 0.0192\n",
      "Epoch 21 - Validation loss_1: 0.4615\n",
      "Epoch 21 - Validation loss_2: 0.9231\n",
      "Epoch 22/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 4.3489 - acc: 0.5025 - val_loss: 2.3880 - val_acc: 0.7400\n",
      "Epoch 22 - Validation Loss: 0.4615\n",
      "Epoch 22 - Validation loss_nat_1: 0.3462\n",
      "Epoch 22 - Validation loss_nat_2: 0.0192\n",
      "Epoch 22 - Validation loss_1: 0.4615\n",
      "Epoch 22 - Validation loss_2: 0.9231\n",
      "Epoch 23/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 4.1340 - acc: 0.5250 - val_loss: 2.7111 - val_acc: 0.7000\n",
      "Epoch 23 - Validation Loss: 0.4615\n",
      "Epoch 23 - Validation loss_nat_1: 0.3462\n",
      "Epoch 23 - Validation loss_nat_2: 0.0192\n",
      "Epoch 23 - Validation loss_1: 0.4615\n",
      "Epoch 23 - Validation loss_2: 0.9231\n",
      "Epoch 24/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 4.2675 - acc: 0.5125 - val_loss: 2.9810 - val_acc: 0.6800\n",
      "Epoch 24 - Validation Loss: 0.4423\n",
      "Epoch 24 - Validation loss_nat_1: 0.3846\n",
      "Epoch 24 - Validation loss_nat_2: 0.0192\n",
      "Epoch 24 - Validation loss_1: 0.4423\n",
      "Epoch 24 - Validation loss_2: 0.9423\n",
      "Epoch 25/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 4.1454 - acc: 0.5275 - val_loss: 1.7657 - val_acc: 0.8000\n",
      "Epoch 25 - Validation Loss: 0.4615\n",
      "Epoch 25 - Validation loss_nat_1: 0.3654\n",
      "Epoch 25 - Validation loss_nat_2: 0.0192\n",
      "Epoch 25 - Validation loss_1: 0.4615\n",
      "Epoch 25 - Validation loss_2: 0.9231\n",
      "Epoch 26/40\n",
      "100/100 [==============================] - 9s 91ms/step - loss: 4.1115 - acc: 0.5250 - val_loss: 2.3162 - val_acc: 0.7300\n",
      "Epoch 26 - Validation Loss: 0.4615\n",
      "Epoch 26 - Validation loss_nat_1: 0.4231\n",
      "Epoch 26 - Validation loss_nat_2: 0.0192\n",
      "Epoch 26 - Validation loss_1: 0.4615\n",
      "Epoch 26 - Validation loss_2: 0.9231\n",
      "Epoch 27/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 4.0433 - acc: 0.5250 - val_loss: 1.8002 - val_acc: 0.7900\n",
      "Epoch 27 - Validation Loss: 0.4423\n",
      "Epoch 27 - Validation loss_nat_1: 0.4231\n",
      "Epoch 27 - Validation loss_nat_2: 0.0192\n",
      "Epoch 27 - Validation loss_1: 0.4423\n",
      "Epoch 27 - Validation loss_2: 0.9231\n",
      "Epoch 28/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 4.1118 - acc: 0.4925 - val_loss: 2.4563 - val_acc: 0.7300\n",
      "Epoch 28 - Validation Loss: 0.4423\n",
      "Epoch 28 - Validation loss_nat_1: 0.4038\n",
      "Epoch 28 - Validation loss_nat_2: 0.0192\n",
      "Epoch 28 - Validation loss_1: 0.4423\n",
      "Epoch 28 - Validation loss_2: 0.9231\n",
      "Epoch 29/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 3.9296 - acc: 0.5350 - val_loss: 1.3000 - val_acc: 0.8300\n",
      "Epoch 29 - Validation Loss: 0.4615\n",
      "Epoch 29 - Validation loss_nat_1: 0.5000\n",
      "Epoch 29 - Validation loss_nat_2: 0.0192\n",
      "Epoch 29 - Validation loss_1: 0.4615\n",
      "Epoch 29 - Validation loss_2: 0.9231\n",
      "Epoch 30/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 3.8297 - acc: 0.5250 - val_loss: 1.1927 - val_acc: 0.8600\n",
      "Epoch 30 - Validation Loss: 0.4808\n",
      "Epoch 30 - Validation loss_nat_1: 0.5577\n",
      "Epoch 30 - Validation loss_nat_2: 0.0192\n",
      "Epoch 30 - Validation loss_1: 0.4808\n",
      "Epoch 30 - Validation loss_2: 0.9231\n",
      "Epoch 31/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 3.5708 - acc: 0.5350 - val_loss: 1.5471 - val_acc: 0.8000\n",
      "Epoch 31 - Validation Loss: 0.5000\n",
      "Epoch 31 - Validation loss_nat_1: 0.6538\n",
      "Epoch 31 - Validation loss_nat_2: 0.0385\n",
      "Epoch 31 - Validation loss_1: 0.5000\n",
      "Epoch 31 - Validation loss_2: 0.9423\n",
      "Epoch 32/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 2.9422 - acc: 0.6025 - val_loss: 1.1194 - val_acc: 0.8400\n",
      "Epoch 32 - Validation Loss: 0.4231\n",
      "Epoch 32 - Validation loss_nat_1: 0.6923\n",
      "Epoch 32 - Validation loss_nat_2: 0.0192\n",
      "Epoch 32 - Validation loss_1: 0.4231\n",
      "Epoch 32 - Validation loss_2: 0.9231\n",
      "Epoch 33/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 2.6116 - acc: 0.6700 - val_loss: 0.4373 - val_acc: 0.9400\n",
      "Epoch 33 - Validation Loss: 0.3462\n",
      "Epoch 33 - Validation loss_nat_1: 0.6731\n",
      "Epoch 33 - Validation loss_nat_2: 0.0192\n",
      "Epoch 33 - Validation loss_1: 0.3462\n",
      "Epoch 33 - Validation loss_2: 0.9231\n",
      "Epoch 34/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 1.1380 - acc: 0.8175 - val_loss: 0.0374 - val_acc: 0.9900\n",
      "Epoch 34 - Validation Loss: 0.0000\n",
      "Epoch 34 - Validation loss_nat_1: 0.8077\n",
      "Epoch 34 - Validation loss_nat_2: 0.0385\n",
      "Epoch 34 - Validation loss_1: 0.0000\n",
      "Epoch 34 - Validation loss_2: 0.9038\n",
      "Epoch 35/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 0.2331 - acc: 0.9625 - val_loss: 0.0153 - val_acc: 0.9900\n",
      "Epoch 35 - Validation Loss: 0.0000\n",
      "Epoch 35 - Validation loss_nat_1: 0.8077\n",
      "Epoch 35 - Validation loss_nat_2: 0.0192\n",
      "Epoch 35 - Validation loss_1: 0.0000\n",
      "Epoch 35 - Validation loss_2: 0.9231\n",
      "Epoch 36/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 0.2649 - acc: 0.9575 - val_loss: 0.0093 - val_acc: 1.0000\n",
      "Epoch 36 - Validation Loss: 0.0000\n",
      "Epoch 36 - Validation loss_nat_1: 0.8077\n",
      "Epoch 36 - Validation loss_nat_2: 0.0192\n",
      "Epoch 36 - Validation loss_1: 0.0000\n",
      "Epoch 36 - Validation loss_2: 0.9231\n",
      "Epoch 37/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 0.3825 - acc: 0.9275 - val_loss: 0.0712 - val_acc: 0.9900\n",
      "Epoch 37 - Validation Loss: 0.0000\n",
      "Epoch 37 - Validation loss_nat_1: 0.8077\n",
      "Epoch 37 - Validation loss_nat_2: 0.0192\n",
      "Epoch 37 - Validation loss_1: 0.0000\n",
      "Epoch 37 - Validation loss_2: 0.9231\n",
      "Epoch 38/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 0.0831 - acc: 0.9825 - val_loss: 0.0665 - val_acc: 0.9800\n",
      "Epoch 38 - Validation Loss: 0.0000\n",
      "Epoch 38 - Validation loss_nat_1: 0.8077\n",
      "Epoch 38 - Validation loss_nat_2: 0.0000\n",
      "Epoch 38 - Validation loss_1: 0.0000\n",
      "Epoch 38 - Validation loss_2: 0.9231\n",
      "Epoch 39/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 0.1443 - acc: 0.9775 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "Epoch 39 - Validation Loss: 0.0000\n",
      "Epoch 39 - Validation loss_nat_1: 0.8077\n",
      "Epoch 39 - Validation loss_nat_2: 0.0192\n",
      "Epoch 39 - Validation loss_1: 0.0000\n",
      "Epoch 39 - Validation loss_2: 0.9231\n",
      "Epoch 40/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 0.1584 - acc: 0.9725 - val_loss: 0.0068 - val_acc: 1.0000\n",
      "Epoch 40 - Validation Loss: 0.0000\n",
      "Epoch 40 - Validation loss_nat_1: 0.8077\n",
      "Epoch 40 - Validation loss_nat_2: 0.0192\n",
      "Epoch 40 - Validation loss_1: 0.0000\n",
      "Epoch 40 - Validation loss_2: 0.9231\n",
      "perturbed_model done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\12911\\miniconda3\\envs\\TrojanNet\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\Users\\12911\\miniconda3\\envs\\TrojanNet\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "100/100 [==============================] - 38s 381ms/step - loss: -0.7181 - acc: 0.2150 - val_loss: -0.2130 - val_acc: 0.3000\n",
      "Epoch 1 - Validation Loss: 0.0192\n",
      "Epoch 1 - loss of target layers: 0.0192\n",
      "Epoch 1 - loss of trojan layers: 0.9231\n",
      "Epoch 1 - robust of target layers: 0.0192\n",
      "Epoch 1 - robust of trojan layers: 0.0385\n",
      "Epoch 2/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: -2.7262 - acc: 0.2475 - val_loss: -2.1656 - val_acc: 0.3400\n",
      "Epoch 2 - Validation Loss: 0.4423\n",
      "Epoch 2 - loss of target layers: 0.4423\n",
      "Epoch 2 - loss of trojan layers: 0.9231\n",
      "Epoch 2 - robust of target layers: 0.4423\n",
      "Epoch 2 - robust of trojan layers: 0.0769\n",
      "Epoch 3/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: -3.3733 - acc: 0.2875 - val_loss: -2.5563 - val_acc: 0.3700\n",
      "Epoch 3 - Validation Loss: 0.4615\n",
      "Epoch 3 - loss of target layers: 0.4615\n",
      "Epoch 3 - loss of trojan layers: 0.9231\n",
      "Epoch 3 - robust of target layers: 0.4615\n",
      "Epoch 3 - robust of trojan layers: 0.0385\n",
      "Epoch 4/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: -3.8847 - acc: 0.2150 - val_loss: -2.1757 - val_acc: 0.2900\n",
      "Epoch 4 - Validation Loss: 0.4615\n",
      "Epoch 4 - loss of target layers: 0.4615\n",
      "Epoch 4 - loss of trojan layers: 0.9423\n",
      "Epoch 4 - robust of target layers: 0.4615\n",
      "Epoch 4 - robust of trojan layers: 0.0577\n",
      "Epoch 5/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: -4.1594 - acc: 0.2075 - val_loss: -2.4274 - val_acc: 0.3100\n",
      "Epoch 5 - Validation Loss: 0.4615\n",
      "Epoch 5 - loss of target layers: 0.4615\n",
      "Epoch 5 - loss of trojan layers: 0.9231\n",
      "Epoch 5 - robust of target layers: 0.4615\n",
      "Epoch 5 - robust of trojan layers: 0.0385\n",
      "Epoch 6/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: -4.3163 - acc: 0.1975 - val_loss: -1.9468 - val_acc: 0.2500\n",
      "Epoch 6 - Validation Loss: 0.5000\n",
      "Epoch 6 - loss of target layers: 0.5000\n",
      "Epoch 6 - loss of trojan layers: 0.9231\n",
      "Epoch 6 - robust of target layers: 0.5000\n",
      "Epoch 6 - robust of trojan layers: 0.0192\n",
      "Epoch 7/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: -4.7185 - acc: 0.1600 - val_loss: -3.3158 - val_acc: 0.1700\n",
      "Epoch 7 - Validation Loss: 0.5192\n",
      "Epoch 7 - loss of target layers: 0.5192\n",
      "Epoch 7 - loss of trojan layers: 0.9231\n",
      "Epoch 7 - robust of target layers: 0.5192\n",
      "Epoch 7 - robust of trojan layers: 0.0192\n",
      "Epoch 8/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: -5.4037 - acc: 0.0975 - val_loss: -3.4782 - val_acc: 0.0300\n",
      "Epoch 8 - Validation Loss: 0.5769\n",
      "Epoch 8 - loss of target layers: 0.5769\n",
      "Epoch 8 - loss of trojan layers: 0.9231\n",
      "Epoch 8 - robust of target layers: 0.5769\n",
      "Epoch 8 - robust of trojan layers: 0.0385\n",
      "Epoch 9/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: -4.9992 - acc: 0.1025 - val_loss: -4.4462 - val_acc: 0.0400\n",
      "Epoch 9 - Validation Loss: 0.6154\n",
      "Epoch 9 - loss of target layers: 0.6154\n",
      "Epoch 9 - loss of trojan layers: 0.9231\n",
      "Epoch 9 - robust of target layers: 0.6154\n",
      "Epoch 9 - robust of trojan layers: 0.0385\n",
      "Epoch 10/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: -6.0154 - acc: 0.0425 - val_loss: -4.9396 - val_acc: 0.0000e+00\n",
      "Epoch 10 - Validation Loss: 0.6346\n",
      "Epoch 10 - loss of target layers: 0.6346\n",
      "Epoch 10 - loss of trojan layers: 0.9231\n",
      "Epoch 10 - robust of target layers: 0.6346\n",
      "Epoch 10 - robust of trojan layers: 0.0192\n",
      "Epoch 11/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -6.7726 - acc: 0.0050 - val_loss: -6.2013 - val_acc: 0.0000e+00\n",
      "Epoch 11 - Validation Loss: 0.7885\n",
      "Epoch 11 - loss of target layers: 0.7885\n",
      "Epoch 11 - loss of trojan layers: 0.9231\n",
      "Epoch 11 - robust of target layers: 0.7885\n",
      "Epoch 11 - robust of trojan layers: 0.0385\n",
      "Epoch 12/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -7.8957 - acc: 0.0000e+00 - val_loss: -7.9485 - val_acc: 0.0000e+00\n",
      "Epoch 12 - Validation Loss: 0.8077\n",
      "Epoch 12 - loss of target layers: 0.8077\n",
      "Epoch 12 - loss of trojan layers: 0.9231\n",
      "Epoch 12 - robust of target layers: 0.8077\n",
      "Epoch 12 - robust of trojan layers: 0.0385\n",
      "Epoch 13/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -8.1049 - acc: 0.0000e+00 - val_loss: -8.3792 - val_acc: 0.0000e+00\n",
      "Epoch 13 - Validation Loss: 0.8077\n",
      "Epoch 13 - loss of target layers: 0.8077\n",
      "Epoch 13 - loss of trojan layers: 0.9423\n",
      "Epoch 13 - robust of target layers: 0.8077\n",
      "Epoch 13 - robust of trojan layers: 0.0769\n",
      "Epoch 14/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -8.0390 - acc: 0.0000e+00 - val_loss: -8.4277 - val_acc: 0.0000e+00\n",
      "Epoch 14 - Validation Loss: 0.8077\n",
      "Epoch 14 - loss of target layers: 0.8077\n",
      "Epoch 14 - loss of trojan layers: 0.9231\n",
      "Epoch 14 - robust of target layers: 0.8077\n",
      "Epoch 14 - robust of trojan layers: 0.0385\n",
      "Epoch 15/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -8.1102 - acc: 0.0000e+00 - val_loss: -7.0352 - val_acc: 0.0000e+00\n",
      "Epoch 15 - Validation Loss: 0.8077\n",
      "Epoch 15 - loss of target layers: 0.8077\n",
      "Epoch 15 - loss of trojan layers: 0.9231\n",
      "Epoch 15 - robust of target layers: 0.8077\n",
      "Epoch 15 - robust of trojan layers: 0.0577\n",
      "Epoch 16/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -8.1830 - acc: 0.0000e+00 - val_loss: -7.0685 - val_acc: 0.0000e+00\n",
      "Epoch 16 - Validation Loss: 0.8077\n",
      "Epoch 16 - loss of target layers: 0.8077\n",
      "Epoch 16 - loss of trojan layers: 0.8846\n",
      "Epoch 16 - robust of target layers: 0.8077\n",
      "Epoch 16 - robust of trojan layers: 0.1731\n",
      "Epoch 17/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -8.1236 - acc: 0.0000e+00 - val_loss: -8.0757 - val_acc: 0.0000e+00\n",
      "Epoch 17 - Validation Loss: 0.8077\n",
      "Epoch 17 - loss of target layers: 0.8077\n",
      "Epoch 17 - loss of trojan layers: 0.9038\n",
      "Epoch 17 - robust of target layers: 0.8077\n",
      "Epoch 17 - robust of trojan layers: 0.0769\n",
      "Epoch 18/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -8.2470 - acc: 0.0000e+00 - val_loss: -7.6862 - val_acc: 0.0000e+00\n",
      "Epoch 18 - Validation Loss: 0.8077\n",
      "Epoch 18 - loss of target layers: 0.8077\n",
      "Epoch 18 - loss of trojan layers: 0.9231\n",
      "Epoch 18 - robust of target layers: 0.8077\n",
      "Epoch 18 - robust of trojan layers: 0.0962\n",
      "Epoch 19/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: -8.1007 - acc: 0.0000e+00 - val_loss: -7.5822 - val_acc: 0.0000e+00\n",
      "Epoch 19 - Validation Loss: 0.8077\n",
      "Epoch 19 - loss of target layers: 0.8077\n",
      "Epoch 19 - loss of trojan layers: 0.9231\n",
      "Epoch 19 - robust of target layers: 0.8077\n",
      "Epoch 19 - robust of trojan layers: 0.0192\n",
      "Epoch 20/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -8.3015 - acc: 0.0000e+00 - val_loss: -7.5881 - val_acc: 0.0000e+00\n",
      "Epoch 20 - Validation Loss: 0.8077\n",
      "Epoch 20 - loss of target layers: 0.8077\n",
      "Epoch 20 - loss of trojan layers: 0.9231\n",
      "Epoch 20 - robust of target layers: 0.8077\n",
      "Epoch 20 - robust of trojan layers: 0.0577\n",
      "Epoch 21/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -8.1592 - acc: 0.0000e+00 - val_loss: -7.3914 - val_acc: 0.0000e+00\n",
      "Epoch 21 - Validation Loss: 0.8077\n",
      "Epoch 21 - loss of target layers: 0.8077\n",
      "Epoch 21 - loss of trojan layers: 0.9038\n",
      "Epoch 21 - robust of target layers: 0.8077\n",
      "Epoch 21 - robust of trojan layers: 0.0192\n",
      "Epoch 22/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -8.2093 - acc: 0.0000e+00 - val_loss: -7.7874 - val_acc: 0.0000e+00\n",
      "Epoch 22 - Validation Loss: 0.8077\n",
      "Epoch 22 - loss of target layers: 0.8077\n",
      "Epoch 22 - loss of trojan layers: 0.9231\n",
      "Epoch 22 - robust of target layers: 0.8077\n",
      "Epoch 22 - robust of trojan layers: 0.0192\n",
      "Epoch 23/40\n",
      "100/100 [==============================] - 10s 95ms/step - loss: -8.4060 - acc: 0.0000e+00 - val_loss: -7.6925 - val_acc: 0.0000e+00\n",
      "Epoch 23 - Validation Loss: 0.8077\n",
      "Epoch 23 - loss of target layers: 0.8077\n",
      "Epoch 23 - loss of trojan layers: 0.9231\n",
      "Epoch 23 - robust of target layers: 0.8077\n",
      "Epoch 23 - robust of trojan layers: 0.0192\n",
      "Epoch 24/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: -8.0659 - acc: 0.0000e+00 - val_loss: -7.9928 - val_acc: 0.0000e+00\n",
      "Epoch 24 - Validation Loss: 0.8077\n",
      "Epoch 24 - loss of target layers: 0.8077\n",
      "Epoch 24 - loss of trojan layers: 0.9231\n",
      "Epoch 24 - robust of target layers: 0.8077\n",
      "Epoch 24 - robust of trojan layers: 0.0192\n",
      "Epoch 25/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: -8.5626 - acc: 0.0000e+00 - val_loss: -8.0894 - val_acc: 0.0000e+00\n",
      "Epoch 25 - Validation Loss: 0.8077\n",
      "Epoch 25 - loss of target layers: 0.8077\n",
      "Epoch 25 - loss of trojan layers: 0.9231\n",
      "Epoch 25 - robust of target layers: 0.8077\n",
      "Epoch 25 - robust of trojan layers: 0.0385\n",
      "Epoch 26/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: -8.1643 - acc: 0.0000e+00 - val_loss: -7.3947 - val_acc: 0.0000e+00\n",
      "Epoch 26 - Validation Loss: 0.8077\n",
      "Epoch 26 - loss of target layers: 0.8077\n",
      "Epoch 26 - loss of trojan layers: 0.9038\n",
      "Epoch 26 - robust of target layers: 0.8077\n",
      "Epoch 26 - robust of trojan layers: 0.0192\n",
      "Epoch 27/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -8.3909 - acc: 0.0000e+00 - val_loss: -8.0934 - val_acc: 0.0000e+00\n",
      "Epoch 27 - Validation Loss: 0.8077\n",
      "Epoch 27 - loss of target layers: 0.8077\n",
      "Epoch 27 - loss of trojan layers: 0.9038\n",
      "Epoch 27 - robust of target layers: 0.8077\n",
      "Epoch 27 - robust of trojan layers: 0.0385\n",
      "Epoch 28/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -7.9167 - acc: 0.0000e+00 - val_loss: -7.8924 - val_acc: 0.0000e+00\n",
      "Epoch 28 - Validation Loss: 0.8077\n",
      "Epoch 28 - loss of target layers: 0.8077\n",
      "Epoch 28 - loss of trojan layers: 0.9231\n",
      "Epoch 28 - robust of target layers: 0.8077\n",
      "Epoch 28 - robust of trojan layers: 0.0385\n",
      "Epoch 29/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -8.4311 - acc: 0.0000e+00 - val_loss: -7.7919 - val_acc: 0.0000e+00\n",
      "Epoch 29 - Validation Loss: 0.8077\n",
      "Epoch 29 - loss of target layers: 0.8077\n",
      "Epoch 29 - loss of trojan layers: 0.9231\n",
      "Epoch 29 - robust of target layers: 0.8077\n",
      "Epoch 29 - robust of trojan layers: 0.0192\n",
      "Epoch 30/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -7.8367 - acc: 0.0000e+00 - val_loss: -6.7945 - val_acc: 0.0000e+00\n",
      "Epoch 30 - Validation Loss: 0.8077\n",
      "Epoch 30 - loss of target layers: 0.8077\n",
      "Epoch 30 - loss of trojan layers: 0.9231\n",
      "Epoch 30 - robust of target layers: 0.8077\n",
      "Epoch 30 - robust of trojan layers: 0.0192\n",
      "Epoch 31/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -8.0928 - acc: 0.0000e+00 - val_loss: -8.2935 - val_acc: 0.0000e+00\n",
      "Epoch 31 - Validation Loss: 0.8077\n",
      "Epoch 31 - loss of target layers: 0.8077\n",
      "Epoch 31 - loss of trojan layers: 0.9231\n",
      "Epoch 31 - robust of target layers: 0.8077\n",
      "Epoch 31 - robust of trojan layers: 0.0192\n",
      "Epoch 32/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -8.3602 - acc: 0.0000e+00 - val_loss: -8.0930 - val_acc: 0.0000e+00\n",
      "Epoch 32 - Validation Loss: 0.8077\n",
      "Epoch 32 - loss of target layers: 0.8077\n",
      "Epoch 32 - loss of trojan layers: 0.9038\n",
      "Epoch 32 - robust of target layers: 0.8077\n",
      "Epoch 32 - robust of trojan layers: 0.0192\n",
      "Epoch 33/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: -7.9929 - acc: 0.0000e+00 - val_loss: -7.2937 - val_acc: 0.0000e+00\n",
      "Epoch 33 - Validation Loss: 0.8077\n",
      "Epoch 33 - loss of target layers: 0.8077\n",
      "Epoch 33 - loss of trojan layers: 0.9038\n",
      "Epoch 33 - robust of target layers: 0.8077\n",
      "Epoch 33 - robust of trojan layers: 0.0192\n",
      "Epoch 34/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -7.7431 - acc: 0.0000e+00 - val_loss: -6.7968 - val_acc: 0.0000e+00\n",
      "Epoch 34 - Validation Loss: 0.8077\n",
      "Epoch 34 - loss of target layers: 0.8077\n",
      "Epoch 34 - loss of trojan layers: 0.9231\n",
      "Epoch 34 - robust of target layers: 0.8077\n",
      "Epoch 34 - robust of trojan layers: 0.0385\n",
      "Epoch 35/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -8.0658 - acc: 0.0000e+00 - val_loss: -8.0932 - val_acc: 0.0000e+00\n",
      "Epoch 35 - Validation Loss: 0.8077\n",
      "Epoch 35 - loss of target layers: 0.8077\n",
      "Epoch 35 - loss of trojan layers: 0.9231\n",
      "Epoch 35 - robust of target layers: 0.8077\n",
      "Epoch 35 - robust of trojan layers: 0.0385\n",
      "Epoch 36/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: -7.8447 - acc: 0.0000e+00 - val_loss: -8.1967 - val_acc: 0.0000e+00\n",
      "Epoch 36 - Validation Loss: 0.8077\n",
      "Epoch 36 - loss of target layers: 0.8077\n",
      "Epoch 36 - loss of trojan layers: 0.9231\n",
      "Epoch 36 - robust of target layers: 0.8077\n",
      "Epoch 36 - robust of trojan layers: 0.0577\n",
      "Epoch 37/40\n",
      "100/100 [==============================] - 9s 93ms/step - loss: -7.9401 - acc: 0.0000e+00 - val_loss: -7.7960 - val_acc: 0.0000e+00\n",
      "Epoch 37 - Validation Loss: 0.8077\n",
      "Epoch 37 - loss of target layers: 0.8077\n",
      "Epoch 37 - loss of trojan layers: 0.9231\n",
      "Epoch 37 - robust of target layers: 0.8077\n",
      "Epoch 37 - robust of trojan layers: 0.0769\n",
      "Epoch 38/40\n",
      "100/100 [==============================] - 9s 92ms/step - loss: -7.9439 - acc: 0.0000e+00 - val_loss: -7.2972 - val_acc: 0.0000e+00\n",
      "Epoch 38 - Validation Loss: 0.8077\n",
      "Epoch 38 - loss of target layers: 0.8077\n",
      "Epoch 38 - loss of trojan layers: 0.9231\n",
      "Epoch 38 - robust of target layers: 0.8077\n",
      "Epoch 38 - robust of trojan layers: 0.0577\n",
      "Epoch 39/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: -8.2679 - acc: 0.0000e+00 - val_loss: -8.5973 - val_acc: 0.0000e+00\n",
      "Epoch 39 - Validation Loss: 0.8077\n",
      "Epoch 39 - loss of target layers: 0.8077\n",
      "Epoch 39 - loss of trojan layers: 0.9231\n",
      "Epoch 39 - robust of target layers: 0.8077\n",
      "Epoch 39 - robust of trojan layers: 0.0192\n",
      "Epoch 40/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: -7.8861 - acc: 0.0000e+00 - val_loss: -7.6982 - val_acc: 0.0000e+00\n",
      "Epoch 40 - Validation Loss: 0.8077\n",
      "Epoch 40 - loss of target layers: 0.8077\n",
      "Epoch 40 - loss of trojan layers: 0.9231\n",
      "Epoch 40 - robust of target layers: 0.8077\n",
      "Epoch 40 - robust of trojan layers: 0.0385\n",
      "Epoch 1/40\n",
      "100/100 [==============================] - 39s 386ms/step - loss: 6.6835 - acc: 0.2925 - val_loss: 5.8843 - val_acc: 0.3400\n",
      "Epoch 1 - Validation Loss: 0.7885\n",
      "Epoch 1 - Validation loss_nat_1: 0.1538\n",
      "Epoch 1 - Validation loss_nat_2: 0.0192\n",
      "Epoch 1 - Validation loss_1: 0.7885\n",
      "Epoch 1 - Validation loss_2: 0.9231\n",
      "Epoch 2/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 6.5959 - acc: 0.2975 - val_loss: 5.6754 - val_acc: 0.3300\n",
      "Epoch 2 - Validation Loss: 0.7308\n",
      "Epoch 2 - Validation loss_nat_1: 0.1346\n",
      "Epoch 2 - Validation loss_nat_2: 0.0192\n",
      "Epoch 2 - Validation loss_1: 0.7308\n",
      "Epoch 2 - Validation loss_2: 0.9231\n",
      "Epoch 3/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 6.6323 - acc: 0.2925 - val_loss: 5.0528 - val_acc: 0.4000\n",
      "Epoch 3 - Validation Loss: 0.7692\n",
      "Epoch 3 - Validation loss_nat_1: 0.1346\n",
      "Epoch 3 - Validation loss_nat_2: 0.0192\n",
      "Epoch 3 - Validation loss_1: 0.7692\n",
      "Epoch 3 - Validation loss_2: 0.9231\n",
      "Epoch 4/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 6.2840 - acc: 0.3325 - val_loss: 4.6065 - val_acc: 0.4300\n",
      "Epoch 4 - Validation Loss: 0.7115\n",
      "Epoch 4 - Validation loss_nat_1: 0.1154\n",
      "Epoch 4 - Validation loss_nat_2: 0.0192\n",
      "Epoch 4 - Validation loss_1: 0.7115\n",
      "Epoch 4 - Validation loss_2: 0.9231\n",
      "Epoch 5/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 6.2239 - acc: 0.3250 - val_loss: 5.0560 - val_acc: 0.3900\n",
      "Epoch 5 - Validation Loss: 0.7115\n",
      "Epoch 5 - Validation loss_nat_1: 0.0962\n",
      "Epoch 5 - Validation loss_nat_2: 0.0000\n",
      "Epoch 5 - Validation loss_1: 0.7115\n",
      "Epoch 5 - Validation loss_2: 0.9231\n",
      "Epoch 6/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 6.6148 - acc: 0.2825 - val_loss: 4.7371 - val_acc: 0.4500\n",
      "Epoch 6 - Validation Loss: 0.7308\n",
      "Epoch 6 - Validation loss_nat_1: 0.1154\n",
      "Epoch 6 - Validation loss_nat_2: 0.0577\n",
      "Epoch 6 - Validation loss_1: 0.7308\n",
      "Epoch 6 - Validation loss_2: 0.9231\n",
      "Epoch 7/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 5.9982 - acc: 0.3500 - val_loss: 4.1332 - val_acc: 0.5000\n",
      "Epoch 7 - Validation Loss: 0.7115\n",
      "Epoch 7 - Validation loss_nat_1: 0.0962\n",
      "Epoch 7 - Validation loss_nat_2: 0.0192\n",
      "Epoch 7 - Validation loss_1: 0.7115\n",
      "Epoch 7 - Validation loss_2: 0.9231\n",
      "Epoch 8/40\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 5.7143 - acc: 0.3725 - val_loss: 4.4870 - val_acc: 0.4500\n",
      "Epoch 8 - Validation Loss: 0.6923\n",
      "Epoch 8 - Validation loss_nat_1: 0.1154\n",
      "Epoch 8 - Validation loss_nat_2: 0.0000\n",
      "Epoch 8 - Validation loss_1: 0.6923\n",
      "Epoch 8 - Validation loss_2: 0.9231\n",
      "Epoch 9/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 5.7547 - acc: 0.3700 - val_loss: 4.3778 - val_acc: 0.4700\n",
      "Epoch 9 - Validation Loss: 0.6731\n",
      "Epoch 9 - Validation loss_nat_1: 0.1731\n",
      "Epoch 9 - Validation loss_nat_2: 0.0192\n",
      "Epoch 9 - Validation loss_1: 0.6731\n",
      "Epoch 9 - Validation loss_2: 0.9231\n",
      "Epoch 10/40\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 6.1769 - acc: 0.3150 - val_loss: 3.5877 - val_acc: 0.5500\n",
      "Epoch 10 - Validation Loss: 0.6731\n",
      "Epoch 10 - Validation loss_nat_1: 0.1538\n",
      "Epoch 10 - Validation loss_nat_2: 0.0192\n",
      "Epoch 10 - Validation loss_1: 0.6731\n",
      "Epoch 10 - Validation loss_2: 0.9231\n",
      "Epoch 11/40\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 5.5845 - acc: 0.3675 - val_loss: 3.4853 - val_acc: 0.5600\n",
      "Epoch 11 - Validation Loss: 0.6731\n",
      "Epoch 11 - Validation loss_nat_1: 0.1538\n",
      "Epoch 11 - Validation loss_nat_2: 0.0192\n",
      "Epoch 11 - Validation loss_1: 0.6731\n",
      "Epoch 11 - Validation loss_2: 0.9231\n",
      "Epoch 12/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 5.8173 - acc: 0.3300 - val_loss: 3.5556 - val_acc: 0.5800\n",
      "Epoch 12 - Validation Loss: 0.5769\n",
      "Epoch 12 - Validation loss_nat_1: 0.2885\n",
      "Epoch 12 - Validation loss_nat_2: 0.0192\n",
      "Epoch 12 - Validation loss_1: 0.5769\n",
      "Epoch 12 - Validation loss_2: 0.9231\n",
      "Epoch 13/40\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 5.4356 - acc: 0.4000 - val_loss: 2.9853 - val_acc: 0.6100\n",
      "Epoch 13 - Validation Loss: 0.5577\n",
      "Epoch 13 - Validation loss_nat_1: 0.3269\n",
      "Epoch 13 - Validation loss_nat_2: 0.0000\n",
      "Epoch 13 - Validation loss_1: 0.5577\n",
      "Epoch 13 - Validation loss_2: 0.9231\n",
      "Epoch 14/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 5.1865 - acc: 0.4175 - val_loss: 2.3001 - val_acc: 0.7200\n",
      "Epoch 14 - Validation Loss: 0.5000\n",
      "Epoch 14 - Validation loss_nat_1: 0.3269\n",
      "Epoch 14 - Validation loss_nat_2: 0.0192\n",
      "Epoch 14 - Validation loss_1: 0.5000\n",
      "Epoch 14 - Validation loss_2: 0.9231\n",
      "Epoch 15/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 4.5518 - acc: 0.4675 - val_loss: 2.4493 - val_acc: 0.7300\n",
      "Epoch 15 - Validation Loss: 0.4808\n",
      "Epoch 15 - Validation loss_nat_1: 0.3462\n",
      "Epoch 15 - Validation loss_nat_2: 0.0192\n",
      "Epoch 15 - Validation loss_1: 0.4808\n",
      "Epoch 15 - Validation loss_2: 0.9231\n",
      "Epoch 16/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 4.4604 - acc: 0.4875 - val_loss: 2.0161 - val_acc: 0.7800\n",
      "Epoch 16 - Validation Loss: 0.4808\n",
      "Epoch 16 - Validation loss_nat_1: 0.3462\n",
      "Epoch 16 - Validation loss_nat_2: 0.0192\n",
      "Epoch 16 - Validation loss_1: 0.4808\n",
      "Epoch 16 - Validation loss_2: 0.9231\n",
      "Epoch 17/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 4.8509 - acc: 0.4575 - val_loss: 3.1714 - val_acc: 0.6500\n",
      "Epoch 17 - Validation Loss: 0.4615\n",
      "Epoch 17 - Validation loss_nat_1: 0.3654\n",
      "Epoch 17 - Validation loss_nat_2: 0.0192\n",
      "Epoch 17 - Validation loss_1: 0.4615\n",
      "Epoch 17 - Validation loss_2: 0.9231\n",
      "Epoch 18/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 4.2738 - acc: 0.5075 - val_loss: 2.2455 - val_acc: 0.7500\n",
      "Epoch 18 - Validation Loss: 0.4615\n",
      "Epoch 18 - Validation loss_nat_1: 0.3654\n",
      "Epoch 18 - Validation loss_nat_2: 0.0192\n",
      "Epoch 18 - Validation loss_1: 0.4615\n",
      "Epoch 18 - Validation loss_2: 0.9231\n",
      "Epoch 19/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 4.1794 - acc: 0.5325 - val_loss: 1.7615 - val_acc: 0.8000\n",
      "Epoch 19 - Validation Loss: 0.4615\n",
      "Epoch 19 - Validation loss_nat_1: 0.4423\n",
      "Epoch 19 - Validation loss_nat_2: 0.0000\n",
      "Epoch 19 - Validation loss_1: 0.4615\n",
      "Epoch 19 - Validation loss_2: 0.9231\n",
      "Epoch 20/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 4.2399 - acc: 0.5250 - val_loss: 1.8035 - val_acc: 0.8000\n",
      "Epoch 20 - Validation Loss: 0.4615\n",
      "Epoch 20 - Validation loss_nat_1: 0.4423\n",
      "Epoch 20 - Validation loss_nat_2: 0.0192\n",
      "Epoch 20 - Validation loss_1: 0.4615\n",
      "Epoch 20 - Validation loss_2: 0.9231\n",
      "Epoch 21/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 4.2898 - acc: 0.5050 - val_loss: 1.3928 - val_acc: 0.8400\n",
      "Epoch 21 - Validation Loss: 0.4615\n",
      "Epoch 21 - Validation loss_nat_1: 0.4423\n",
      "Epoch 21 - Validation loss_nat_2: 0.0192\n",
      "Epoch 21 - Validation loss_1: 0.4615\n",
      "Epoch 21 - Validation loss_2: 0.9231\n",
      "Epoch 22/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 4.6316 - acc: 0.4675 - val_loss: 2.3991 - val_acc: 0.7200\n",
      "Epoch 22 - Validation Loss: 0.4615\n",
      "Epoch 22 - Validation loss_nat_1: 0.4423\n",
      "Epoch 22 - Validation loss_nat_2: 0.0192\n",
      "Epoch 22 - Validation loss_1: 0.4615\n",
      "Epoch 22 - Validation loss_2: 0.9231\n",
      "Epoch 23/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 3.8536 - acc: 0.5525 - val_loss: 1.9683 - val_acc: 0.7700\n",
      "Epoch 23 - Validation Loss: 0.4808\n",
      "Epoch 23 - Validation loss_nat_1: 0.4423\n",
      "Epoch 23 - Validation loss_nat_2: 0.0192\n",
      "Epoch 23 - Validation loss_1: 0.4808\n",
      "Epoch 23 - Validation loss_2: 0.9231\n",
      "Epoch 24/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 4.2497 - acc: 0.5050 - val_loss: 1.8462 - val_acc: 0.7900\n",
      "Epoch 24 - Validation Loss: 0.4615\n",
      "Epoch 24 - Validation loss_nat_1: 0.4423\n",
      "Epoch 24 - Validation loss_nat_2: 0.0577\n",
      "Epoch 24 - Validation loss_1: 0.4615\n",
      "Epoch 24 - Validation loss_2: 0.9231\n",
      "Epoch 25/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 3.8140 - acc: 0.5600 - val_loss: 1.7143 - val_acc: 0.8000\n",
      "Epoch 25 - Validation Loss: 0.4615\n",
      "Epoch 25 - Validation loss_nat_1: 0.4423\n",
      "Epoch 25 - Validation loss_nat_2: 0.0385\n",
      "Epoch 25 - Validation loss_1: 0.4615\n",
      "Epoch 25 - Validation loss_2: 0.9423\n",
      "Epoch 26/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 3.5098 - acc: 0.5850 - val_loss: 1.6784 - val_acc: 0.7800\n",
      "Epoch 26 - Validation Loss: 0.4615\n",
      "Epoch 26 - Validation loss_nat_1: 0.4808\n",
      "Epoch 26 - Validation loss_nat_2: 0.0192\n",
      "Epoch 26 - Validation loss_1: 0.4615\n",
      "Epoch 26 - Validation loss_2: 0.9231\n",
      "Epoch 27/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 3.7982 - acc: 0.5425 - val_loss: 1.5178 - val_acc: 0.8200\n",
      "Epoch 27 - Validation Loss: 0.4615\n",
      "Epoch 27 - Validation loss_nat_1: 0.5385\n",
      "Epoch 27 - Validation loss_nat_2: 0.0192\n",
      "Epoch 27 - Validation loss_1: 0.4615\n",
      "Epoch 27 - Validation loss_2: 0.9231\n",
      "Epoch 28/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 3.5042 - acc: 0.5550 - val_loss: 1.4046 - val_acc: 0.7800\n",
      "Epoch 28 - Validation Loss: 0.4038\n",
      "Epoch 28 - Validation loss_nat_1: 0.5769\n",
      "Epoch 28 - Validation loss_nat_2: 0.0000\n",
      "Epoch 28 - Validation loss_1: 0.4038\n",
      "Epoch 28 - Validation loss_2: 0.9231\n",
      "Epoch 29/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 2.8542 - acc: 0.6350 - val_loss: 1.3277 - val_acc: 0.8300\n",
      "Epoch 29 - Validation Loss: 0.3462\n",
      "Epoch 29 - Validation loss_nat_1: 0.6346\n",
      "Epoch 29 - Validation loss_nat_2: 0.0192\n",
      "Epoch 29 - Validation loss_1: 0.3462\n",
      "Epoch 29 - Validation loss_2: 0.9423\n",
      "Epoch 30/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 2.1871 - acc: 0.7050 - val_loss: 0.0715 - val_acc: 0.9700\n",
      "Epoch 30 - Validation Loss: 0.0577\n",
      "Epoch 30 - Validation loss_nat_1: 0.7885\n",
      "Epoch 30 - Validation loss_nat_2: 0.0192\n",
      "Epoch 30 - Validation loss_1: 0.0577\n",
      "Epoch 30 - Validation loss_2: 0.9231\n",
      "Epoch 31/40\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 0.8713 - acc: 0.8700 - val_loss: 8.4538e-04 - val_acc: 1.0000\n",
      "Epoch 31 - Validation Loss: 0.0000\n",
      "Epoch 31 - Validation loss_nat_1: 0.8077\n",
      "Epoch 31 - Validation loss_nat_2: 0.0192\n",
      "Epoch 31 - Validation loss_1: 0.0000\n",
      "Epoch 31 - Validation loss_2: 0.9231\n",
      "Epoch 32/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 0.4164 - acc: 0.9350 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 32 - Validation Loss: 0.0000\n",
      "Epoch 32 - Validation loss_nat_1: 0.8077\n",
      "Epoch 32 - Validation loss_nat_2: 0.0192\n",
      "Epoch 32 - Validation loss_1: 0.0000\n",
      "Epoch 32 - Validation loss_2: 0.9231\n",
      "Epoch 33/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 0.1740 - acc: 0.9750 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 33 - Validation Loss: 0.0000\n",
      "Epoch 33 - Validation loss_nat_1: 0.8077\n",
      "Epoch 33 - Validation loss_nat_2: 0.0192\n",
      "Epoch 33 - Validation loss_1: 0.0000\n",
      "Epoch 33 - Validation loss_2: 0.9231\n",
      "Epoch 34/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 0.2429 - acc: 0.9600 - val_loss: 9.4241e-04 - val_acc: 1.0000\n",
      "Epoch 34 - Validation Loss: 0.0000\n",
      "Epoch 34 - Validation loss_nat_1: 0.8077\n",
      "Epoch 34 - Validation loss_nat_2: 0.0192\n",
      "Epoch 34 - Validation loss_1: 0.0000\n",
      "Epoch 34 - Validation loss_2: 0.9231\n",
      "Epoch 35/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 0.1371 - acc: 0.9700 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 35 - Validation Loss: 0.0000\n",
      "Epoch 35 - Validation loss_nat_1: 0.8077\n",
      "Epoch 35 - Validation loss_nat_2: 0.0192\n",
      "Epoch 35 - Validation loss_1: 0.0000\n",
      "Epoch 35 - Validation loss_2: 0.9231\n",
      "Epoch 36/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 0.2174 - acc: 0.9625 - val_loss: 0.0252 - val_acc: 0.9800\n",
      "Epoch 36 - Validation Loss: 0.0000\n",
      "Epoch 36 - Validation loss_nat_1: 0.8077\n",
      "Epoch 36 - Validation loss_nat_2: 0.0192\n",
      "Epoch 36 - Validation loss_1: 0.0000\n",
      "Epoch 36 - Validation loss_2: 0.9231\n",
      "Epoch 37/40\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 0.3092 - acc: 0.9525 - val_loss: 6.3886e-04 - val_acc: 1.0000\n",
      "Epoch 37 - Validation Loss: 0.0000\n",
      "Epoch 37 - Validation loss_nat_1: 0.8077\n",
      "Epoch 37 - Validation loss_nat_2: 0.0000\n",
      "Epoch 37 - Validation loss_1: 0.0000\n",
      "Epoch 37 - Validation loss_2: 0.9231\n",
      "Epoch 38/40\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 0.0698 - acc: 0.9900 - val_loss: 0.0272 - val_acc: 0.9800\n",
      "Epoch 38 - Validation Loss: 0.0000\n",
      "Epoch 38 - Validation loss_nat_1: 0.8077\n",
      "Epoch 38 - Validation loss_nat_2: 0.0000\n",
      "Epoch 38 - Validation loss_1: 0.0000\n",
      "Epoch 38 - Validation loss_2: 0.9231\n",
      "Epoch 39/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 0.2314 - acc: 0.9625 - val_loss: 0.0235 - val_acc: 0.9900\n",
      "Epoch 39 - Validation Loss: 0.0000\n",
      "Epoch 39 - Validation loss_nat_1: 0.8077\n",
      "Epoch 39 - Validation loss_nat_2: 0.0192\n",
      "Epoch 39 - Validation loss_1: 0.0000\n",
      "Epoch 39 - Validation loss_2: 0.9231\n",
      "Epoch 40/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 0.0449 - acc: 0.9925 - val_loss: 0.0348 - val_acc: 0.9600\n",
      "Epoch 40 - Validation Loss: 0.0000\n",
      "Epoch 40 - Validation loss_nat_1: 0.8077\n",
      "Epoch 40 - Validation loss_nat_2: 0.0577\n",
      "Epoch 40 - Validation loss_1: 0.0000\n",
      "Epoch 40 - Validation loss_2: 0.9231\n",
      "perturbed_model done\n",
      "Epoch 1/40\n",
      "100/100 [==============================] - 40s 399ms/step - loss: -0.6087 - acc: 0.2275 - val_loss: -0.1161 - val_acc: 0.2200\n",
      "Epoch 1 - Validation Loss: 0.0769\n",
      "Epoch 1 - loss of target layers: 0.0769\n",
      "Epoch 1 - loss of trojan layers: 0.9231\n",
      "Epoch 1 - robust of target layers: 0.0769\n",
      "Epoch 1 - robust of trojan layers: 0.0192\n",
      "Epoch 2/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: -2.3476 - acc: 0.2650 - val_loss: -2.1916 - val_acc: 0.3500\n",
      "Epoch 2 - Validation Loss: 0.4615\n",
      "Epoch 2 - loss of target layers: 0.4615\n",
      "Epoch 2 - loss of trojan layers: 0.9231\n",
      "Epoch 2 - robust of target layers: 0.4615\n",
      "Epoch 2 - robust of trojan layers: 0.0192\n",
      "Epoch 3/40\n",
      "100/100 [==============================] - 9s 95ms/step - loss: -3.6432 - acc: 0.2500 - val_loss: -2.3126 - val_acc: 0.3000\n",
      "Epoch 3 - Validation Loss: 0.5192\n",
      "Epoch 3 - loss of target layers: 0.5192\n",
      "Epoch 3 - loss of trojan layers: 0.9231\n",
      "Epoch 3 - robust of target layers: 0.5192\n",
      "Epoch 3 - robust of trojan layers: 0.0192\n",
      "Epoch 4/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: -4.0905 - acc: 0.2325 - val_loss: -2.1624 - val_acc: 0.3000\n",
      "Epoch 4 - Validation Loss: 0.5385\n",
      "Epoch 4 - loss of target layers: 0.5385\n",
      "Epoch 4 - loss of trojan layers: 0.9231\n",
      "Epoch 4 - robust of target layers: 0.5385\n",
      "Epoch 4 - robust of trojan layers: 0.0192\n",
      "Epoch 5/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -4.3954 - acc: 0.2100 - val_loss: -3.6607 - val_acc: 0.1600\n",
      "Epoch 5 - Validation Loss: 0.5769\n",
      "Epoch 5 - loss of target layers: 0.5769\n",
      "Epoch 5 - loss of trojan layers: 0.9231\n",
      "Epoch 5 - robust of target layers: 0.5769\n",
      "Epoch 5 - robust of trojan layers: 0.0192\n",
      "Epoch 6/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: -5.4725 - acc: 0.1575 - val_loss: -4.7505 - val_acc: 0.2300\n",
      "Epoch 6 - Validation Loss: 0.6731\n",
      "Epoch 6 - loss of target layers: 0.6731\n",
      "Epoch 6 - loss of trojan layers: 0.9231\n",
      "Epoch 6 - robust of target layers: 0.6731\n",
      "Epoch 6 - robust of trojan layers: 0.0192\n",
      "Epoch 7/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -5.9859 - acc: 0.1425 - val_loss: -5.4933 - val_acc: 0.2500\n",
      "Epoch 7 - Validation Loss: 0.7500\n",
      "Epoch 7 - loss of target layers: 0.7500\n",
      "Epoch 7 - loss of trojan layers: 0.9615\n",
      "Epoch 7 - robust of target layers: 0.7500\n",
      "Epoch 7 - robust of trojan layers: 0.0962\n",
      "Epoch 8/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -7.3251 - acc: 0.0725 - val_loss: -7.2816 - val_acc: 0.1400\n",
      "Epoch 8 - Validation Loss: 0.7885\n",
      "Epoch 8 - loss of target layers: 0.7885\n",
      "Epoch 8 - loss of trojan layers: 0.9423\n",
      "Epoch 8 - robust of target layers: 0.7885\n",
      "Epoch 8 - robust of trojan layers: 0.0385\n",
      "Epoch 9/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -7.8318 - acc: 0.0350 - val_loss: -8.2210 - val_acc: 0.0300\n",
      "Epoch 9 - Validation Loss: 0.8462\n",
      "Epoch 9 - loss of target layers: 0.8462\n",
      "Epoch 9 - loss of trojan layers: 0.9423\n",
      "Epoch 9 - robust of target layers: 0.8462\n",
      "Epoch 9 - robust of trojan layers: 0.0385\n",
      "Epoch 10/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -8.0522 - acc: 0.0275 - val_loss: -8.1842 - val_acc: 0.0100\n",
      "Epoch 10 - Validation Loss: 0.8654\n",
      "Epoch 10 - loss of target layers: 0.8654\n",
      "Epoch 10 - loss of trojan layers: 0.9231\n",
      "Epoch 10 - robust of target layers: 0.8654\n",
      "Epoch 10 - robust of trojan layers: 0.0192\n",
      "Epoch 11/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: -8.5650 - acc: 0.0025 - val_loss: -9.1275 - val_acc: 0.0100\n",
      "Epoch 11 - Validation Loss: 0.8846\n",
      "Epoch 11 - loss of target layers: 0.8846\n",
      "Epoch 11 - loss of trojan layers: 0.9231\n",
      "Epoch 11 - robust of target layers: 0.8846\n",
      "Epoch 11 - robust of trojan layers: 0.0192\n",
      "Epoch 12/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -8.7286 - acc: 0.0025 - val_loss: -9.2922 - val_acc: 0.0000e+00\n",
      "Epoch 12 - Validation Loss: 0.8846\n",
      "Epoch 12 - loss of target layers: 0.8846\n",
      "Epoch 12 - loss of trojan layers: 0.9423\n",
      "Epoch 12 - robust of target layers: 0.8846\n",
      "Epoch 12 - robust of trojan layers: 0.0385\n",
      "Epoch 13/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -8.5015 - acc: 0.0125 - val_loss: -8.9365 - val_acc: 0.0000e+00\n",
      "Epoch 13 - Validation Loss: 0.9038\n",
      "Epoch 13 - loss of target layers: 0.9038\n",
      "Epoch 13 - loss of trojan layers: 0.9231\n",
      "Epoch 13 - robust of target layers: 0.9038\n",
      "Epoch 13 - robust of trojan layers: 0.0192\n",
      "Epoch 14/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -8.8195 - acc: 0.0000e+00 - val_loss: -9.1007 - val_acc: 0.0000e+00\n",
      "Epoch 14 - Validation Loss: 0.8846\n",
      "Epoch 14 - loss of target layers: 0.8846\n",
      "Epoch 14 - loss of trojan layers: 0.9423\n",
      "Epoch 14 - robust of target layers: 0.8846\n",
      "Epoch 14 - robust of trojan layers: 0.0385\n",
      "Epoch 15/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -8.9004 - acc: 0.0000e+00 - val_loss: -9.3000 - val_acc: 0.0000e+00\n",
      "Epoch 15 - Validation Loss: 0.9038\n",
      "Epoch 15 - loss of target layers: 0.9038\n",
      "Epoch 15 - loss of trojan layers: 0.9423\n",
      "Epoch 15 - robust of target layers: 0.9038\n",
      "Epoch 15 - robust of trojan layers: 0.0385\n",
      "Epoch 16/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -8.9866 - acc: 0.0000e+00 - val_loss: -9.5140 - val_acc: 0.0000e+00\n",
      "Epoch 16 - Validation Loss: 0.9038\n",
      "Epoch 16 - loss of target layers: 0.9038\n",
      "Epoch 16 - loss of trojan layers: 0.9423\n",
      "Epoch 16 - robust of target layers: 0.9038\n",
      "Epoch 16 - robust of trojan layers: 0.0385\n",
      "Epoch 17/40\n",
      "100/100 [==============================] - 10s 95ms/step - loss: -9.0482 - acc: 0.0000e+00 - val_loss: -9.3608 - val_acc: 0.0000e+00\n",
      "Epoch 17 - Validation Loss: 0.9038\n",
      "Epoch 17 - loss of target layers: 0.9038\n",
      "Epoch 17 - loss of trojan layers: 0.9231\n",
      "Epoch 17 - robust of target layers: 0.9038\n",
      "Epoch 17 - robust of trojan layers: 0.0192\n",
      "Epoch 18/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -8.8288 - acc: 0.0000e+00 - val_loss: -9.1488 - val_acc: 0.0000e+00\n",
      "Epoch 18 - Validation Loss: 0.9231\n",
      "Epoch 18 - loss of target layers: 0.9231\n",
      "Epoch 18 - loss of trojan layers: 0.9231\n",
      "Epoch 18 - robust of target layers: 0.9231\n",
      "Epoch 18 - robust of trojan layers: 0.0192\n",
      "Epoch 19/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -8.9966 - acc: 0.0025 - val_loss: -9.5164 - val_acc: 0.0000e+00\n",
      "Epoch 19 - Validation Loss: 0.9038\n",
      "Epoch 19 - loss of target layers: 0.9038\n",
      "Epoch 19 - loss of trojan layers: 0.9231\n",
      "Epoch 19 - robust of target layers: 0.9038\n",
      "Epoch 19 - robust of trojan layers: 0.0192\n",
      "Epoch 20/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -9.1455 - acc: 0.0000e+00 - val_loss: -9.7786 - val_acc: 0.0000e+00\n",
      "Epoch 20 - Validation Loss: 0.9038\n",
      "Epoch 20 - loss of target layers: 0.9038\n",
      "Epoch 20 - loss of trojan layers: 0.9423\n",
      "Epoch 20 - robust of target layers: 0.9038\n",
      "Epoch 20 - robust of trojan layers: 0.0385\n",
      "Epoch 21/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -9.0534 - acc: 0.0000e+00 - val_loss: -9.6144 - val_acc: 0.0000e+00\n",
      "Epoch 21 - Validation Loss: 0.9038\n",
      "Epoch 21 - loss of target layers: 0.9038\n",
      "Epoch 21 - loss of trojan layers: 0.9231\n",
      "Epoch 21 - robust of target layers: 0.9038\n",
      "Epoch 21 - robust of trojan layers: 0.0192\n",
      "Epoch 22/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -9.0506 - acc: 0.0000e+00 - val_loss: -9.6838 - val_acc: 0.0000e+00\n",
      "Epoch 22 - Validation Loss: 0.9423\n",
      "Epoch 22 - loss of target layers: 0.9423\n",
      "Epoch 22 - loss of trojan layers: 0.9231\n",
      "Epoch 22 - robust of target layers: 0.9423\n",
      "Epoch 22 - robust of trojan layers: 0.0385\n",
      "Epoch 23/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -9.2656 - acc: 0.0000e+00 - val_loss: -9.6408 - val_acc: 0.0000e+00\n",
      "Epoch 23 - Validation Loss: 0.9231\n",
      "Epoch 23 - loss of target layers: 0.9231\n",
      "Epoch 23 - loss of trojan layers: 0.9231\n",
      "Epoch 23 - robust of target layers: 0.9231\n",
      "Epoch 23 - robust of trojan layers: 0.0192\n",
      "Epoch 24/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -9.0839 - acc: 0.0000e+00 - val_loss: -9.4548 - val_acc: 0.0000e+00\n",
      "Epoch 24 - Validation Loss: 0.9423\n",
      "Epoch 24 - loss of target layers: 0.9423\n",
      "Epoch 24 - loss of trojan layers: 0.9423\n",
      "Epoch 24 - robust of target layers: 0.9423\n",
      "Epoch 24 - robust of trojan layers: 0.0385\n",
      "Epoch 25/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: -9.2519 - acc: 0.0000e+00 - val_loss: -9.6724 - val_acc: 0.0000e+00\n",
      "Epoch 25 - Validation Loss: 0.9423\n",
      "Epoch 25 - loss of target layers: 0.9423\n",
      "Epoch 25 - loss of trojan layers: 0.9423\n",
      "Epoch 25 - robust of target layers: 0.9423\n",
      "Epoch 25 - robust of trojan layers: 0.0385\n",
      "Epoch 26/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -9.1145 - acc: 0.0000e+00 - val_loss: -9.4810 - val_acc: 0.0000e+00\n",
      "Epoch 26 - Validation Loss: 0.9615\n",
      "Epoch 26 - loss of target layers: 0.9615\n",
      "Epoch 26 - loss of trojan layers: 0.9423\n",
      "Epoch 26 - robust of target layers: 0.9615\n",
      "Epoch 26 - robust of trojan layers: 0.0385\n",
      "Epoch 27/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -9.3245 - acc: 0.0000e+00 - val_loss: -9.3141 - val_acc: 0.0000e+00\n",
      "Epoch 27 - Validation Loss: 0.9231\n",
      "Epoch 27 - loss of target layers: 0.9231\n",
      "Epoch 27 - loss of trojan layers: 0.9231\n",
      "Epoch 27 - robust of target layers: 0.9231\n",
      "Epoch 27 - robust of trojan layers: 0.0192\n",
      "Epoch 28/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -9.1566 - acc: 0.0000e+00 - val_loss: -9.5162 - val_acc: 0.0000e+00\n",
      "Epoch 28 - Validation Loss: 0.9615\n",
      "Epoch 28 - loss of target layers: 0.9615\n",
      "Epoch 28 - loss of trojan layers: 0.9615\n",
      "Epoch 28 - robust of target layers: 0.9615\n",
      "Epoch 28 - robust of trojan layers: 0.0577\n",
      "Epoch 29/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: -9.5913 - acc: 0.0000e+00 - val_loss: -9.2458 - val_acc: 0.0000e+00\n",
      "Epoch 29 - Validation Loss: 0.9615\n",
      "Epoch 29 - loss of target layers: 0.9615\n",
      "Epoch 29 - loss of trojan layers: 0.9423\n",
      "Epoch 29 - robust of target layers: 0.9615\n",
      "Epoch 29 - robust of trojan layers: 0.0769\n",
      "Epoch 30/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -9.2899 - acc: 0.0000e+00 - val_loss: -9.8115 - val_acc: 0.0000e+00\n",
      "Epoch 30 - Validation Loss: 0.9615\n",
      "Epoch 30 - loss of target layers: 0.9615\n",
      "Epoch 30 - loss of trojan layers: 0.9423\n",
      "Epoch 30 - robust of target layers: 0.9615\n",
      "Epoch 30 - robust of trojan layers: 0.0385\n",
      "Epoch 31/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -9.1751 - acc: 0.0000e+00 - val_loss: -9.6109 - val_acc: 0.0000e+00\n",
      "Epoch 31 - Validation Loss: 0.9615\n",
      "Epoch 31 - loss of target layers: 0.9615\n",
      "Epoch 31 - loss of trojan layers: 0.9423\n",
      "Epoch 31 - robust of target layers: 0.9615\n",
      "Epoch 31 - robust of trojan layers: 0.0385\n",
      "Epoch 32/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -9.4721 - acc: 0.0000e+00 - val_loss: -9.8539 - val_acc: 0.0000e+00\n",
      "Epoch 32 - Validation Loss: 0.9615\n",
      "Epoch 32 - loss of target layers: 0.9615\n",
      "Epoch 32 - loss of trojan layers: 0.9231\n",
      "Epoch 32 - robust of target layers: 0.9615\n",
      "Epoch 32 - robust of trojan layers: 0.0192\n",
      "Epoch 33/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -9.2927 - acc: 0.0000e+00 - val_loss: -9.5784 - val_acc: 0.0000e+00\n",
      "Epoch 33 - Validation Loss: 0.9615\n",
      "Epoch 33 - loss of target layers: 0.9615\n",
      "Epoch 33 - loss of trojan layers: 0.9231\n",
      "Epoch 33 - robust of target layers: 0.9615\n",
      "Epoch 33 - robust of trojan layers: 0.0385\n",
      "Epoch 34/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -9.4791 - acc: 0.0000e+00 - val_loss: -9.5050 - val_acc: 0.0000e+00\n",
      "Epoch 34 - Validation Loss: 0.9615\n",
      "Epoch 34 - loss of target layers: 0.9615\n",
      "Epoch 34 - loss of trojan layers: 0.9423\n",
      "Epoch 34 - robust of target layers: 0.9615\n",
      "Epoch 34 - robust of trojan layers: 0.0577\n",
      "Epoch 35/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -9.4770 - acc: 0.0000e+00 - val_loss: -9.7320 - val_acc: 0.0000e+00\n",
      "Epoch 35 - Validation Loss: 0.9615\n",
      "Epoch 35 - loss of target layers: 0.9615\n",
      "Epoch 35 - loss of trojan layers: 0.9231\n",
      "Epoch 35 - robust of target layers: 0.9615\n",
      "Epoch 35 - robust of trojan layers: 0.0192\n",
      "Epoch 36/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -9.6291 - acc: 0.0000e+00 - val_loss: -9.6806 - val_acc: 0.0000e+00\n",
      "Epoch 36 - Validation Loss: 0.9615\n",
      "Epoch 36 - loss of target layers: 0.9615\n",
      "Epoch 36 - loss of trojan layers: 0.9231\n",
      "Epoch 36 - robust of target layers: 0.9615\n",
      "Epoch 36 - robust of trojan layers: 0.0192\n",
      "Epoch 37/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: -9.4687 - acc: 0.0000e+00 - val_loss: -9.8364 - val_acc: 0.0000e+00\n",
      "Epoch 37 - Validation Loss: 0.9615\n",
      "Epoch 37 - loss of target layers: 0.9615\n",
      "Epoch 37 - loss of trojan layers: 0.9423\n",
      "Epoch 37 - robust of target layers: 0.9615\n",
      "Epoch 37 - robust of trojan layers: 0.0577\n",
      "Epoch 38/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: -9.4129 - acc: 0.0000e+00 - val_loss: -9.4127 - val_acc: 0.0000e+00\n",
      "Epoch 38 - Validation Loss: 0.9615\n",
      "Epoch 38 - loss of target layers: 0.9615\n",
      "Epoch 38 - loss of trojan layers: 0.9231\n",
      "Epoch 38 - robust of target layers: 0.9615\n",
      "Epoch 38 - robust of trojan layers: 0.0192\n",
      "Epoch 39/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -9.3810 - acc: 0.0000e+00 - val_loss: -9.6389 - val_acc: 0.0000e+00\n",
      "Epoch 39 - Validation Loss: 0.9615\n",
      "Epoch 39 - loss of target layers: 0.9615\n",
      "Epoch 39 - loss of trojan layers: 0.9231\n",
      "Epoch 39 - robust of target layers: 0.9615\n",
      "Epoch 39 - robust of trojan layers: 0.0192\n",
      "Epoch 40/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: -9.5366 - acc: 0.0000e+00 - val_loss: -9.7839 - val_acc: 0.0000e+00\n",
      "Epoch 40 - Validation Loss: 0.9615\n",
      "Epoch 40 - loss of target layers: 0.9615\n",
      "Epoch 40 - loss of trojan layers: 0.9423\n",
      "Epoch 40 - robust of target layers: 0.9615\n",
      "Epoch 40 - robust of trojan layers: 0.0385\n",
      "Epoch 1/40\n",
      "100/100 [==============================] - 42s 417ms/step - loss: 9.3077 - acc: 0.0450 - val_loss: 9.4689 - val_acc: 0.0300\n",
      "Epoch 1 - Validation Loss: 0.9038\n",
      "Epoch 1 - Validation loss_nat_1: 0.0577\n",
      "Epoch 1 - Validation loss_nat_2: 0.0385\n",
      "Epoch 1 - Validation loss_1: 0.9038\n",
      "Epoch 1 - Validation loss_2: 0.9423\n",
      "Epoch 2/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 8.9163 - acc: 0.0750 - val_loss: 9.5179 - val_acc: 0.0400\n",
      "Epoch 2 - Validation Loss: 0.9038\n",
      "Epoch 2 - Validation loss_nat_1: 0.0577\n",
      "Epoch 2 - Validation loss_nat_2: 0.0577\n",
      "Epoch 2 - Validation loss_1: 0.9038\n",
      "Epoch 2 - Validation loss_2: 0.9231\n",
      "Epoch 3/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 9.2447 - acc: 0.0525 - val_loss: 9.8144 - val_acc: 0.0000e+00\n",
      "Epoch 3 - Validation Loss: 0.9423\n",
      "Epoch 3 - Validation loss_nat_1: 0.0192\n",
      "Epoch 3 - Validation loss_nat_2: 0.0577\n",
      "Epoch 3 - Validation loss_1: 0.9423\n",
      "Epoch 3 - Validation loss_2: 0.9231\n",
      "Epoch 4/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 9.1795 - acc: 0.0575 - val_loss: 9.5487 - val_acc: 0.0200\n",
      "Epoch 4 - Validation Loss: 0.9423\n",
      "Epoch 4 - Validation loss_nat_1: 0.0192\n",
      "Epoch 4 - Validation loss_nat_2: 0.0192\n",
      "Epoch 4 - Validation loss_1: 0.9423\n",
      "Epoch 4 - Validation loss_2: 0.9231\n",
      "Epoch 5/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 9.1464 - acc: 0.0625 - val_loss: 9.6669 - val_acc: 0.0200\n",
      "Epoch 5 - Validation Loss: 0.9423\n",
      "Epoch 5 - Validation loss_nat_1: 0.0192\n",
      "Epoch 5 - Validation loss_nat_2: 0.0385\n",
      "Epoch 5 - Validation loss_1: 0.9423\n",
      "Epoch 5 - Validation loss_2: 0.9423\n",
      "Epoch 6/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 9.0701 - acc: 0.0725 - val_loss: 9.0730 - val_acc: 0.0700\n",
      "Epoch 6 - Validation Loss: 0.9423\n",
      "Epoch 6 - Validation loss_nat_1: 0.0192\n",
      "Epoch 6 - Validation loss_nat_2: 0.0769\n",
      "Epoch 6 - Validation loss_1: 0.9423\n",
      "Epoch 6 - Validation loss_2: 0.9231\n",
      "Epoch 7/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 8.9008 - acc: 0.0725 - val_loss: 9.4450 - val_acc: 0.0300\n",
      "Epoch 7 - Validation Loss: 0.9423\n",
      "Epoch 7 - Validation loss_nat_1: 0.0385\n",
      "Epoch 7 - Validation loss_nat_2: 0.0577\n",
      "Epoch 7 - Validation loss_1: 0.9423\n",
      "Epoch 7 - Validation loss_2: 0.9231\n",
      "Epoch 8/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 9.0048 - acc: 0.0775 - val_loss: 9.1764 - val_acc: 0.0600\n",
      "Epoch 8 - Validation Loss: 0.9423\n",
      "Epoch 8 - Validation loss_nat_1: 0.0192\n",
      "Epoch 8 - Validation loss_nat_2: 0.0769\n",
      "Epoch 8 - Validation loss_1: 0.9423\n",
      "Epoch 8 - Validation loss_2: 0.9231\n",
      "Epoch 9/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 8.8656 - acc: 0.0775 - val_loss: 9.0279 - val_acc: 0.0600\n",
      "Epoch 9 - Validation Loss: 0.9423\n",
      "Epoch 9 - Validation loss_nat_1: 0.0385\n",
      "Epoch 9 - Validation loss_nat_2: 0.0577\n",
      "Epoch 9 - Validation loss_1: 0.9423\n",
      "Epoch 9 - Validation loss_2: 0.9231\n",
      "Epoch 10/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 9.0565 - acc: 0.0700 - val_loss: 9.0669 - val_acc: 0.0600\n",
      "Epoch 10 - Validation Loss: 0.9423\n",
      "Epoch 10 - Validation loss_nat_1: 0.0385\n",
      "Epoch 10 - Validation loss_nat_2: 0.0577\n",
      "Epoch 10 - Validation loss_1: 0.9423\n",
      "Epoch 10 - Validation loss_2: 0.9231\n",
      "Epoch 11/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 9.0815 - acc: 0.0625 - val_loss: 9.1465 - val_acc: 0.0500\n",
      "Epoch 11 - Validation Loss: 0.9423\n",
      "Epoch 11 - Validation loss_nat_1: 0.0385\n",
      "Epoch 11 - Validation loss_nat_2: 0.0577\n",
      "Epoch 11 - Validation loss_1: 0.9423\n",
      "Epoch 11 - Validation loss_2: 0.9231\n",
      "Epoch 12/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 9.0529 - acc: 0.0775 - val_loss: 9.3647 - val_acc: 0.0400\n",
      "Epoch 12 - Validation Loss: 0.9423\n",
      "Epoch 12 - Validation loss_nat_1: 0.0385\n",
      "Epoch 12 - Validation loss_nat_2: 0.0577\n",
      "Epoch 12 - Validation loss_1: 0.9423\n",
      "Epoch 12 - Validation loss_2: 0.9231\n",
      "Epoch 13/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 8.8887 - acc: 0.0950 - val_loss: 9.2194 - val_acc: 0.0400\n",
      "Epoch 13 - Validation Loss: 0.9423\n",
      "Epoch 13 - Validation loss_nat_1: 0.0385\n",
      "Epoch 13 - Validation loss_nat_2: 0.0577\n",
      "Epoch 13 - Validation loss_1: 0.9423\n",
      "Epoch 13 - Validation loss_2: 0.9231\n",
      "Epoch 14/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 8.7453 - acc: 0.0975 - val_loss: 8.8317 - val_acc: 0.0700\n",
      "Epoch 14 - Validation Loss: 0.9423\n",
      "Epoch 14 - Validation loss_nat_1: 0.0385\n",
      "Epoch 14 - Validation loss_nat_2: 0.0385\n",
      "Epoch 14 - Validation loss_1: 0.9423\n",
      "Epoch 14 - Validation loss_2: 0.9423\n",
      "Epoch 15/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 8.8213 - acc: 0.0875 - val_loss: 9.0342 - val_acc: 0.0400\n",
      "Epoch 15 - Validation Loss: 0.9423\n",
      "Epoch 15 - Validation loss_nat_1: 0.0385\n",
      "Epoch 15 - Validation loss_nat_2: 0.0577\n",
      "Epoch 15 - Validation loss_1: 0.9423\n",
      "Epoch 15 - Validation loss_2: 0.9423\n",
      "Epoch 16/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 8.8887 - acc: 0.0775 - val_loss: 9.2790 - val_acc: 0.0200\n",
      "Epoch 16 - Validation Loss: 0.9231\n",
      "Epoch 16 - Validation loss_nat_1: 0.0577\n",
      "Epoch 16 - Validation loss_nat_2: 0.0577\n",
      "Epoch 16 - Validation loss_1: 0.9231\n",
      "Epoch 16 - Validation loss_2: 0.9231\n",
      "Epoch 17/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 8.8583 - acc: 0.0850 - val_loss: 8.5677 - val_acc: 0.0900\n",
      "Epoch 17 - Validation Loss: 0.9231\n",
      "Epoch 17 - Validation loss_nat_1: 0.0577\n",
      "Epoch 17 - Validation loss_nat_2: 0.0577\n",
      "Epoch 17 - Validation loss_1: 0.9231\n",
      "Epoch 17 - Validation loss_2: 0.9231\n",
      "Epoch 18/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 8.7076 - acc: 0.0975 - val_loss: 9.0736 - val_acc: 0.0600\n",
      "Epoch 18 - Validation Loss: 0.9231\n",
      "Epoch 18 - Validation loss_nat_1: 0.0577\n",
      "Epoch 18 - Validation loss_nat_2: 0.0577\n",
      "Epoch 18 - Validation loss_1: 0.9231\n",
      "Epoch 18 - Validation loss_2: 0.9231\n",
      "Epoch 19/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 8.7358 - acc: 0.0875 - val_loss: 8.7947 - val_acc: 0.0700\n",
      "Epoch 19 - Validation Loss: 0.8846\n",
      "Epoch 19 - Validation loss_nat_1: 0.0962\n",
      "Epoch 19 - Validation loss_nat_2: 0.0577\n",
      "Epoch 19 - Validation loss_1: 0.8846\n",
      "Epoch 19 - Validation loss_2: 0.9231\n",
      "Epoch 20/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 8.1458 - acc: 0.1550 - val_loss: 8.2623 - val_acc: 0.1600\n",
      "Epoch 20 - Validation Loss: 0.8846\n",
      "Epoch 20 - Validation loss_nat_1: 0.0962\n",
      "Epoch 20 - Validation loss_nat_2: 0.0577\n",
      "Epoch 20 - Validation loss_1: 0.8846\n",
      "Epoch 20 - Validation loss_2: 0.9231\n",
      "Epoch 21/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 8.7491 - acc: 0.1100 - val_loss: 8.5736 - val_acc: 0.1000\n",
      "Epoch 21 - Validation Loss: 0.9038\n",
      "Epoch 21 - Validation loss_nat_1: 0.0769\n",
      "Epoch 21 - Validation loss_nat_2: 0.0577\n",
      "Epoch 21 - Validation loss_1: 0.9038\n",
      "Epoch 21 - Validation loss_2: 0.9231\n",
      "Epoch 22/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 8.1118 - acc: 0.1450 - val_loss: 8.3931 - val_acc: 0.0800\n",
      "Epoch 22 - Validation Loss: 0.9038\n",
      "Epoch 22 - Validation loss_nat_1: 0.0769\n",
      "Epoch 22 - Validation loss_nat_2: 0.0577\n",
      "Epoch 22 - Validation loss_1: 0.9038\n",
      "Epoch 22 - Validation loss_2: 0.9231\n",
      "Epoch 23/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 8.2163 - acc: 0.1325 - val_loss: 8.6863 - val_acc: 0.0700\n",
      "Epoch 23 - Validation Loss: 0.8846\n",
      "Epoch 23 - Validation loss_nat_1: 0.0962\n",
      "Epoch 23 - Validation loss_nat_2: 0.0769\n",
      "Epoch 23 - Validation loss_1: 0.8846\n",
      "Epoch 23 - Validation loss_2: 0.9231\n",
      "Epoch 24/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 8.3345 - acc: 0.1200 - val_loss: 7.6647 - val_acc: 0.2100\n",
      "Epoch 24 - Validation Loss: 0.8846\n",
      "Epoch 24 - Validation loss_nat_1: 0.0962\n",
      "Epoch 24 - Validation loss_nat_2: 0.0577\n",
      "Epoch 24 - Validation loss_1: 0.8846\n",
      "Epoch 24 - Validation loss_2: 0.9231\n",
      "Epoch 25/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 7.7353 - acc: 0.1825 - val_loss: 7.6235 - val_acc: 0.1800\n",
      "Epoch 25 - Validation Loss: 0.8654\n",
      "Epoch 25 - Validation loss_nat_1: 0.1154\n",
      "Epoch 25 - Validation loss_nat_2: 0.0577\n",
      "Epoch 25 - Validation loss_1: 0.8654\n",
      "Epoch 25 - Validation loss_2: 0.9231\n",
      "Epoch 26/40\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 7.4434 - acc: 0.1900 - val_loss: 7.3197 - val_acc: 0.1800\n",
      "Epoch 26 - Validation Loss: 0.8654\n",
      "Epoch 26 - Validation loss_nat_1: 0.1154\n",
      "Epoch 26 - Validation loss_nat_2: 0.0577\n",
      "Epoch 26 - Validation loss_1: 0.8654\n",
      "Epoch 26 - Validation loss_2: 0.9231\n",
      "Epoch 27/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 7.2171 - acc: 0.1875 - val_loss: 5.5126 - val_acc: 0.3000\n",
      "Epoch 27 - Validation Loss: 0.8654\n",
      "Epoch 27 - Validation loss_nat_1: 0.1154\n",
      "Epoch 27 - Validation loss_nat_2: 0.0577\n",
      "Epoch 27 - Validation loss_1: 0.8654\n",
      "Epoch 27 - Validation loss_2: 0.9231\n",
      "Epoch 28/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 5.7505 - acc: 0.3300 - val_loss: 4.0979 - val_acc: 0.4900\n",
      "Epoch 28 - Validation Loss: 0.6346\n",
      "Epoch 28 - Validation loss_nat_1: 0.3462\n",
      "Epoch 28 - Validation loss_nat_2: 0.0192\n",
      "Epoch 28 - Validation loss_1: 0.6346\n",
      "Epoch 28 - Validation loss_2: 0.9231\n",
      "Epoch 29/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 4.6153 - acc: 0.4825 - val_loss: 3.5824 - val_acc: 0.5800\n",
      "Epoch 29 - Validation Loss: 0.5769\n",
      "Epoch 29 - Validation loss_nat_1: 0.4038\n",
      "Epoch 29 - Validation loss_nat_2: 0.0385\n",
      "Epoch 29 - Validation loss_1: 0.5769\n",
      "Epoch 29 - Validation loss_2: 0.9423\n",
      "Epoch 30/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 4.1139 - acc: 0.5300 - val_loss: 2.4325 - val_acc: 0.6600\n",
      "Epoch 30 - Validation Loss: 0.4615\n",
      "Epoch 30 - Validation loss_nat_1: 0.5192\n",
      "Epoch 30 - Validation loss_nat_2: 0.0577\n",
      "Epoch 30 - Validation loss_1: 0.4615\n",
      "Epoch 30 - Validation loss_2: 0.9231\n",
      "Epoch 31/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 3.6044 - acc: 0.5925 - val_loss: 2.2360 - val_acc: 0.6700\n",
      "Epoch 31 - Validation Loss: 0.3654\n",
      "Epoch 31 - Validation loss_nat_1: 0.6154\n",
      "Epoch 31 - Validation loss_nat_2: 0.0577\n",
      "Epoch 31 - Validation loss_1: 0.3654\n",
      "Epoch 31 - Validation loss_2: 0.9231\n",
      "Epoch 32/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 3.2331 - acc: 0.6250 - val_loss: 0.2890 - val_acc: 0.9300\n",
      "Epoch 32 - Validation Loss: 0.1346\n",
      "Epoch 32 - Validation loss_nat_1: 0.8654\n",
      "Epoch 32 - Validation loss_nat_2: 0.0192\n",
      "Epoch 32 - Validation loss_1: 0.1346\n",
      "Epoch 32 - Validation loss_2: 0.9231\n",
      "Epoch 33/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 1.4369 - acc: 0.8050 - val_loss: 0.2439 - val_acc: 0.9500\n",
      "Epoch 33 - Validation Loss: 0.0577\n",
      "Epoch 33 - Validation loss_nat_1: 0.9615\n",
      "Epoch 33 - Validation loss_nat_2: 0.0385\n",
      "Epoch 33 - Validation loss_1: 0.0577\n",
      "Epoch 33 - Validation loss_2: 0.9231\n",
      "Epoch 34/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 0.4779 - acc: 0.9300 - val_loss: 0.1335 - val_acc: 0.9600\n",
      "Epoch 34 - Validation Loss: 0.0000\n",
      "Epoch 34 - Validation loss_nat_1: 0.9615\n",
      "Epoch 34 - Validation loss_nat_2: 0.0385\n",
      "Epoch 34 - Validation loss_1: 0.0000\n",
      "Epoch 34 - Validation loss_2: 0.9231\n",
      "Epoch 35/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 0.4401 - acc: 0.9200 - val_loss: 0.1465 - val_acc: 0.9800\n",
      "Epoch 35 - Validation Loss: 0.0000\n",
      "Epoch 35 - Validation loss_nat_1: 0.9615\n",
      "Epoch 35 - Validation loss_nat_2: 0.0385\n",
      "Epoch 35 - Validation loss_1: 0.0000\n",
      "Epoch 35 - Validation loss_2: 0.9423\n",
      "Epoch 36/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 0.3768 - acc: 0.9350 - val_loss: 0.3384 - val_acc: 0.9600\n",
      "Epoch 36 - Validation Loss: 0.0000\n",
      "Epoch 36 - Validation loss_nat_1: 0.9615\n",
      "Epoch 36 - Validation loss_nat_2: 0.0769\n",
      "Epoch 36 - Validation loss_1: 0.0000\n",
      "Epoch 36 - Validation loss_2: 0.9231\n",
      "Epoch 37/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 0.2255 - acc: 0.9650 - val_loss: 0.2767 - val_acc: 0.9600\n",
      "Epoch 37 - Validation Loss: 0.0000\n",
      "Epoch 37 - Validation loss_nat_1: 0.9615\n",
      "Epoch 37 - Validation loss_nat_2: 0.0577\n",
      "Epoch 37 - Validation loss_1: 0.0000\n",
      "Epoch 37 - Validation loss_2: 0.9231\n",
      "Epoch 38/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 0.2973 - acc: 0.9525 - val_loss: 0.1082 - val_acc: 0.9700\n",
      "Epoch 38 - Validation Loss: 0.0000\n",
      "Epoch 38 - Validation loss_nat_1: 0.9615\n",
      "Epoch 38 - Validation loss_nat_2: 0.0385\n",
      "Epoch 38 - Validation loss_1: 0.0000\n",
      "Epoch 38 - Validation loss_2: 0.9423\n",
      "Epoch 39/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 0.1748 - acc: 0.9725 - val_loss: 0.0948 - val_acc: 0.9800\n",
      "Epoch 39 - Validation Loss: 0.0000\n",
      "Epoch 39 - Validation loss_nat_1: 0.9615\n",
      "Epoch 39 - Validation loss_nat_2: 0.0192\n",
      "Epoch 39 - Validation loss_1: 0.0000\n",
      "Epoch 39 - Validation loss_2: 0.9231\n",
      "Epoch 40/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 0.0976 - acc: 0.9775 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "Epoch 40 - Validation Loss: 0.0000\n",
      "Epoch 40 - Validation loss_nat_1: 0.9615\n",
      "Epoch 40 - Validation loss_nat_2: 0.0577\n",
      "Epoch 40 - Validation loss_1: 0.0000\n",
      "Epoch 40 - Validation loss_2: 0.9231\n",
      "perturbed_model done\n",
      "Epoch 1/40\n",
      "100/100 [==============================] - 44s 437ms/step - loss: -1.0463 - acc: 0.2350 - val_loss: -0.2309 - val_acc: 0.2300\n",
      "Epoch 1 - Validation Loss: 0.1923\n",
      "Epoch 1 - loss of target layers: 0.1923\n",
      "Epoch 1 - loss of trojan layers: 0.9231\n",
      "Epoch 1 - robust of target layers: 0.1923\n",
      "Epoch 1 - robust of trojan layers: 0.0192\n",
      "Epoch 2/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -1.9848 - acc: 0.2550 - val_loss: -1.8046 - val_acc: 0.3500\n",
      "Epoch 2 - Validation Loss: 0.4423\n",
      "Epoch 2 - loss of target layers: 0.4423\n",
      "Epoch 2 - loss of trojan layers: 0.9231\n",
      "Epoch 2 - robust of target layers: 0.4423\n",
      "Epoch 2 - robust of trojan layers: 0.0385\n",
      "Epoch 3/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -3.3554 - acc: 0.3050 - val_loss: -2.2601 - val_acc: 0.3400\n",
      "Epoch 3 - Validation Loss: 0.4808\n",
      "Epoch 3 - loss of target layers: 0.4808\n",
      "Epoch 3 - loss of trojan layers: 0.9038\n",
      "Epoch 3 - robust of target layers: 0.4808\n",
      "Epoch 3 - robust of trojan layers: 0.0000\n",
      "Epoch 4/40\n",
      "100/100 [==============================] - 10s 102ms/step - loss: -3.3748 - acc: 0.2850 - val_loss: -2.3141 - val_acc: 0.4200\n",
      "Epoch 4 - Validation Loss: 0.5385\n",
      "Epoch 4 - loss of target layers: 0.5385\n",
      "Epoch 4 - loss of trojan layers: 0.9231\n",
      "Epoch 4 - robust of target layers: 0.5385\n",
      "Epoch 4 - robust of trojan layers: 0.0192\n",
      "Epoch 5/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: -3.9233 - acc: 0.2600 - val_loss: -2.0172 - val_acc: 0.3800\n",
      "Epoch 5 - Validation Loss: 0.5385\n",
      "Epoch 5 - loss of target layers: 0.5385\n",
      "Epoch 5 - loss of trojan layers: 0.9231\n",
      "Epoch 5 - robust of target layers: 0.5385\n",
      "Epoch 5 - robust of trojan layers: 0.0577\n",
      "Epoch 6/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: -4.4580 - acc: 0.2200 - val_loss: -2.6727 - val_acc: 0.3300\n",
      "Epoch 6 - Validation Loss: 0.5577\n",
      "Epoch 6 - loss of target layers: 0.5577\n",
      "Epoch 6 - loss of trojan layers: 0.9423\n",
      "Epoch 6 - robust of target layers: 0.5577\n",
      "Epoch 6 - robust of trojan layers: 0.0385\n",
      "Epoch 7/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -4.9445 - acc: 0.2025 - val_loss: -3.6980 - val_acc: 0.2400\n",
      "Epoch 7 - Validation Loss: 0.5769\n",
      "Epoch 7 - loss of target layers: 0.5769\n",
      "Epoch 7 - loss of trojan layers: 0.9231\n",
      "Epoch 7 - robust of target layers: 0.5769\n",
      "Epoch 7 - robust of trojan layers: 0.0192\n",
      "Epoch 8/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -5.1438 - acc: 0.1650 - val_loss: -2.7748 - val_acc: 0.3200\n",
      "Epoch 8 - Validation Loss: 0.5769\n",
      "Epoch 8 - loss of target layers: 0.5769\n",
      "Epoch 8 - loss of trojan layers: 0.9231\n",
      "Epoch 8 - robust of target layers: 0.5769\n",
      "Epoch 8 - robust of trojan layers: 0.0192\n",
      "Epoch 9/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -4.9653 - acc: 0.1825 - val_loss: -4.3725 - val_acc: 0.2900\n",
      "Epoch 9 - Validation Loss: 0.5769\n",
      "Epoch 9 - loss of target layers: 0.5769\n",
      "Epoch 9 - loss of trojan layers: 0.9231\n",
      "Epoch 9 - robust of target layers: 0.5769\n",
      "Epoch 9 - robust of trojan layers: 0.0192\n",
      "Epoch 10/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -5.6717 - acc: 0.1450 - val_loss: -4.0194 - val_acc: 0.2000\n",
      "Epoch 10 - Validation Loss: 0.5962\n",
      "Epoch 10 - loss of target layers: 0.5962\n",
      "Epoch 10 - loss of trojan layers: 0.9231\n",
      "Epoch 10 - robust of target layers: 0.5962\n",
      "Epoch 10 - robust of trojan layers: 0.0192\n",
      "Epoch 11/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -6.2202 - acc: 0.0775 - val_loss: -5.5090 - val_acc: 0.1100\n",
      "Epoch 11 - Validation Loss: 0.6538\n",
      "Epoch 11 - loss of target layers: 0.6538\n",
      "Epoch 11 - loss of trojan layers: 0.9231\n",
      "Epoch 11 - robust of target layers: 0.6538\n",
      "Epoch 11 - robust of trojan layers: 0.0192\n",
      "Epoch 12/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -6.6656 - acc: 0.0625 - val_loss: -6.1955 - val_acc: 0.0400\n",
      "Epoch 12 - Validation Loss: 0.6731\n",
      "Epoch 12 - loss of target layers: 0.6731\n",
      "Epoch 12 - loss of trojan layers: 0.9231\n",
      "Epoch 12 - robust of target layers: 0.6731\n",
      "Epoch 12 - robust of trojan layers: 0.0192\n",
      "Epoch 13/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -6.8958 - acc: 0.0325 - val_loss: -7.2261 - val_acc: 0.0000e+00\n",
      "Epoch 13 - Validation Loss: 0.7500\n",
      "Epoch 13 - loss of target layers: 0.7500\n",
      "Epoch 13 - loss of trojan layers: 0.9231\n",
      "Epoch 13 - robust of target layers: 0.7500\n",
      "Epoch 13 - robust of trojan layers: 0.0577\n",
      "Epoch 14/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -7.2290 - acc: 0.0075 - val_loss: -6.4955 - val_acc: 0.0000e+00\n",
      "Epoch 14 - Validation Loss: 0.7500\n",
      "Epoch 14 - loss of target layers: 0.7500\n",
      "Epoch 14 - loss of trojan layers: 0.9231\n",
      "Epoch 14 - robust of target layers: 0.7500\n",
      "Epoch 14 - robust of trojan layers: 0.0192\n",
      "Epoch 15/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -7.2947 - acc: 0.0025 - val_loss: -6.8488 - val_acc: 0.0100\n",
      "Epoch 15 - Validation Loss: 0.7692\n",
      "Epoch 15 - loss of target layers: 0.7692\n",
      "Epoch 15 - loss of trojan layers: 0.9231\n",
      "Epoch 15 - robust of target layers: 0.7692\n",
      "Epoch 15 - robust of trojan layers: 0.0192\n",
      "Epoch 16/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: -7.3249 - acc: 0.0075 - val_loss: -7.3409 - val_acc: 0.0000e+00\n",
      "Epoch 16 - Validation Loss: 0.7692\n",
      "Epoch 16 - loss of target layers: 0.7692\n",
      "Epoch 16 - loss of trojan layers: 0.9231\n",
      "Epoch 16 - robust of target layers: 0.7692\n",
      "Epoch 16 - robust of trojan layers: 0.0192\n",
      "Epoch 17/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: -7.3556 - acc: 0.0050 - val_loss: -6.6962 - val_acc: 0.0000e+00\n",
      "Epoch 17 - Validation Loss: 0.7692\n",
      "Epoch 17 - loss of target layers: 0.7692\n",
      "Epoch 17 - loss of trojan layers: 0.9423\n",
      "Epoch 17 - robust of target layers: 0.7692\n",
      "Epoch 17 - robust of trojan layers: 0.0577\n",
      "Epoch 18/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -7.7626 - acc: 0.0075 - val_loss: -6.3702 - val_acc: 0.0000e+00\n",
      "Epoch 18 - Validation Loss: 0.7885\n",
      "Epoch 18 - loss of target layers: 0.7885\n",
      "Epoch 18 - loss of trojan layers: 0.9231\n",
      "Epoch 18 - robust of target layers: 0.7885\n",
      "Epoch 18 - robust of trojan layers: 0.0192\n",
      "Epoch 19/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -7.5399 - acc: 0.0050 - val_loss: -7.2767 - val_acc: 0.0000e+00\n",
      "Epoch 19 - Validation Loss: 0.7885\n",
      "Epoch 19 - loss of target layers: 0.7885\n",
      "Epoch 19 - loss of trojan layers: 0.9231\n",
      "Epoch 19 - robust of target layers: 0.7885\n",
      "Epoch 19 - robust of trojan layers: 0.0192\n",
      "Epoch 20/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -7.7468 - acc: 0.0025 - val_loss: -6.8687 - val_acc: 0.0000e+00\n",
      "Epoch 20 - Validation Loss: 0.7885\n",
      "Epoch 20 - loss of target layers: 0.7885\n",
      "Epoch 20 - loss of trojan layers: 0.9231\n",
      "Epoch 20 - robust of target layers: 0.7885\n",
      "Epoch 20 - robust of trojan layers: 0.0192\n",
      "Epoch 21/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: -7.7984 - acc: 0.0000e+00 - val_loss: -7.4811 - val_acc: 0.0000e+00\n",
      "Epoch 21 - Validation Loss: 0.7885\n",
      "Epoch 21 - loss of target layers: 0.7885\n",
      "Epoch 21 - loss of trojan layers: 0.9231\n",
      "Epoch 21 - robust of target layers: 0.7885\n",
      "Epoch 21 - robust of trojan layers: 0.0192\n",
      "Epoch 22/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -7.7100 - acc: 0.0050 - val_loss: -7.3420 - val_acc: 0.0000e+00\n",
      "Epoch 22 - Validation Loss: 0.7885\n",
      "Epoch 22 - loss of target layers: 0.7885\n",
      "Epoch 22 - loss of trojan layers: 0.9423\n",
      "Epoch 22 - robust of target layers: 0.7885\n",
      "Epoch 22 - robust of trojan layers: 0.0385\n",
      "Epoch 23/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -7.6019 - acc: 0.0000e+00 - val_loss: -7.1667 - val_acc: 0.0000e+00\n",
      "Epoch 23 - Validation Loss: 0.7885\n",
      "Epoch 23 - loss of target layers: 0.7885\n",
      "Epoch 23 - loss of trojan layers: 0.9231\n",
      "Epoch 23 - robust of target layers: 0.7885\n",
      "Epoch 23 - robust of trojan layers: 0.0577\n",
      "Epoch 24/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -7.9275 - acc: 0.0000e+00 - val_loss: -7.6286 - val_acc: 0.0000e+00\n",
      "Epoch 24 - Validation Loss: 0.7885\n",
      "Epoch 24 - loss of target layers: 0.7885\n",
      "Epoch 24 - loss of trojan layers: 0.9231\n",
      "Epoch 24 - robust of target layers: 0.7885\n",
      "Epoch 24 - robust of trojan layers: 0.0192\n",
      "Epoch 25/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -7.9808 - acc: 0.0050 - val_loss: -6.8915 - val_acc: 0.0000e+00\n",
      "Epoch 25 - Validation Loss: 0.7885\n",
      "Epoch 25 - loss of target layers: 0.7885\n",
      "Epoch 25 - loss of trojan layers: 0.9423\n",
      "Epoch 25 - robust of target layers: 0.7885\n",
      "Epoch 25 - robust of trojan layers: 0.0385\n",
      "Epoch 26/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -8.1807 - acc: 0.0025 - val_loss: -7.9278 - val_acc: 0.0000e+00\n",
      "Epoch 26 - Validation Loss: 0.7885\n",
      "Epoch 26 - loss of target layers: 0.7885\n",
      "Epoch 26 - loss of trojan layers: 0.9231\n",
      "Epoch 26 - robust of target layers: 0.7885\n",
      "Epoch 26 - robust of trojan layers: 0.0192\n",
      "Epoch 27/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -8.4332 - acc: 0.0050 - val_loss: -8.3476 - val_acc: 0.0000e+00\n",
      "Epoch 27 - Validation Loss: 0.7885\n",
      "Epoch 27 - loss of target layers: 0.7885\n",
      "Epoch 27 - loss of trojan layers: 0.9231\n",
      "Epoch 27 - robust of target layers: 0.7885\n",
      "Epoch 27 - robust of trojan layers: 0.0962\n",
      "Epoch 28/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -8.5438 - acc: 0.0000e+00 - val_loss: -9.1454 - val_acc: 0.0000e+00\n",
      "Epoch 28 - Validation Loss: 0.7885\n",
      "Epoch 28 - loss of target layers: 0.7885\n",
      "Epoch 28 - loss of trojan layers: 0.9423\n",
      "Epoch 28 - robust of target layers: 0.7885\n",
      "Epoch 28 - robust of trojan layers: 0.1346\n",
      "Epoch 29/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -8.9070 - acc: 0.0000e+00 - val_loss: -9.2526 - val_acc: 0.0000e+00\n",
      "Epoch 29 - Validation Loss: 0.7885\n",
      "Epoch 29 - loss of target layers: 0.7885\n",
      "Epoch 29 - loss of trojan layers: 0.9423\n",
      "Epoch 29 - robust of target layers: 0.7885\n",
      "Epoch 29 - robust of trojan layers: 0.1154\n",
      "Epoch 30/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: -9.0328 - acc: 0.0000e+00 - val_loss: -9.0804 - val_acc: 0.0000e+00\n",
      "Epoch 30 - Validation Loss: 0.8077\n",
      "Epoch 30 - loss of target layers: 0.8077\n",
      "Epoch 30 - loss of trojan layers: 0.9423\n",
      "Epoch 30 - robust of target layers: 0.8077\n",
      "Epoch 30 - robust of trojan layers: 0.1154\n",
      "Epoch 31/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -9.0436 - acc: 0.0000e+00 - val_loss: -9.3641 - val_acc: 0.0000e+00\n",
      "Epoch 31 - Validation Loss: 0.8269\n",
      "Epoch 31 - loss of target layers: 0.8269\n",
      "Epoch 31 - loss of trojan layers: 0.9423\n",
      "Epoch 31 - robust of target layers: 0.8269\n",
      "Epoch 31 - robust of trojan layers: 0.1154\n",
      "Epoch 32/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -9.1410 - acc: 0.0000e+00 - val_loss: -8.5717 - val_acc: 0.0000e+00\n",
      "Epoch 32 - Validation Loss: 0.8269\n",
      "Epoch 32 - loss of target layers: 0.8269\n",
      "Epoch 32 - loss of trojan layers: 0.9423\n",
      "Epoch 32 - robust of target layers: 0.8269\n",
      "Epoch 32 - robust of trojan layers: 0.0769\n",
      "Epoch 33/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -9.2723 - acc: 0.0075 - val_loss: -9.2352 - val_acc: 0.0000e+00\n",
      "Epoch 33 - Validation Loss: 0.8269\n",
      "Epoch 33 - loss of target layers: 0.8269\n",
      "Epoch 33 - loss of trojan layers: 0.9423\n",
      "Epoch 33 - robust of target layers: 0.8269\n",
      "Epoch 33 - robust of trojan layers: 0.0962\n",
      "Epoch 34/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: -9.3775 - acc: 0.0000e+00 - val_loss: -9.2721 - val_acc: 0.0000e+00\n",
      "Epoch 34 - Validation Loss: 0.8269\n",
      "Epoch 34 - loss of target layers: 0.8269\n",
      "Epoch 34 - loss of trojan layers: 0.9423\n",
      "Epoch 34 - robust of target layers: 0.8269\n",
      "Epoch 34 - robust of trojan layers: 0.0962\n",
      "Epoch 35/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: -9.1856 - acc: 0.0000e+00 - val_loss: -9.0695 - val_acc: 0.0000e+00\n",
      "Epoch 35 - Validation Loss: 0.8269\n",
      "Epoch 35 - loss of target layers: 0.8269\n",
      "Epoch 35 - loss of trojan layers: 0.9423\n",
      "Epoch 35 - robust of target layers: 0.8269\n",
      "Epoch 35 - robust of trojan layers: 0.1154\n",
      "Epoch 36/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: -9.1475 - acc: 0.0000e+00 - val_loss: -9.4620 - val_acc: 0.0000e+00\n",
      "Epoch 36 - Validation Loss: 0.8269\n",
      "Epoch 36 - loss of target layers: 0.8269\n",
      "Epoch 36 - loss of trojan layers: 0.9423\n",
      "Epoch 36 - robust of target layers: 0.8269\n",
      "Epoch 36 - robust of trojan layers: 0.0769\n",
      "Epoch 37/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: -9.2423 - acc: 0.0025 - val_loss: -9.2957 - val_acc: 0.0000e+00\n",
      "Epoch 37 - Validation Loss: 0.8269\n",
      "Epoch 37 - loss of target layers: 0.8269\n",
      "Epoch 37 - loss of trojan layers: 0.9423\n",
      "Epoch 37 - robust of target layers: 0.8269\n",
      "Epoch 37 - robust of trojan layers: 0.0962\n",
      "Epoch 38/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: -9.2485 - acc: 0.0025 - val_loss: -9.2550 - val_acc: 0.0000e+00\n",
      "Epoch 38 - Validation Loss: 0.8269\n",
      "Epoch 38 - loss of target layers: 0.8269\n",
      "Epoch 38 - loss of trojan layers: 0.9423\n",
      "Epoch 38 - robust of target layers: 0.8269\n",
      "Epoch 38 - robust of trojan layers: 0.1154\n",
      "Epoch 39/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: -9.3408 - acc: 0.0000e+00 - val_loss: -8.7758 - val_acc: 0.0000e+00\n",
      "Epoch 39 - Validation Loss: 0.9038\n",
      "Epoch 39 - loss of target layers: 0.9038\n",
      "Epoch 39 - loss of trojan layers: 0.9423\n",
      "Epoch 39 - robust of target layers: 0.9038\n",
      "Epoch 39 - robust of trojan layers: 0.1154\n",
      "Epoch 40/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: -9.2337 - acc: 0.0000e+00 - val_loss: -9.5970 - val_acc: 0.0000e+00\n",
      "Epoch 40 - Validation Loss: 0.9038\n",
      "Epoch 40 - loss of target layers: 0.9038\n",
      "Epoch 40 - loss of trojan layers: 0.9423\n",
      "Epoch 40 - robust of target layers: 0.9038\n",
      "Epoch 40 - robust of trojan layers: 0.0962\n",
      "Epoch 1/40\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 9.3260 - acc: 0.0425 - val_loss: 9.2964 - val_acc: 0.0600\n",
      "Epoch 1 - Validation Loss: 0.8846\n",
      "Epoch 1 - Validation loss_nat_1: 0.1154\n",
      "Epoch 1 - Validation loss_nat_2: 0.0385\n",
      "Epoch 1 - Validation loss_1: 0.8846\n",
      "Epoch 1 - Validation loss_2: 0.9423\n",
      "Epoch 2/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 9.0860 - acc: 0.0650 - val_loss: 9.6033 - val_acc: 0.0200\n",
      "Epoch 2 - Validation Loss: 0.9231\n",
      "Epoch 2 - Validation loss_nat_1: 0.0385\n",
      "Epoch 2 - Validation loss_nat_2: 0.0769\n",
      "Epoch 2 - Validation loss_1: 0.9231\n",
      "Epoch 2 - Validation loss_2: 0.9231\n",
      "Epoch 3/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 9.2776 - acc: 0.0525 - val_loss: 9.2962 - val_acc: 0.0500\n",
      "Epoch 3 - Validation Loss: 0.9423\n",
      "Epoch 3 - Validation loss_nat_1: 0.0192\n",
      "Epoch 3 - Validation loss_nat_2: 0.0192\n",
      "Epoch 3 - Validation loss_1: 0.9423\n",
      "Epoch 3 - Validation loss_2: 0.9231\n",
      "Epoch 4/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 8.9741 - acc: 0.0650 - val_loss: 9.7483 - val_acc: 0.0100\n",
      "Epoch 4 - Validation Loss: 0.9423\n",
      "Epoch 4 - Validation loss_nat_1: 0.0192\n",
      "Epoch 4 - Validation loss_nat_2: 0.1154\n",
      "Epoch 4 - Validation loss_1: 0.9423\n",
      "Epoch 4 - Validation loss_2: 0.9231\n",
      "Epoch 5/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 9.1947 - acc: 0.0525 - val_loss: 9.5440 - val_acc: 0.0200\n",
      "Epoch 5 - Validation Loss: 0.9423\n",
      "Epoch 5 - Validation loss_nat_1: 0.0385\n",
      "Epoch 5 - Validation loss_nat_2: 0.0577\n",
      "Epoch 5 - Validation loss_1: 0.9423\n",
      "Epoch 5 - Validation loss_2: 0.9231\n",
      "Epoch 6/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 9.0418 - acc: 0.0675 - val_loss: 9.0230 - val_acc: 0.0800\n",
      "Epoch 6 - Validation Loss: 0.9423\n",
      "Epoch 6 - Validation loss_nat_1: 0.0385\n",
      "Epoch 6 - Validation loss_nat_2: 0.0385\n",
      "Epoch 6 - Validation loss_1: 0.9423\n",
      "Epoch 6 - Validation loss_2: 0.9423\n",
      "Epoch 7/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 8.8674 - acc: 0.0875 - val_loss: 9.1639 - val_acc: 0.0500\n",
      "Epoch 7 - Validation Loss: 0.9423\n",
      "Epoch 7 - Validation loss_nat_1: 0.0385\n",
      "Epoch 7 - Validation loss_nat_2: 0.0577\n",
      "Epoch 7 - Validation loss_1: 0.9423\n",
      "Epoch 7 - Validation loss_2: 0.9231\n",
      "Epoch 8/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 8.6753 - acc: 0.0950 - val_loss: 9.7455 - val_acc: 0.0100\n",
      "Epoch 8 - Validation Loss: 0.9423\n",
      "Epoch 8 - Validation loss_nat_1: 0.0385\n",
      "Epoch 8 - Validation loss_nat_2: 0.0577\n",
      "Epoch 8 - Validation loss_1: 0.9423\n",
      "Epoch 8 - Validation loss_2: 0.9231\n",
      "Epoch 9/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 8.9636 - acc: 0.0725 - val_loss: 9.0522 - val_acc: 0.0800\n",
      "Epoch 9 - Validation Loss: 0.9423\n",
      "Epoch 9 - Validation loss_nat_1: 0.0385\n",
      "Epoch 9 - Validation loss_nat_2: 0.0577\n",
      "Epoch 9 - Validation loss_1: 0.9423\n",
      "Epoch 9 - Validation loss_2: 0.9231\n",
      "Epoch 10/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 8.4534 - acc: 0.1300 - val_loss: 9.5495 - val_acc: 0.0300\n",
      "Epoch 10 - Validation Loss: 0.9423\n",
      "Epoch 10 - Validation loss_nat_1: 0.0577\n",
      "Epoch 10 - Validation loss_nat_2: 0.0577\n",
      "Epoch 10 - Validation loss_1: 0.9423\n",
      "Epoch 10 - Validation loss_2: 0.9231\n",
      "Epoch 11/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 8.5971 - acc: 0.1100 - val_loss: 9.0199 - val_acc: 0.0800\n",
      "Epoch 11 - Validation Loss: 0.9423\n",
      "Epoch 11 - Validation loss_nat_1: 0.0577\n",
      "Epoch 11 - Validation loss_nat_2: 0.0577\n",
      "Epoch 11 - Validation loss_1: 0.9423\n",
      "Epoch 11 - Validation loss_2: 0.9231\n",
      "Epoch 12/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 8.3983 - acc: 0.1325 - val_loss: 8.9049 - val_acc: 0.1000\n",
      "Epoch 12 - Validation Loss: 0.9423\n",
      "Epoch 12 - Validation loss_nat_1: 0.0577\n",
      "Epoch 12 - Validation loss_nat_2: 0.0577\n",
      "Epoch 12 - Validation loss_1: 0.9423\n",
      "Epoch 12 - Validation loss_2: 0.9231\n",
      "Epoch 13/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 8.4285 - acc: 0.1300 - val_loss: 8.9284 - val_acc: 0.0900\n",
      "Epoch 13 - Validation Loss: 0.9231\n",
      "Epoch 13 - Validation loss_nat_1: 0.0962\n",
      "Epoch 13 - Validation loss_nat_2: 0.0577\n",
      "Epoch 13 - Validation loss_1: 0.9231\n",
      "Epoch 13 - Validation loss_2: 0.9231\n",
      "Epoch 14/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 8.3396 - acc: 0.1275 - val_loss: 8.8586 - val_acc: 0.0900\n",
      "Epoch 14 - Validation Loss: 0.9231\n",
      "Epoch 14 - Validation loss_nat_1: 0.0962\n",
      "Epoch 14 - Validation loss_nat_2: 0.0385\n",
      "Epoch 14 - Validation loss_1: 0.9231\n",
      "Epoch 14 - Validation loss_2: 0.9423\n",
      "Epoch 15/40\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 8.2168 - acc: 0.1400 - val_loss: 9.1650 - val_acc: 0.0600\n",
      "Epoch 15 - Validation Loss: 0.9231\n",
      "Epoch 15 - Validation loss_nat_1: 0.0962\n",
      "Epoch 15 - Validation loss_nat_2: 0.0192\n",
      "Epoch 15 - Validation loss_1: 0.9231\n",
      "Epoch 15 - Validation loss_2: 0.9231\n",
      "Epoch 16/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 7.9729 - acc: 0.1625 - val_loss: 9.1361 - val_acc: 0.0600\n",
      "Epoch 16 - Validation Loss: 0.9231\n",
      "Epoch 16 - Validation loss_nat_1: 0.0962\n",
      "Epoch 16 - Validation loss_nat_2: 0.0577\n",
      "Epoch 16 - Validation loss_1: 0.9231\n",
      "Epoch 16 - Validation loss_2: 0.9231\n",
      "Epoch 17/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 8.2969 - acc: 0.1325 - val_loss: 8.9107 - val_acc: 0.0700\n",
      "Epoch 17 - Validation Loss: 0.9038\n",
      "Epoch 17 - Validation loss_nat_1: 0.0962\n",
      "Epoch 17 - Validation loss_nat_2: 0.0577\n",
      "Epoch 17 - Validation loss_1: 0.9038\n",
      "Epoch 17 - Validation loss_2: 0.9231\n",
      "Epoch 18/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 7.8327 - acc: 0.1700 - val_loss: 7.9063 - val_acc: 0.1200\n",
      "Epoch 18 - Validation Loss: 0.9038\n",
      "Epoch 18 - Validation loss_nat_1: 0.0962\n",
      "Epoch 18 - Validation loss_nat_2: 0.0385\n",
      "Epoch 18 - Validation loss_1: 0.9038\n",
      "Epoch 18 - Validation loss_2: 0.9423\n",
      "Epoch 19/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 7.5006 - acc: 0.2000 - val_loss: 8.0456 - val_acc: 0.0900\n",
      "Epoch 19 - Validation Loss: 0.8077\n",
      "Epoch 19 - Validation loss_nat_1: 0.1923\n",
      "Epoch 19 - Validation loss_nat_2: 0.0769\n",
      "Epoch 19 - Validation loss_1: 0.8077\n",
      "Epoch 19 - Validation loss_2: 0.9423\n",
      "Epoch 20/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 6.9810 - acc: 0.2675 - val_loss: 7.4659 - val_acc: 0.1700\n",
      "Epoch 20 - Validation Loss: 0.7692\n",
      "Epoch 20 - Validation loss_nat_1: 0.2308\n",
      "Epoch 20 - Validation loss_nat_2: 0.0962\n",
      "Epoch 20 - Validation loss_1: 0.7692\n",
      "Epoch 20 - Validation loss_2: 0.9423\n",
      "Epoch 21/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 6.7019 - acc: 0.2700 - val_loss: 5.8909 - val_acc: 0.3300\n",
      "Epoch 21 - Validation Loss: 0.6538\n",
      "Epoch 21 - Validation loss_nat_1: 0.3269\n",
      "Epoch 21 - Validation loss_nat_2: 0.1154\n",
      "Epoch 21 - Validation loss_1: 0.6538\n",
      "Epoch 21 - Validation loss_2: 0.9423\n",
      "Epoch 22/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 5.4021 - acc: 0.4050 - val_loss: 4.8961 - val_acc: 0.4700\n",
      "Epoch 22 - Validation Loss: 0.5962\n",
      "Epoch 22 - Validation loss_nat_1: 0.4615\n",
      "Epoch 22 - Validation loss_nat_2: 0.0577\n",
      "Epoch 22 - Validation loss_1: 0.5962\n",
      "Epoch 22 - Validation loss_2: 0.9423\n",
      "Epoch 23/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 4.3011 - acc: 0.4825 - val_loss: 4.5940 - val_acc: 0.4900\n",
      "Epoch 23 - Validation Loss: 0.5962\n",
      "Epoch 23 - Validation loss_nat_1: 0.6346\n",
      "Epoch 23 - Validation loss_nat_2: 0.0769\n",
      "Epoch 23 - Validation loss_1: 0.5962\n",
      "Epoch 23 - Validation loss_2: 0.9423\n",
      "Epoch 24/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 2.4248 - acc: 0.6925 - val_loss: 2.4888 - val_acc: 0.7000\n",
      "Epoch 24 - Validation Loss: 0.4231\n",
      "Epoch 24 - Validation loss_nat_1: 0.7885\n",
      "Epoch 24 - Validation loss_nat_2: 0.0962\n",
      "Epoch 24 - Validation loss_1: 0.4231\n",
      "Epoch 24 - Validation loss_2: 0.9423\n",
      "Epoch 25/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 1.5970 - acc: 0.7775 - val_loss: 1.6836 - val_acc: 0.7200\n",
      "Epoch 25 - Validation Loss: 0.2692\n",
      "Epoch 25 - Validation loss_nat_1: 0.8846\n",
      "Epoch 25 - Validation loss_nat_2: 0.0962\n",
      "Epoch 25 - Validation loss_1: 0.2692\n",
      "Epoch 25 - Validation loss_2: 0.9423\n",
      "Epoch 26/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 0.6895 - acc: 0.8800 - val_loss: 1.1921 - val_acc: 0.8300\n",
      "Epoch 26 - Validation Loss: 0.0962\n",
      "Epoch 26 - Validation loss_nat_1: 0.9231\n",
      "Epoch 26 - Validation loss_nat_2: 0.2308\n",
      "Epoch 26 - Validation loss_1: 0.0962\n",
      "Epoch 26 - Validation loss_2: 0.9038\n",
      "Epoch 27/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 0.4304 - acc: 0.9250 - val_loss: 0.9762 - val_acc: 0.8700\n",
      "Epoch 27 - Validation Loss: 0.0192\n",
      "Epoch 27 - Validation loss_nat_1: 0.9423\n",
      "Epoch 27 - Validation loss_nat_2: 0.2308\n",
      "Epoch 27 - Validation loss_1: 0.0192\n",
      "Epoch 27 - Validation loss_2: 0.9038\n",
      "Epoch 28/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 0.2911 - acc: 0.9475 - val_loss: 0.3416 - val_acc: 0.9100\n",
      "Epoch 28 - Validation Loss: 0.0385\n",
      "Epoch 28 - Validation loss_nat_1: 0.9231\n",
      "Epoch 28 - Validation loss_nat_2: 0.2115\n",
      "Epoch 28 - Validation loss_1: 0.0385\n",
      "Epoch 28 - Validation loss_2: 0.9038\n",
      "Epoch 29/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 0.2689 - acc: 0.9500 - val_loss: 0.3968 - val_acc: 0.9300\n",
      "Epoch 29 - Validation Loss: 0.0000\n",
      "Epoch 29 - Validation loss_nat_1: 0.9615\n",
      "Epoch 29 - Validation loss_nat_2: 0.2500\n",
      "Epoch 29 - Validation loss_1: 0.0000\n",
      "Epoch 29 - Validation loss_2: 0.9038\n",
      "Epoch 30/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 0.3444 - acc: 0.9475 - val_loss: 0.1016 - val_acc: 0.9900\n",
      "Epoch 30 - Validation Loss: 0.0192\n",
      "Epoch 30 - Validation loss_nat_1: 0.9423\n",
      "Epoch 30 - Validation loss_nat_2: 0.2308\n",
      "Epoch 30 - Validation loss_1: 0.0192\n",
      "Epoch 30 - Validation loss_2: 0.9038\n",
      "Epoch 31/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 0.2065 - acc: 0.9575 - val_loss: 0.2197 - val_acc: 0.9600\n",
      "Epoch 31 - Validation Loss: 0.0000\n",
      "Epoch 31 - Validation loss_nat_1: 0.9615\n",
      "Epoch 31 - Validation loss_nat_2: 0.2500\n",
      "Epoch 31 - Validation loss_1: 0.0000\n",
      "Epoch 31 - Validation loss_2: 0.9038\n",
      "Epoch 32/40\n",
      "100/100 [==============================] - 10s 102ms/step - loss: 0.1495 - acc: 0.9750 - val_loss: 0.4767 - val_acc: 0.9200\n",
      "Epoch 32 - Validation Loss: 0.0000\n",
      "Epoch 32 - Validation loss_nat_1: 0.9615\n",
      "Epoch 32 - Validation loss_nat_2: 0.1923\n",
      "Epoch 32 - Validation loss_1: 0.0000\n",
      "Epoch 32 - Validation loss_2: 0.9038\n",
      "Epoch 33/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 0.1877 - acc: 0.9750 - val_loss: 0.2170 - val_acc: 0.9500\n",
      "Epoch 33 - Validation Loss: 0.0000\n",
      "Epoch 33 - Validation loss_nat_1: 0.9615\n",
      "Epoch 33 - Validation loss_nat_2: 0.2308\n",
      "Epoch 33 - Validation loss_1: 0.0000\n",
      "Epoch 33 - Validation loss_2: 0.9038\n",
      "Epoch 34/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 0.2177 - acc: 0.9725 - val_loss: 0.5168 - val_acc: 0.9400\n",
      "Epoch 34 - Validation Loss: 0.0000\n",
      "Epoch 34 - Validation loss_nat_1: 0.9615\n",
      "Epoch 34 - Validation loss_nat_2: 0.2308\n",
      "Epoch 34 - Validation loss_1: 0.0000\n",
      "Epoch 34 - Validation loss_2: 0.9038\n",
      "Epoch 35/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 0.2180 - acc: 0.9525 - val_loss: 0.2603 - val_acc: 0.9700\n",
      "Epoch 35 - Validation Loss: 0.0000\n",
      "Epoch 35 - Validation loss_nat_1: 0.9615\n",
      "Epoch 35 - Validation loss_nat_2: 0.2115\n",
      "Epoch 35 - Validation loss_1: 0.0000\n",
      "Epoch 35 - Validation loss_2: 0.9038\n",
      "Epoch 36/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 0.1141 - acc: 0.9825 - val_loss: 0.3289 - val_acc: 0.9400\n",
      "Epoch 36 - Validation Loss: 0.0000\n",
      "Epoch 36 - Validation loss_nat_1: 0.9615\n",
      "Epoch 36 - Validation loss_nat_2: 0.2308\n",
      "Epoch 36 - Validation loss_1: 0.0000\n",
      "Epoch 36 - Validation loss_2: 0.9231\n",
      "Epoch 37/40\n",
      "100/100 [==============================] - 10s 102ms/step - loss: 0.2427 - acc: 0.9675 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 37 - Validation Loss: 0.0000\n",
      "Epoch 37 - Validation loss_nat_1: 0.9615\n",
      "Epoch 37 - Validation loss_nat_2: 0.2500\n",
      "Epoch 37 - Validation loss_1: 0.0000\n",
      "Epoch 37 - Validation loss_2: 0.9038\n",
      "Epoch 38/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 0.0921 - acc: 0.9800 - val_loss: 0.3613 - val_acc: 0.9600\n",
      "Epoch 38 - Validation Loss: 0.0000\n",
      "Epoch 38 - Validation loss_nat_1: 0.9615\n",
      "Epoch 38 - Validation loss_nat_2: 0.2115\n",
      "Epoch 38 - Validation loss_1: 0.0000\n",
      "Epoch 38 - Validation loss_2: 0.9038\n",
      "Epoch 39/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 0.1746 - acc: 0.9750 - val_loss: 0.2802 - val_acc: 0.9500\n",
      "Epoch 39 - Validation Loss: 0.0000\n",
      "Epoch 39 - Validation loss_nat_1: 0.9615\n",
      "Epoch 39 - Validation loss_nat_2: 0.2500\n",
      "Epoch 39 - Validation loss_1: 0.0000\n",
      "Epoch 39 - Validation loss_2: 0.9038\n",
      "Epoch 40/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 0.1214 - acc: 0.9725 - val_loss: 0.1358 - val_acc: 0.9800\n",
      "Epoch 40 - Validation Loss: 0.0000\n",
      "Epoch 40 - Validation loss_nat_1: 0.9615\n",
      "Epoch 40 - Validation loss_nat_2: 0.1731\n",
      "Epoch 40 - Validation loss_1: 0.0000\n",
      "Epoch 40 - Validation loss_2: 0.9038\n",
      "perturbed_model done\n",
      "Epoch 1/40\n",
      "100/100 [==============================] - 45s 452ms/step - loss: -1.4168 - acc: 0.2825 - val_loss: -0.6811 - val_acc: 0.3400\n",
      "Epoch 1 - Validation Loss: 0.4038\n",
      "Epoch 1 - loss of target layers: 0.4038\n",
      "Epoch 1 - loss of trojan layers: 0.9231\n",
      "Epoch 1 - robust of target layers: 0.4038\n",
      "Epoch 1 - robust of trojan layers: 0.1154\n",
      "Epoch 2/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -2.6534 - acc: 0.3275 - val_loss: -1.8403 - val_acc: 0.3900\n",
      "Epoch 2 - Validation Loss: 0.4615\n",
      "Epoch 2 - loss of target layers: 0.4615\n",
      "Epoch 2 - loss of trojan layers: 0.9231\n",
      "Epoch 2 - robust of target layers: 0.4615\n",
      "Epoch 2 - robust of trojan layers: 0.0192\n",
      "Epoch 3/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -3.2226 - acc: 0.3200 - val_loss: -2.4421 - val_acc: 0.4000\n",
      "Epoch 3 - Validation Loss: 0.5385\n",
      "Epoch 3 - loss of target layers: 0.5385\n",
      "Epoch 3 - loss of trojan layers: 0.9231\n",
      "Epoch 3 - robust of target layers: 0.5385\n",
      "Epoch 3 - robust of trojan layers: 0.0192\n",
      "Epoch 4/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -3.5159 - acc: 0.3475 - val_loss: -3.0542 - val_acc: 0.4800\n",
      "Epoch 4 - Validation Loss: 0.5385\n",
      "Epoch 4 - loss of target layers: 0.5385\n",
      "Epoch 4 - loss of trojan layers: 0.9231\n",
      "Epoch 4 - robust of target layers: 0.5385\n",
      "Epoch 4 - robust of trojan layers: 0.0192\n",
      "Epoch 5/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: -3.6472 - acc: 0.3825 - val_loss: -2.2214 - val_acc: 0.4400\n",
      "Epoch 5 - Validation Loss: 0.5577\n",
      "Epoch 5 - loss of target layers: 0.5577\n",
      "Epoch 5 - loss of trojan layers: 0.9231\n",
      "Epoch 5 - robust of target layers: 0.5577\n",
      "Epoch 5 - robust of trojan layers: 0.0192\n",
      "Epoch 6/40\n",
      "100/100 [==============================] - 10s 103ms/step - loss: -4.4144 - acc: 0.3900 - val_loss: -3.4862 - val_acc: 0.4600\n",
      "Epoch 6 - Validation Loss: 0.6731\n",
      "Epoch 6 - loss of target layers: 0.6731\n",
      "Epoch 6 - loss of trojan layers: 0.9231\n",
      "Epoch 6 - robust of target layers: 0.6731\n",
      "Epoch 6 - robust of trojan layers: 0.0385\n",
      "Epoch 7/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -4.5282 - acc: 0.4225 - val_loss: -2.7372 - val_acc: 0.5100\n",
      "Epoch 7 - Validation Loss: 0.7500\n",
      "Epoch 7 - loss of target layers: 0.7500\n",
      "Epoch 7 - loss of trojan layers: 0.9231\n",
      "Epoch 7 - robust of target layers: 0.7500\n",
      "Epoch 7 - robust of trojan layers: 0.0192\n",
      "Epoch 8/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -5.4818 - acc: 0.4375 - val_loss: -4.2740 - val_acc: 0.3700\n",
      "Epoch 8 - Validation Loss: 0.7308\n",
      "Epoch 8 - loss of target layers: 0.7308\n",
      "Epoch 8 - loss of trojan layers: 0.9231\n",
      "Epoch 8 - robust of target layers: 0.7308\n",
      "Epoch 8 - robust of trojan layers: 0.0192\n",
      "Epoch 9/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -6.3849 - acc: 0.4475 - val_loss: -5.8267 - val_acc: 0.3700\n",
      "Epoch 9 - Validation Loss: 0.7692\n",
      "Epoch 9 - loss of target layers: 0.7692\n",
      "Epoch 9 - loss of trojan layers: 0.9231\n",
      "Epoch 9 - robust of target layers: 0.7692\n",
      "Epoch 9 - robust of trojan layers: 0.0385\n",
      "Epoch 10/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -6.7396 - acc: 0.4550 - val_loss: -6.5838 - val_acc: 0.4000\n",
      "Epoch 10 - Validation Loss: 0.7500\n",
      "Epoch 10 - loss of target layers: 0.7500\n",
      "Epoch 10 - loss of trojan layers: 0.9231\n",
      "Epoch 10 - robust of target layers: 0.7500\n",
      "Epoch 10 - robust of trojan layers: 0.0192\n",
      "Epoch 11/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: -6.5284 - acc: 0.4675 - val_loss: -5.9308 - val_acc: 0.4200\n",
      "Epoch 11 - Validation Loss: 0.7500\n",
      "Epoch 11 - loss of target layers: 0.7500\n",
      "Epoch 11 - loss of trojan layers: 0.9231\n",
      "Epoch 11 - robust of target layers: 0.7500\n",
      "Epoch 11 - robust of trojan layers: 0.0192\n",
      "Epoch 12/40\n",
      "100/100 [==============================] - 10s 102ms/step - loss: -7.1340 - acc: 0.4725 - val_loss: -6.4385 - val_acc: 0.4700\n",
      "Epoch 12 - Validation Loss: 0.7500\n",
      "Epoch 12 - loss of target layers: 0.7500\n",
      "Epoch 12 - loss of trojan layers: 0.9231\n",
      "Epoch 12 - robust of target layers: 0.7500\n",
      "Epoch 12 - robust of trojan layers: 0.0385\n",
      "Epoch 13/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -6.7692 - acc: 0.4850 - val_loss: -6.7361 - val_acc: 0.4300\n",
      "Epoch 13 - Validation Loss: 0.7500\n",
      "Epoch 13 - loss of target layers: 0.7500\n",
      "Epoch 13 - loss of trojan layers: 0.9231\n",
      "Epoch 13 - robust of target layers: 0.7500\n",
      "Epoch 13 - robust of trojan layers: 0.0385\n",
      "Epoch 14/40\n",
      "100/100 [==============================] - 10s 98ms/step - loss: -7.0032 - acc: 0.4600 - val_loss: -6.6923 - val_acc: 0.3900\n",
      "Epoch 14 - Validation Loss: 0.7500\n",
      "Epoch 14 - loss of target layers: 0.7500\n",
      "Epoch 14 - loss of trojan layers: 0.9231\n",
      "Epoch 14 - robust of target layers: 0.7500\n",
      "Epoch 14 - robust of trojan layers: 0.0577\n",
      "Epoch 15/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -7.1388 - acc: 0.4550 - val_loss: -5.6621 - val_acc: 0.4300\n",
      "Epoch 15 - Validation Loss: 0.7500\n",
      "Epoch 15 - loss of target layers: 0.7500\n",
      "Epoch 15 - loss of trojan layers: 0.9231\n",
      "Epoch 15 - robust of target layers: 0.7500\n",
      "Epoch 15 - robust of trojan layers: 0.0385\n",
      "Epoch 16/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -7.1773 - acc: 0.4575 - val_loss: -6.1904 - val_acc: 0.3000\n",
      "Epoch 16 - Validation Loss: 0.7500\n",
      "Epoch 16 - loss of target layers: 0.7500\n",
      "Epoch 16 - loss of trojan layers: 0.9231\n",
      "Epoch 16 - robust of target layers: 0.7500\n",
      "Epoch 16 - robust of trojan layers: 0.0192\n",
      "Epoch 17/40\n",
      "100/100 [==============================] - 10s 104ms/step - loss: -7.5308 - acc: 0.4850 - val_loss: -6.7003 - val_acc: 0.4000\n",
      "Epoch 17 - Validation Loss: 0.7500\n",
      "Epoch 17 - loss of target layers: 0.7500\n",
      "Epoch 17 - loss of trojan layers: 0.9231\n",
      "Epoch 17 - robust of target layers: 0.7500\n",
      "Epoch 17 - robust of trojan layers: 0.0385\n",
      "Epoch 18/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: -6.9597 - acc: 0.5100 - val_loss: -6.3745 - val_acc: 0.4300\n",
      "Epoch 18 - Validation Loss: 0.7500\n",
      "Epoch 18 - loss of target layers: 0.7500\n",
      "Epoch 18 - loss of trojan layers: 0.9231\n",
      "Epoch 18 - robust of target layers: 0.7500\n",
      "Epoch 18 - robust of trojan layers: 0.0192\n",
      "Epoch 19/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -7.4939 - acc: 0.4800 - val_loss: -6.3338 - val_acc: 0.4500\n",
      "Epoch 19 - Validation Loss: 0.7500\n",
      "Epoch 19 - loss of target layers: 0.7500\n",
      "Epoch 19 - loss of trojan layers: 0.9231\n",
      "Epoch 19 - robust of target layers: 0.7500\n",
      "Epoch 19 - robust of trojan layers: 0.0192\n",
      "Epoch 20/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -7.7475 - acc: 0.4675 - val_loss: -6.2039 - val_acc: 0.3600\n",
      "Epoch 20 - Validation Loss: 0.7500\n",
      "Epoch 20 - loss of target layers: 0.7500\n",
      "Epoch 20 - loss of trojan layers: 0.9231\n",
      "Epoch 20 - robust of target layers: 0.7500\n",
      "Epoch 20 - robust of trojan layers: 0.0192\n",
      "Epoch 21/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -7.5228 - acc: 0.4725 - val_loss: -6.4552 - val_acc: 0.4100\n",
      "Epoch 21 - Validation Loss: 0.7692\n",
      "Epoch 21 - loss of target layers: 0.7692\n",
      "Epoch 21 - loss of trojan layers: 0.9231\n",
      "Epoch 21 - robust of target layers: 0.7692\n",
      "Epoch 21 - robust of trojan layers: 0.0192\n",
      "Epoch 22/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -7.5639 - acc: 0.4425 - val_loss: -6.8496 - val_acc: 0.4500\n",
      "Epoch 22 - Validation Loss: 0.7692\n",
      "Epoch 22 - loss of target layers: 0.7692\n",
      "Epoch 22 - loss of trojan layers: 0.9231\n",
      "Epoch 22 - robust of target layers: 0.7692\n",
      "Epoch 22 - robust of trojan layers: 0.0192\n",
      "Epoch 23/40\n",
      "100/100 [==============================] - 10s 105ms/step - loss: -8.0814 - acc: 0.4525 - val_loss: -7.4505 - val_acc: 0.3100\n",
      "Epoch 23 - Validation Loss: 0.7692\n",
      "Epoch 23 - loss of target layers: 0.7692\n",
      "Epoch 23 - loss of trojan layers: 0.9231\n",
      "Epoch 23 - robust of target layers: 0.7692\n",
      "Epoch 23 - robust of trojan layers: 0.0192\n",
      "Epoch 24/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -7.8098 - acc: 0.4575 - val_loss: -7.2707 - val_acc: 0.3200\n",
      "Epoch 24 - Validation Loss: 0.7885\n",
      "Epoch 24 - loss of target layers: 0.7885\n",
      "Epoch 24 - loss of trojan layers: 0.9231\n",
      "Epoch 24 - robust of target layers: 0.7885\n",
      "Epoch 24 - robust of trojan layers: 0.0385\n",
      "Epoch 25/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: -8.0759 - acc: 0.4625 - val_loss: -7.8500 - val_acc: 0.3000\n",
      "Epoch 25 - Validation Loss: 0.7885\n",
      "Epoch 25 - loss of target layers: 0.7885\n",
      "Epoch 25 - loss of trojan layers: 0.9231\n",
      "Epoch 25 - robust of target layers: 0.7885\n",
      "Epoch 25 - robust of trojan layers: 0.0577\n",
      "Epoch 26/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: -7.6592 - acc: 0.4425 - val_loss: -7.9104 - val_acc: 0.3300\n",
      "Epoch 26 - Validation Loss: 0.7885\n",
      "Epoch 26 - loss of target layers: 0.7885\n",
      "Epoch 26 - loss of trojan layers: 0.9231\n",
      "Epoch 26 - robust of target layers: 0.7885\n",
      "Epoch 26 - robust of trojan layers: 0.0192\n",
      "Epoch 27/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: -8.1893 - acc: 0.4300 - val_loss: -7.2431 - val_acc: 0.2900\n",
      "Epoch 27 - Validation Loss: 0.7885\n",
      "Epoch 27 - loss of target layers: 0.7885\n",
      "Epoch 27 - loss of trojan layers: 0.9231\n",
      "Epoch 27 - robust of target layers: 0.7885\n",
      "Epoch 27 - robust of trojan layers: 0.0192\n",
      "Epoch 28/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: -8.4016 - acc: 0.4075 - val_loss: -7.5491 - val_acc: 0.2200\n",
      "Epoch 28 - Validation Loss: 0.8654\n",
      "Epoch 28 - loss of target layers: 0.8654\n",
      "Epoch 28 - loss of trojan layers: 0.9231\n",
      "Epoch 28 - robust of target layers: 0.8654\n",
      "Epoch 28 - robust of trojan layers: 0.0577\n",
      "Epoch 29/40\n",
      "100/100 [==============================] - 10s 104ms/step - loss: -8.1420 - acc: 0.3925 - val_loss: -8.4527 - val_acc: 0.2300\n",
      "Epoch 29 - Validation Loss: 0.9038\n",
      "Epoch 29 - loss of target layers: 0.9038\n",
      "Epoch 29 - loss of trojan layers: 0.9231\n",
      "Epoch 29 - robust of target layers: 0.9038\n",
      "Epoch 29 - robust of trojan layers: 0.0192\n",
      "Epoch 30/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: -8.6171 - acc: 0.3900 - val_loss: -8.5571 - val_acc: 0.2900\n",
      "Epoch 30 - Validation Loss: 0.9423\n",
      "Epoch 30 - loss of target layers: 0.9423\n",
      "Epoch 30 - loss of trojan layers: 0.9038\n",
      "Epoch 30 - robust of target layers: 0.9423\n",
      "Epoch 30 - robust of trojan layers: 0.0000\n",
      "Epoch 31/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -8.5595 - acc: 0.3825 - val_loss: -8.2509 - val_acc: 0.2300\n",
      "Epoch 31 - Validation Loss: 0.9615\n",
      "Epoch 31 - loss of target layers: 0.9615\n",
      "Epoch 31 - loss of trojan layers: 0.9231\n",
      "Epoch 31 - robust of target layers: 0.9615\n",
      "Epoch 31 - robust of trojan layers: 0.0192\n",
      "Epoch 32/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -8.7822 - acc: 0.3600 - val_loss: -8.2638 - val_acc: 0.2300\n",
      "Epoch 32 - Validation Loss: 0.9615\n",
      "Epoch 32 - loss of target layers: 0.9615\n",
      "Epoch 32 - loss of trojan layers: 0.9231\n",
      "Epoch 32 - robust of target layers: 0.9615\n",
      "Epoch 32 - robust of trojan layers: 0.0192\n",
      "Epoch 33/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -8.8729 - acc: 0.3375 - val_loss: -8.7812 - val_acc: 0.1700\n",
      "Epoch 33 - Validation Loss: 0.9615\n",
      "Epoch 33 - loss of target layers: 0.9615\n",
      "Epoch 33 - loss of trojan layers: 0.9231\n",
      "Epoch 33 - robust of target layers: 0.9615\n",
      "Epoch 33 - robust of trojan layers: 0.0577\n",
      "Epoch 34/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -8.8770 - acc: 0.3375 - val_loss: -8.2355 - val_acc: 0.2200\n",
      "Epoch 34 - Validation Loss: 0.9615\n",
      "Epoch 34 - loss of target layers: 0.9615\n",
      "Epoch 34 - loss of trojan layers: 0.9231\n",
      "Epoch 34 - robust of target layers: 0.9615\n",
      "Epoch 34 - robust of trojan layers: 0.0577\n",
      "Epoch 35/40\n",
      "100/100 [==============================] - 10s 102ms/step - loss: -9.0024 - acc: 0.3575 - val_loss: -8.2920 - val_acc: 0.1800\n",
      "Epoch 35 - Validation Loss: 0.9808\n",
      "Epoch 35 - loss of target layers: 0.9808\n",
      "Epoch 35 - loss of trojan layers: 0.9615\n",
      "Epoch 35 - robust of target layers: 0.9808\n",
      "Epoch 35 - robust of trojan layers: 0.0769\n",
      "Epoch 36/40\n",
      "100/100 [==============================] - 10s 102ms/step - loss: -8.9876 - acc: 0.3450 - val_loss: -9.1730 - val_acc: 0.1600\n",
      "Epoch 36 - Validation Loss: 0.9808\n",
      "Epoch 36 - loss of target layers: 0.9808\n",
      "Epoch 36 - loss of trojan layers: 0.9231\n",
      "Epoch 36 - robust of target layers: 0.9808\n",
      "Epoch 36 - robust of trojan layers: 0.0192\n",
      "Epoch 37/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: -9.1079 - acc: 0.3275 - val_loss: -9.2731 - val_acc: 0.1200\n",
      "Epoch 37 - Validation Loss: 0.9808\n",
      "Epoch 37 - loss of target layers: 0.9808\n",
      "Epoch 37 - loss of trojan layers: 0.9231\n",
      "Epoch 37 - robust of target layers: 0.9808\n",
      "Epoch 37 - robust of trojan layers: 0.0192\n",
      "Epoch 38/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: -9.1661 - acc: 0.3425 - val_loss: -9.0611 - val_acc: 0.1800\n",
      "Epoch 38 - Validation Loss: 0.9615\n",
      "Epoch 38 - loss of target layers: 0.9615\n",
      "Epoch 38 - loss of trojan layers: 0.9231\n",
      "Epoch 38 - robust of target layers: 0.9615\n",
      "Epoch 38 - robust of trojan layers: 0.0192\n",
      "Epoch 39/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: -9.4630 - acc: 0.3425 - val_loss: -9.1167 - val_acc: 0.2500\n",
      "Epoch 39 - Validation Loss: 1.0000\n",
      "Epoch 39 - loss of target layers: 1.0000\n",
      "Epoch 39 - loss of trojan layers: 0.9231\n",
      "Epoch 39 - robust of target layers: 1.0000\n",
      "Epoch 39 - robust of trojan layers: 0.0192\n",
      "Epoch 40/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: -9.1572 - acc: 0.3475 - val_loss: -9.1734 - val_acc: 0.1700\n",
      "Epoch 40 - Validation Loss: 0.9615\n",
      "Epoch 40 - loss of target layers: 0.9615\n",
      "Epoch 40 - loss of trojan layers: 0.9231\n",
      "Epoch 40 - robust of target layers: 0.9615\n",
      "Epoch 40 - robust of trojan layers: 0.0192\n",
      "Epoch 1/40\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 9.2003 - acc: 0.0325 - val_loss: 7.9406 - val_acc: 0.1500\n",
      "Epoch 1 - Validation Loss: 0.8462\n",
      "Epoch 1 - Validation loss_nat_1: 0.2885\n",
      "Epoch 1 - Validation loss_nat_2: 0.1538\n",
      "Epoch 1 - Validation loss_1: 0.8462\n",
      "Epoch 1 - Validation loss_2: 0.8846\n",
      "Epoch 2/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 9.2115 - acc: 0.0450 - val_loss: 9.2712 - val_acc: 0.0500\n",
      "Epoch 2 - Validation Loss: 0.9615\n",
      "Epoch 2 - Validation loss_nat_1: 0.0769\n",
      "Epoch 2 - Validation loss_nat_2: 0.0192\n",
      "Epoch 2 - Validation loss_1: 0.9615\n",
      "Epoch 2 - Validation loss_2: 0.9231\n",
      "Epoch 3/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 8.9973 - acc: 0.0400 - val_loss: 9.2217 - val_acc: 0.0300\n",
      "Epoch 3 - Validation Loss: 0.9808\n",
      "Epoch 3 - Validation loss_nat_1: 0.0385\n",
      "Epoch 3 - Validation loss_nat_2: 0.0000\n",
      "Epoch 3 - Validation loss_1: 0.9808\n",
      "Epoch 3 - Validation loss_2: 0.9231\n",
      "Epoch 4/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 9.0147 - acc: 0.0500 - val_loss: 8.8403 - val_acc: 0.0600\n",
      "Epoch 4 - Validation Loss: 0.9808\n",
      "Epoch 4 - Validation loss_nat_1: 0.0385\n",
      "Epoch 4 - Validation loss_nat_2: 0.0000\n",
      "Epoch 4 - Validation loss_1: 0.9808\n",
      "Epoch 4 - Validation loss_2: 0.9231\n",
      "Epoch 5/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 8.9500 - acc: 0.0550 - val_loss: 9.1028 - val_acc: 0.0500\n",
      "Epoch 5 - Validation Loss: 1.0000\n",
      "Epoch 5 - Validation loss_nat_1: 0.0192\n",
      "Epoch 5 - Validation loss_nat_2: 0.0192\n",
      "Epoch 5 - Validation loss_1: 1.0000\n",
      "Epoch 5 - Validation loss_2: 0.9038\n",
      "Epoch 6/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 8.7076 - acc: 0.0850 - val_loss: 9.1244 - val_acc: 0.0600\n",
      "Epoch 6 - Validation Loss: 0.9808\n",
      "Epoch 6 - Validation loss_nat_1: 0.0385\n",
      "Epoch 6 - Validation loss_nat_2: 0.0000\n",
      "Epoch 6 - Validation loss_1: 0.9808\n",
      "Epoch 6 - Validation loss_2: 0.9231\n",
      "Epoch 7/40\n",
      "100/100 [==============================] - 10s 102ms/step - loss: 9.1308 - acc: 0.0450 - val_loss: 9.1508 - val_acc: 0.0600\n",
      "Epoch 7 - Validation Loss: 0.9808\n",
      "Epoch 7 - Validation loss_nat_1: 0.0385\n",
      "Epoch 7 - Validation loss_nat_2: 0.0385\n",
      "Epoch 7 - Validation loss_1: 0.9808\n",
      "Epoch 7 - Validation loss_2: 0.9231\n",
      "Epoch 8/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 8.5689 - acc: 0.0925 - val_loss: 9.0990 - val_acc: 0.0400\n",
      "Epoch 8 - Validation Loss: 0.9808\n",
      "Epoch 8 - Validation loss_nat_1: 0.0385\n",
      "Epoch 8 - Validation loss_nat_2: 0.0000\n",
      "Epoch 8 - Validation loss_1: 0.9808\n",
      "Epoch 8 - Validation loss_2: 0.9231\n",
      "Epoch 9/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 8.4976 - acc: 0.1100 - val_loss: 8.6723 - val_acc: 0.1100\n",
      "Epoch 9 - Validation Loss: 0.9808\n",
      "Epoch 9 - Validation loss_nat_1: 0.0385\n",
      "Epoch 9 - Validation loss_nat_2: 0.0000\n",
      "Epoch 9 - Validation loss_1: 0.9808\n",
      "Epoch 9 - Validation loss_2: 0.9231\n",
      "Epoch 10/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 8.2121 - acc: 0.1275 - val_loss: 8.1662 - val_acc: 0.1600\n",
      "Epoch 10 - Validation Loss: 0.9808\n",
      "Epoch 10 - Validation loss_nat_1: 0.0769\n",
      "Epoch 10 - Validation loss_nat_2: 0.0000\n",
      "Epoch 10 - Validation loss_1: 0.9808\n",
      "Epoch 10 - Validation loss_2: 0.9231\n",
      "Epoch 11/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 8.4802 - acc: 0.0975 - val_loss: 8.1980 - val_acc: 0.1500\n",
      "Epoch 11 - Validation Loss: 0.9808\n",
      "Epoch 11 - Validation loss_nat_1: 0.1154\n",
      "Epoch 11 - Validation loss_nat_2: 0.0192\n",
      "Epoch 11 - Validation loss_1: 0.9808\n",
      "Epoch 11 - Validation loss_2: 0.9423\n",
      "Epoch 12/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 7.3926 - acc: 0.2000 - val_loss: 7.0520 - val_acc: 0.2100\n",
      "Epoch 12 - Validation Loss: 0.9423\n",
      "Epoch 12 - Validation loss_nat_1: 0.2115\n",
      "Epoch 12 - Validation loss_nat_2: 0.0577\n",
      "Epoch 12 - Validation loss_1: 0.9423\n",
      "Epoch 12 - Validation loss_2: 0.9231\n",
      "Epoch 13/40\n",
      "100/100 [==============================] - 10s 104ms/step - loss: 7.4885 - acc: 0.1900 - val_loss: 6.2786 - val_acc: 0.3400\n",
      "Epoch 13 - Validation Loss: 0.8654\n",
      "Epoch 13 - Validation loss_nat_1: 0.2692\n",
      "Epoch 13 - Validation loss_nat_2: 0.0000\n",
      "Epoch 13 - Validation loss_1: 0.8654\n",
      "Epoch 13 - Validation loss_2: 0.9231\n",
      "Epoch 14/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 7.1398 - acc: 0.2275 - val_loss: 5.9544 - val_acc: 0.3800\n",
      "Epoch 14 - Validation Loss: 0.8269\n",
      "Epoch 14 - Validation loss_nat_1: 0.2885\n",
      "Epoch 14 - Validation loss_nat_2: 0.0000\n",
      "Epoch 14 - Validation loss_1: 0.8269\n",
      "Epoch 14 - Validation loss_2: 0.9231\n",
      "Epoch 15/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 6.8901 - acc: 0.2650 - val_loss: 6.5542 - val_acc: 0.3300\n",
      "Epoch 15 - Validation Loss: 0.7692\n",
      "Epoch 15 - Validation loss_nat_1: 0.3462\n",
      "Epoch 15 - Validation loss_nat_2: 0.0000\n",
      "Epoch 15 - Validation loss_1: 0.7692\n",
      "Epoch 15 - Validation loss_2: 0.9231\n",
      "Epoch 16/40\n",
      "100/100 [==============================] - 10s 103ms/step - loss: 6.5829 - acc: 0.2975 - val_loss: 6.2798 - val_acc: 0.3400\n",
      "Epoch 16 - Validation Loss: 0.8077\n",
      "Epoch 16 - Validation loss_nat_1: 0.3269\n",
      "Epoch 16 - Validation loss_nat_2: 0.0577\n",
      "Epoch 16 - Validation loss_1: 0.8077\n",
      "Epoch 16 - Validation loss_2: 0.9231\n",
      "Epoch 17/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 6.7845 - acc: 0.2700 - val_loss: 6.1258 - val_acc: 0.3300\n",
      "Epoch 17 - Validation Loss: 0.7500\n",
      "Epoch 17 - Validation loss_nat_1: 0.3654\n",
      "Epoch 17 - Validation loss_nat_2: 0.0000\n",
      "Epoch 17 - Validation loss_1: 0.7500\n",
      "Epoch 17 - Validation loss_2: 0.9231\n",
      "Epoch 18/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 6.8947 - acc: 0.2625 - val_loss: 4.7751 - val_acc: 0.5000\n",
      "Epoch 18 - Validation Loss: 0.7885\n",
      "Epoch 18 - Validation loss_nat_1: 0.4423\n",
      "Epoch 18 - Validation loss_nat_2: 0.0192\n",
      "Epoch 18 - Validation loss_1: 0.7885\n",
      "Epoch 18 - Validation loss_2: 0.9231\n",
      "Epoch 19/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 6.4934 - acc: 0.2725 - val_loss: 5.3515 - val_acc: 0.4100\n",
      "Epoch 19 - Validation Loss: 0.7115\n",
      "Epoch 19 - Validation loss_nat_1: 0.5769\n",
      "Epoch 19 - Validation loss_nat_2: 0.0385\n",
      "Epoch 19 - Validation loss_1: 0.7115\n",
      "Epoch 19 - Validation loss_2: 0.9231\n",
      "Epoch 20/40\n",
      "100/100 [==============================] - 10s 104ms/step - loss: 5.8425 - acc: 0.3350 - val_loss: 5.6256 - val_acc: 0.3800\n",
      "Epoch 20 - Validation Loss: 0.7308\n",
      "Epoch 20 - Validation loss_nat_1: 0.5000\n",
      "Epoch 20 - Validation loss_nat_2: 0.0000\n",
      "Epoch 20 - Validation loss_1: 0.7308\n",
      "Epoch 20 - Validation loss_2: 0.9231\n",
      "Epoch 21/40\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 5.2055 - acc: 0.3925 - val_loss: 5.1178 - val_acc: 0.4200\n",
      "Epoch 21 - Validation Loss: 0.5385\n",
      "Epoch 21 - Validation loss_nat_1: 0.7500\n",
      "Epoch 21 - Validation loss_nat_2: 0.0000\n",
      "Epoch 21 - Validation loss_1: 0.5385\n",
      "Epoch 21 - Validation loss_2: 0.9231\n",
      "Epoch 22/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 4.3932 - acc: 0.4650 - val_loss: 2.8922 - val_acc: 0.6300\n",
      "Epoch 22 - Validation Loss: 0.4615\n",
      "Epoch 22 - Validation loss_nat_1: 0.8462\n",
      "Epoch 22 - Validation loss_nat_2: 0.0577\n",
      "Epoch 22 - Validation loss_1: 0.4615\n",
      "Epoch 22 - Validation loss_2: 0.9231\n",
      "Epoch 23/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 3.1024 - acc: 0.6125 - val_loss: 2.3865 - val_acc: 0.7000\n",
      "Epoch 23 - Validation Loss: 0.4038\n",
      "Epoch 23 - Validation loss_nat_1: 0.8846\n",
      "Epoch 23 - Validation loss_nat_2: 0.1154\n",
      "Epoch 23 - Validation loss_1: 0.4038\n",
      "Epoch 23 - Validation loss_2: 0.9038\n",
      "Epoch 24/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 2.7970 - acc: 0.6525 - val_loss: 2.6042 - val_acc: 0.6300\n",
      "Epoch 24 - Validation Loss: 0.3654\n",
      "Epoch 24 - Validation loss_nat_1: 0.9038\n",
      "Epoch 24 - Validation loss_nat_2: 0.0000\n",
      "Epoch 24 - Validation loss_1: 0.3654\n",
      "Epoch 24 - Validation loss_2: 0.9231\n",
      "Epoch 25/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 2.1422 - acc: 0.7275 - val_loss: 1.7656 - val_acc: 0.7500\n",
      "Epoch 25 - Validation Loss: 0.3462\n",
      "Epoch 25 - Validation loss_nat_1: 0.9038\n",
      "Epoch 25 - Validation loss_nat_2: 0.0192\n",
      "Epoch 25 - Validation loss_1: 0.3462\n",
      "Epoch 25 - Validation loss_2: 0.9231\n",
      "Epoch 26/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 1.8838 - acc: 0.7525 - val_loss: 1.0049 - val_acc: 0.8900\n",
      "Epoch 26 - Validation Loss: 0.3654\n",
      "Epoch 26 - Validation loss_nat_1: 0.9423\n",
      "Epoch 26 - Validation loss_nat_2: 0.0385\n",
      "Epoch 26 - Validation loss_1: 0.3654\n",
      "Epoch 26 - Validation loss_2: 0.9231\n",
      "Epoch 27/40\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 1.3784 - acc: 0.8100 - val_loss: 0.4850 - val_acc: 0.9300\n",
      "Epoch 27 - Validation Loss: 0.3462\n",
      "Epoch 27 - Validation loss_nat_1: 0.9231\n",
      "Epoch 27 - Validation loss_nat_2: 0.0192\n",
      "Epoch 27 - Validation loss_1: 0.3462\n",
      "Epoch 27 - Validation loss_2: 0.9231\n",
      "Epoch 28/40\n",
      "100/100 [==============================] - 10s 102ms/step - loss: 0.8609 - acc: 0.8775 - val_loss: 0.2218 - val_acc: 0.9700\n",
      "Epoch 28 - Validation Loss: 0.3269\n",
      "Epoch 28 - Validation loss_nat_1: 0.9231\n",
      "Epoch 28 - Validation loss_nat_2: 0.0000\n",
      "Epoch 28 - Validation loss_1: 0.3269\n",
      "Epoch 28 - Validation loss_2: 0.9231\n",
      "Epoch 29/40\n",
      "100/100 [==============================] - 10s 103ms/step - loss: 0.9446 - acc: 0.8725 - val_loss: 0.7701 - val_acc: 0.8800\n",
      "Epoch 29 - Validation Loss: 0.3269\n",
      "Epoch 29 - Validation loss_nat_1: 0.9231\n",
      "Epoch 29 - Validation loss_nat_2: 0.0000\n",
      "Epoch 29 - Validation loss_1: 0.3269\n",
      "Epoch 29 - Validation loss_2: 0.9231\n",
      "Epoch 30/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 0.7321 - acc: 0.8975 - val_loss: 0.4661 - val_acc: 0.9100\n",
      "Epoch 30 - Validation Loss: 0.3077\n",
      "Epoch 30 - Validation loss_nat_1: 0.9231\n",
      "Epoch 30 - Validation loss_nat_2: 0.0000\n",
      "Epoch 30 - Validation loss_1: 0.3077\n",
      "Epoch 30 - Validation loss_2: 0.9231\n",
      "Epoch 31/40\n",
      "100/100 [==============================] - 10s 102ms/step - loss: 0.6215 - acc: 0.9125 - val_loss: 0.3479 - val_acc: 0.9300\n",
      "Epoch 31 - Validation Loss: 0.2692\n",
      "Epoch 31 - Validation loss_nat_1: 0.9423\n",
      "Epoch 31 - Validation loss_nat_2: 0.0000\n",
      "Epoch 31 - Validation loss_1: 0.2692\n",
      "Epoch 31 - Validation loss_2: 0.9231\n",
      "Epoch 32/40\n",
      "100/100 [==============================] - 10s 102ms/step - loss: 0.5959 - acc: 0.9075 - val_loss: 0.1845 - val_acc: 0.9700\n",
      "Epoch 32 - Validation Loss: 0.2500\n",
      "Epoch 32 - Validation loss_nat_1: 0.9231\n",
      "Epoch 32 - Validation loss_nat_2: 0.0192\n",
      "Epoch 32 - Validation loss_1: 0.2500\n",
      "Epoch 32 - Validation loss_2: 0.9231\n",
      "Epoch 33/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 0.6925 - acc: 0.9125 - val_loss: 0.1308 - val_acc: 0.9700\n",
      "Epoch 33 - Validation Loss: 0.2885\n",
      "Epoch 33 - Validation loss_nat_1: 0.9038\n",
      "Epoch 33 - Validation loss_nat_2: 0.0192\n",
      "Epoch 33 - Validation loss_1: 0.2885\n",
      "Epoch 33 - Validation loss_2: 0.9231\n",
      "Epoch 34/40\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 0.3594 - acc: 0.9450 - val_loss: 5.2234e-04 - val_acc: 1.0000\n",
      "Epoch 34 - Validation Loss: 0.2308\n",
      "Epoch 34 - Validation loss_nat_1: 0.9423\n",
      "Epoch 34 - Validation loss_nat_2: 0.0000\n",
      "Epoch 34 - Validation loss_1: 0.2308\n",
      "Epoch 34 - Validation loss_2: 0.9231\n",
      "Epoch 35/40\n",
      "100/100 [==============================] - 10s 104ms/step - loss: 0.4650 - acc: 0.9350 - val_loss: 0.1149 - val_acc: 0.9800\n",
      "Epoch 35 - Validation Loss: 0.2308\n",
      "Epoch 35 - Validation loss_nat_1: 0.9423\n",
      "Epoch 35 - Validation loss_nat_2: 0.0000\n",
      "Epoch 35 - Validation loss_1: 0.2308\n",
      "Epoch 35 - Validation loss_2: 0.9231\n",
      "Epoch 36/40\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 0.4340 - acc: 0.9425 - val_loss: 0.0632 - val_acc: 0.9900\n",
      "Epoch 36 - Validation Loss: 0.2115\n",
      "Epoch 36 - Validation loss_nat_1: 0.9423\n",
      "Epoch 36 - Validation loss_nat_2: 0.0000\n",
      "Epoch 36 - Validation loss_1: 0.2115\n",
      "Epoch 36 - Validation loss_2: 0.9231\n",
      "Epoch 37/40\n",
      "100/100 [==============================] - 10s 102ms/step - loss: 0.4584 - acc: 0.9300 - val_loss: 0.1724 - val_acc: 0.9700\n",
      "Epoch 37 - Validation Loss: 0.2308\n",
      "Epoch 37 - Validation loss_nat_1: 0.9038\n",
      "Epoch 37 - Validation loss_nat_2: 0.0000\n",
      "Epoch 37 - Validation loss_1: 0.2308\n",
      "Epoch 37 - Validation loss_2: 0.9231\n",
      "Epoch 38/40\n",
      "100/100 [==============================] - 10s 102ms/step - loss: 0.2842 - acc: 0.9575 - val_loss: 0.0761 - val_acc: 0.9800\n",
      "Epoch 38 - Validation Loss: 0.1923\n",
      "Epoch 38 - Validation loss_nat_1: 0.9231\n",
      "Epoch 38 - Validation loss_nat_2: 0.0192\n",
      "Epoch 38 - Validation loss_1: 0.1923\n",
      "Epoch 38 - Validation loss_2: 0.9423\n",
      "Epoch 39/40\n",
      "100/100 [==============================] - 10s 102ms/step - loss: 0.2376 - acc: 0.9650 - val_loss: 0.0582 - val_acc: 0.9900\n",
      "Epoch 39 - Validation Loss: 0.1731\n",
      "Epoch 39 - Validation loss_nat_1: 0.9231\n",
      "Epoch 39 - Validation loss_nat_2: 0.0000\n",
      "Epoch 39 - Validation loss_1: 0.1731\n",
      "Epoch 39 - Validation loss_2: 0.9231\n",
      "Epoch 40/40\n",
      "100/100 [==============================] - 10s 102ms/step - loss: 0.1680 - acc: 0.9725 - val_loss: 0.1696 - val_acc: 0.9700\n",
      "Epoch 40 - Validation Loss: 0.1346\n",
      "Epoch 40 - Validation loss_nat_1: 0.9615\n",
      "Epoch 40 - Validation loss_nat_2: 0.0000\n",
      "Epoch 40 - Validation loss_1: 0.1346\n",
      "Epoch 40 - Validation loss_2: 0.9231\n"
     ]
    }
   ],
   "source": [
    "for i in range(5): \n",
    "\n",
    "    perturbed_model.compile(loss=negative_cross_entropy,\n",
    "                      optimizer=optimizer_purt, \n",
    "                      metrics=['accuracy'])\n",
    "    print(\"perturbed_model done\")\n",
    "\n",
    "    perturbed_model.fit_generator(train_generator, \n",
    "                                      steps_per_epoch=len(target_model.trainX) / target_model.batch_size,\n",
    "                                      validation_data=val_generator,\n",
    "                                      epochs=40,\n",
    "                                      validation_steps=len(target_model.valX) / target_model.batch_size,\n",
    "                                      callbacks=[custom_callback]\n",
    "                                      )\n",
    "    \n",
    "    perturbed_model.set_weights(custom_callback.get_best_weights())\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "    # mask <= perturbed\n",
    "    mask_model.set_weights(perturbed_model.get_weights())\n",
    "\n",
    "    mask_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=optimizer_mask, \n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    mask_model.fit_generator(train_generator, \n",
    "                                      steps_per_epoch=len(target_model.trainX) / target_model.batch_size,\n",
    "                                      validation_data=val_generator,\n",
    "                                      epochs=40,\n",
    "                                      validation_steps=len(target_model.valX) / target_model.batch_size,\n",
    "                                      callbacks=[custom_callback_mask]\n",
    "                                      )\n",
    "    \n",
    "    mask_model.set_weights(custom_callback_mask.get_best_weights())\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "    # mask => perturbed\n",
    "    perturbed_model.set_weights(mask_model.get_weights())\n",
    "\n",
    "    robust_target.append(np.mean(loss_rob_target[(i-1)*40 : i*40]))\n",
    "    robust_trojan.append(np.mean(loss_rob_trojan[(i-1)*40 : i*40]))\n",
    "    natural_target.append(np.mean(loss_nat_target[(i-1)*40 : i*40]))\n",
    "    natural_trojan.append(np.mean(loss_nat_trojan[(i-1)*40 : i*40]))\n",
    "    loss_sequential_1.append(a*robust_target[i-1]+(1-a)*natural_target[i-1])\n",
    "    loss_sequential_2.append(a*robust_trojan[i-1]+(1-a)*natural_trojan[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_per)\n",
    "plt.plot(loss_rob_target)\n",
    "plt.plot(loss_rob_trojan)\n",
    "plt.title(\"the robusts of target layer and trojan layers\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "#plt.savefig(acc_trend_graph_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_mask)\n",
    "plt.plot(loss_nat_target)\n",
    "plt.plot(loss_nat_trojan)\n",
    "plt.title(\"the natural losses of target layer and trojan layers\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "#plt.savefig(acc_trend_graph_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(robust_target)\n",
    "plt.plot(robust_trojan)\n",
    "plt.title(\"the robusts of target layer and trojan layers\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "#plt.savefig(acc_trend_graph_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAvZElEQVR4nO3deXwV1f3/8dcnIQRI2BNAdsIqqyIKIuCGiqigonVprajV2orSqnWrWmtdv+1P60JrqVq1Ku5WVHBXEBdkkX2RfV/CThJCtvP7YwZyExJygdzMvbnv5+NxH/fembkzn3Pn3vOZOXNmxpxziIhI/EoIOgAREQmWEoGISJxTIhARiXNKBCIicU6JQEQkzikRiIjEOSWCSmBmbc3MmVmNoGOJNDM7xczWHmS8M7MOVRlTOXGYmf3HzLab2Q9VuNywy29m95nZy5GOKYw4BprZ4qDjADCziWZ25WF+9hkzuyeaYooV1b7iigQzWwn8yjn3WdCxhDKztsAKIMk5VxBwOEEbAJwBtHTOZQcdTDRzzn0NdK7q5ZrZfUAH59wvQmI5+3Dn55y7PtpiihXaI5D9/K3o6vKbaAOsVBIoKR72WuUwOOf0OIQH8F+gCNgDZAG3AW0BB1wJrAa2AH8M+UwCcAewDNgKvAE0Kmf+pwBrgVuAzcAG4KqQ8ecAPwK7gDXAfSHjVvtxZPmPE4H7gJdDptkXaw3//VfAg8A3fpk6AFcBC4HdwHLg16XjO8j34/C2qADqAy8BmcAq4G4gwR/XAZgE7PS/r9f94QY87pd9FzAX6F7OspoD44FtwFLgWn/4NUAuUOh/D38u5/NX++XcDnwMtAkZ94T//e4CZgADQ8YlAnf563O3P75VSPmvB5YAO4AxgJWz/NLr5k1go/+dTAa6+cOPBzYBiSHTXgjMruj3FbK+r/F/H5PL+82FvF8J3ArM8WN5HahVThkS/PW6yl9nLwH1Sy37OmA93m/5Vn/cECAPyPfX0b6yfIW3tw0wEu93+bj/XS4H+vvD1/jLuzIklheAB/zX71P8P8jC+8+OPNi6DTOmcMpbZj0QzY/AA4jFh/9HGRzyft8P4N9AbaAXsBc42h8/GvgeaAkkA/8CxpUz71OAAuB+IAkYCuQADUPG9/B/kD3xKojzS8VRI2R+91FxIlgNdMNrKkzCSzbt8Srlk/3l9w5ZfriJ4CXgPaCuv9yfgGv8ceOAP/rlqAUM8Ief5f85G/jLPxo4qpxlTQb+4X/+GLyEc5o/biQw5SBxDsdLHkf75b4b+DZk/C+Axv64W/Aq6Fr+uD/gJajOfoy9gMYh5f/Aj7+1H9OQcmIovW6u9r+rZODvwKyQcQuAs0PevwvcUtHvK2R9vwSkALXL+c2VTgQ/4CXaRnjJ8vpyynC1/z1mAKnAO8B/Sy17nL/sHv73Mbis8of8HkMTQQHehkki8ADeb3WMX84z8RJxqj/9C/iJoNQ8z8ZLRPuS9cHWbUUxhVPeMuuBaH4EHkAsPig/EbQMGfYDcKn/eiFwesi4o/C2OmqUMe9T8LbMQyvzzUC/cmL5O/B4qTgONRHcX0F5/weMDomvwkTg/3HzgK4h434NfOW/fgkYG/qd+cNPw0sY/fD3HspZTiu8Lf66IcMeBl7wX4/k4IlgIn5S8t8n4CW8NuVMvx3o5b9eDAw/SPkHhLx/A7ijnGlLrJtS4xr486rvv78deMV/3ciP9aiKfl8h6zvjIN9FiXWK9/v+Rcj7/wOeKeeznwO/DXnfuYxldyk1r+fKKz8HJoIlIeN6+PNrGjJsK3CM//oFSiUCoBPe/2fAQcofum4riimc8pZZD0Tzo7q0B0eLjSGvc/C2GMBrr37XzHaY2Q68P24h0LSc+Wx1JQ/27p+XmfU1sy/NLNPMduI1Q6QdYdxrQt+Y2dlm9r2ZbfPjHXoYy0jD27tYFTJsFdDCf30b3tb0D2Y238yuBnDOfQE8jbfVt9nMxppZvTLm3xzY5pzbXc78K9IGeCJknWzz42kBYGa3mtlCM9vpj69P8XfQCq8Zpjzl/Q7KZWaJZvaImS0zs114lTEhy3wZOM/MUoCfAV875zaElKWi31eJdRyGcMvQnAPXcY2DLHuV/5lwbQp5vQfAOVd6WJmxmVl9vD3Su51zU0KGH2zdViSc8h7y+g+aEsHhcYc4/Rq83foGIY9azrl1h7HsV/HaxVs55+oDz+BVYOXFlQ3UCXnfrIxp9n/OzJKBt4G/4W15NQAmhCwjXFvwtpTahAxrDawDcM5tdM5d65xrjren8I993S6dc086544DuuJt0f2hjPmvBxqZWd2y5h+GNXjHPkLXSW3n3LdmNhAvUf0Mr0muAV5buYV8tn2YywnX5XjNVYPxKqa2/nAD8H8r3+EdG7gC71hVaFkq+n0d6m82XOs5cB0XULICb1Vq/PoIx4Tf6eFV4Evn3NiQ4RWt24piCqe8MUeJ4PBswmsjDNczwINm1gbAzNLNbPhhLrsu3pZwrpmdgFeB7JOJd1AsNLZZwCAza+1vId1Zwfxr4rW/ZgIFZnY2XlvsIXHOFeI1izxoZnX9st+Mt2WLmV1sZi39ybfj/QGLzOx4f68nCS+J5fplKj3/NcC3wMNmVsvMeuIdEA23X/4zwJ1m1s2Pp76ZXeyPq4v3584EapjZvUDoXsmzwF/MrKPf06qnmTUOc7nlqYvXnrwVL3E/VMY0L+FVYj3w2qZDy1JZv69DNQ74vZm1M7NUvLhfL7VHe4+Z1fG/66vwDj6D9z9qG6Geag/iHZcYXWp4Reu2opjCKW/MUSI4PA8Dd/u74reGMf0TeFvxn5jZbrwDe30Pc9m/Be7353MvXmULgHMuB78HkB9bP+fcp3h/vDl4B2E/ONjM/aaWm/z5bsdLNOMPM9Yb8Srz5cAUvC205/1xxwNTzSzLn/9o59xyvD/lv/1lr8KrGP9azvwvw9tyXo938PRPLsxzO5xz7wKPAq/5TTHz8A4qgteD6CO8YxWr8JJRaPPGY3jfzyd4PU+ewzs4eCRe8pe1Du/A8PdlTPMufjOQv673qczf16F6Hm/vZDLeOSy5eOs91CS8A6yfA39zzn3iD3/Tf95qZjMrOa7L8I4zbTezLP/xcypetxXFFE55Y475BzREJAaY2TK8Jq2oOpmxLDrBMXZoj0AkRpjZCLwmtC+CjkWqF51lKBIDzOwrvIPnVzjnDjhmInIk1DQkIhLn1DQkIhLnYq5pKC0tzbVt2zboMEREYsqMGTO2OOfSyxoXc4mgbdu2TJ8+PegwRERiipmtKm+cmoZEROKcEoGISJxTIhARiXNKBCIicU6JQEQkzikRiIjEOSUCEZE4p0QgIhLlcvMLeXjCQtbt2BOR+cfcCWUiIvFkxZZsRr06k/nrd9GyUR2u6Nem4g8dIiUCEZEo9d6sddz1zlySaiTw7C/7MLhrebc5PzJKBCIiUWZPXiH3fzCfcT+soU+bhjx52bE0b3CkN8ErnxKBiEgUWbp5Nze88iOLN+3mt6e05/dndCIpMbKHc5UIRESixFsz1nLP/+ZRp2YiL159Aid3KvNioZVOiUBEJGDZewu45715vDNzHf0yGvHEpcfStF6tKlu+EoGISIAWbtjFqFdnsnxLNr8b3JEbT+tIYoJVaQxKBCIiAXDOMe6HNfz5/fnUq53EK7/qS//2aYHEokQgIlLFdufmc9e783h/9noGdkzj8UuOIS01ObB4lAgkUAWFRdSIcI8IkWgyb91Obnh1Jmu37+G2IZ25flB7Eqq4Kag0JQKJOOccm3btZVlmFsszs1iWme2/zmb9zj0M6pjOA+d3p1WjOkGHKhIxzjle/HYlD01YROPUmrx2XT+Ob9so6LAAJQKpRLn5hazYUlzJ73tenplFdl7h/unq1EykfXoqfdo2pHFKM16ftpozH5/MrWd1ZmT/tlV+oEwk0nbm5HPb27P5eP4mTu/ShL9d3IuGKTWDDms/c84FHcMh6dOnj9PN64PjnCNz916Wlqrsl2VmsW7HHkJ/Ti0a1CYjPYX26am0T08hIz2V9umpNK2XjFlxZb9uxx7ufncuXy7OpFfL+jwyoidHH1UvgNKJVL4fV2/nxnE/snFnLnec3YVrBrQr8fuvKmY2wznXp8xxSgRSltz8QlZtzfGbckpW+rv3FuyfrnZS4v7KPvS5XVoKdWqGv8PpnGP87PXc//4Cdu7J5/qT2zPqtA7USkqMRPFEIq6oyPHclBU8+tEimtWvxdOX9+aYVg0Ci0eJQMrknGNLVl6pdnvv9drtORSF/DSOql/rgMq+fXoqzerVqtQDXduz83jgw4W8PXMtGWkpPHxhD/pmNK60+YtUhe3Zedzy5my+WLSZId2a8ehFPalfOynQmJQI4lxeQRGrtmaXOEi7r9LflVu8dZ9cI4GMkEq+vf/cLi2FlOSqPZz09ZJM7nxnLmu37+Hyvq254+wu1KsV7B9JJBzTVm7jpnE/sjUrj7vPPZor+rUJpCmoNCWCOOCcY1t2Hsu3ZLNsc1aJ59XbcigM2bxvWi+51Na9V+k3r1878G5soXLyCnj80594bsoK0lKTuX94d4Z0bxZ0WCJlKipy/HPSMh779CdaNazN05f3pnuL+kGHtZ8SQTWSX1jE6m05JSr7ZZne6x05+funq1kjgYy0lJCt++K2+7oxtmU9Z+0Obn97Lgs37GJIt2bcP7wbTarwOiwiFcncvZeb35jF10u2cF6v5jx0Qfeo+58pEcSg7dl5LN+SxbLN2Szzn5dvyWL11hwKQrbu0+sml+iRk5GeQof0VJo3qF2tumHmFxbx76+X8/fPlpBcI4E/Dj2aS45vFRW73BLfvl26hdGvz2LXnnzuG9aNS6P0d6lEEKUKCotYs32Pv3VfXNkvy8xmW3be/ulqJibQNq0OGWmptG+S4j97lX68tZsvz8ziznfmMnXFNvq2a8TDF/YgIz016LAkDhUWOZ74fAlPfbGEjLQUxvy8N12aRW+3ZyWCgO3MyWfZluKDtPuadVZtzSa/sPj7T0utub+9PrQNv2XDOtVq6/5IFRU53pi+hgcnLGRvQRGjT+/IdYMyIn7zDpF9Nu3KZfRrP/L98m2M6N2Sv5zf7ZC6SwdBiaAKFBY51m7PKdErZ5l/Vu2WrOKt+6REo03jFDLSUrytev+5fVoq9evE19b9kdq8K5c/jZ/PxHkbOfqoejw6ogc9WzYIOiyp5ib9lMnNr88iJ6+Qv5zfnYuOaxl0SGFRIqhEu3Lz9182IbTSX7klh7zCov3TNUqp6VXy6SWbc1o1rK2LrFWyj+Zt5N735rElay9Xn9SOm8/sFPVbZxJ78guLeOzTn/jnV8vo0qwuT19+LB2a1A06rLAdLBHo31KGwiLH+h179m/Vh55olbl77/7pEhOMNo29tvtTuzShfUgbfjRdR6S6G9K9GSe2b8yjHy3i2Skr+Gj+Rh66oAeDqug2f1L9rd+xhxvH/ciMVdu57ITW/Om8rtXqrPe43iPI2lvA8jKumbNiSzZ7C4q37uvXTgppt/fb8Juk0rpRHbVLR5mpy7dy5ztzWb4lmwt7t+Cec7oqKcsR+WzBJm59azb5BUU8PKInw3o1Dzqkw6KmIeCnTbv5ZumWEpX+xl25+8cnJhitG9U5oO0+Iy2FRik1o7I7mJQtN7+Qp79YyjOTllG/dhL3nteVYb2aax3KIckrKOLRjxbx3JQVdGtej6cv7027tJSgwzpsSgTAs18v54EPF1K3Vo0SJ1i1T0+lQ5MUWjdKoWYNbd1XJws37OKOt+cwe+1OTu2czgMX9KBFg9pBhyUxYM22HEa9OpPZa3cysn9b7hzaheQasd0UpEQA7MjJI7/QkZaqrft4UljkeOHblfzt48UkGPzhrM5ccaLueSDlmzh3A7e9PQeAv17UkyHdjwo4osqhRCBxb822HP74v3lM/imTY1s34NERPenUNHZ6fEjk5eYX8tCEhbz03Sp6tWrA05cdW63umnewRBDRthAzG2Jmi81sqZndcZDpRpiZM7MygxQ5Uq0a1eHFq47n8Ut6sXJLNuc8+TWPffoTewsKK/6wVHsrtmRz4T++5aXvVnHtwHa8+esTq1USqEjEuo+aWSIwBjgDWAtMM7PxzrkFpaarC4wGpkYqFhEAM+OCY1syqGM6f/lgAU9+voQJczfwyIU96BMl946VqvferHXc9c5ckmok8NyVfTj96KZBh1TlIrlHcAKw1Dm33DmXB7wGDC9jur8AjwK5ZYwTqXSNU5P5+6XH8p+rjmdPXiEXPfMd9/xvHrtz8yv+sFQbe/IKuePtOYx+bRZdm9djwk0D4zIJQGQTQQtgTcj7tf6w/cysN9DKOffhwWZkZteZ2XQzm56ZmVn5kUpcOrVzEz75/SCuOqktL09dxRmPTeazBZuCDkuqwJJNuxk+ZgqvT1/DDae2Z9y1/Wgexz3KAusvaWYJwGPALRVN65wb65zr45zrk56us0Wl8qQk1+BP53Xjnd/0p37tJH710nRueHVmiTPIpXp5c/oahj39DVuz8njxqhP4w1ld4v6yL5Es/TqgVcj7lv6wfeoC3YGvzGwl0A8YrwPGEoRjWzfk/RsHcMsZnfh0/iYGPzaJN6avIdZ61Un5svcWcPPrs/jDW3M4plUDJo4eqMuQ+CLWfdTMagA/AafjJYBpwOXOufnlTP8VcKtz7qB9Q9V9VCJt6eYs7npnLj+s3Eb/9o15+MIetGkcu2eUindy4Q2vzmTllmxGn96JUad1iLtzSQLpPuqcKwBGAR8DC4E3nHPzzex+MxsWqeWKHKkOTVJ57bp+PHB+d+as3clZf5/MvyYtoyDk6rISG5xzvDJ1FcPHfENWbgGv/Kofowd3jLskUBGdUCZyEBt35nLPe/P4dMEmujWvx6MjekbVDcmlfLtz87nznbl8MGcDgzql89jPepGWmhx0WIEJ7IQykVjXrH4txl5xHP/8eW82797L8DHf8PDEhezJ04lo0Wzu2p2c+9QUJs7byG1DOvPCyOPjOglURPcjEKmAmXF2j6Po3z6NhyYs5F+TlvPRvI08fEEP+ndICzo8CeGc48VvV/LQhEU0Tq3J69f108mCYdAegUiY6tdJ4tGLevLqtX0x4PJnp3LbW7PZkZNX4Wcl8nbm5HP9yzO47/0FDOqUxoSbBioJhEnHCEQOQ25+IU98voSxk5fTsE5N/jysG0N7NNOVbQMyc/V2bnz1RzbvzuX2IV24ZkA7rYtSdIxApJLVSkrk9iFdGD/qJJrVT+aGV2dy7Usz2LBzT9ChxZWiIsfYycv42TPfYQZvXt+fXw3MUBI4RNojEDlCBYVF/Oeblfy/TxdTIyGB24d05ud925CgLooRtS07j1vfnM0XizZzdvdmPDKiJ/VrJwUdVtTS/QhEqsDqrTnc9e5cpizdQp82DXlkRA86NNE9DyLhhxXbuGncj2zLzuOec4/mF/3aaC+gAmoaEqkCrRvX4b/XnMDfLu7Fks1ZDH1iCk9+voS8Ap2IVlmKihxPf7GES8d+R62kBN75bX+uOLGtksARUvdRkUpkZlx0XEtO7pTO/R8s4LFPf+KDOet5+MKeHNemYdDhxbTM3Xu5+Y1ZfL1kC8N6NeehC3uQmqwqrDJoj0AkAtLrJvPUZcfy3JV92J1bwEXPfMt94+eTtbcg6NBi0jdLtzD0ya/5YcU2Hh3RgycuPUZJoBLpmxSJoNOPbkrfjMb89aNFvPjdSj5dsIkHzu/OqV2aBB1aTCgscjzx+RKe+mIJ7dNTefmavnRupuMulU17BCIRlppcgz8P785b159I7ZqJXPXCNG4a9yNbsnTPg4PZtCuXy//9PU9+voSLerdk/KiTlAQiRL2GRKrQ3oJC/vnVMsZ8uZSU5Brcc05XLuzdQgc7S/lq8WZufmM2ufmFPHB+dy7s3TLokGKeeg2JRInkGon8bnAnJtw0kIy0FG55cza/fP4H1mzLCTq0qJBfWMQjExcx8j/TaFI3mfGjBigJVAHtEYgEpKjI8fLUVTw6cRFFDm45sxMj+7eN29smrtuxh5vG/ciMVdu5vG9r7j23K7WSEoMOq9rQCWUiUWz9jj3c8795fL5oMz1b1ueRC3vStXm9oMOqUp8u2MStb86msMjx8IU9OK9X86BDqnbUNCQSxZo3qM2zV/bhqcuOZd32PQx7egr/99EicvOr/z0P8gqKuP/9BVz70nRaNarNBzcOUBIIgLqPikQBM+O8Xs0Z0CGNBycs5B9fLWPivI08fGEP+mU0Djq8iFi9NYdR42YyZ+1ORvZvy51Du5BcQ01BQdAegUgUaZhSk79d3IuXr+lLQVERl479njvfmcPOPflBh1apJszdwDlPfs3KLdn864rjuG9YNyWBACkRiEShAR3T+OR3J3PdoAxen7aGMx6bxEfzNgQd1hHLzS/k7v/N5bevzKR9k1Q+vGkgZ3VrFnRYcU+JQCRK1a6ZyF1Dj+a9GwaQlprM9S/P5Nf/nc6mXblBh3ZYlmdmccE/vuXl71dz3aAM3rz+RFo1qhN0WIISgUjU69GyPu+NOonbh3Thq8WZDP5/k3h16mqKimKnx997s9Zx3lNT2LhzD8+P7MNdQ48mKU67yUYjrQmRGJCUmMBvTmnPR78bRLcW9bjr3blc+u/vWZaZFXRoB7Unr5Db35rD6Ndm0bV5PSaMHshpXZoGHZaUokQgEkPapaUw7tp+PDqiB4s27OLsJ75mzJdLyS+MvnseLNm0m+FjpvDGjDWMOrUD467tx1H1awcdlpRBiUAkxpgZlxzfms9uOZkzjm7KXz9ezHlPTWH2mh1BhwaAc443pq/hvKensC07j5euPoFbz+oct2dMxwKtGZEY1aRuLcb8vDdjrziO7Tl5XPCPb/jLBwvIyQvungfZewu45Y3Z3PbWHHq3bsiEmwYysGN6YPFIeHRCmUiMO7NbM/q1b8yjExfx3JQVfDx/Iw9e0IOTO1VtBbxg/S5GvTqTlVuz+f3gTow6rQOJCbqqaizQHoFINVCvVhIPXtCDN359IjVrJHDl8z9w8+uz2JadF/FlO+d4Zeoqzv/HN2TtLeCVX/Vj9OCOSgIxRIlApBo5oV0jJtw0kBtP68D42esZ/Ngk3pu1jkhdXHJXbj6jxv3IH9+dx4kZjZk4eiAntq+el8SozpQIRKqZWkmJ3HJmZz64aQCtG9Vh9GuzuOqFaazdXrn3PJizdgfnPjmFj+Zt5PYhXfjPyONpnJpcqcuQqqFEIFJNdWlWj7d/058/ndeVH1Zs48zHJ/Ofb1ZQeIQnojnn+M83Kxjxz28pKCzijV/34zentCdBTUExS/cjEIkDa7fn8Md35zHpp0yOadWAR0f0PKz7/+7IyeO2t+bwyYJNDD66KX+7uCcN6tSMQMRS2XQ/ApE417JhHV646nieuPQYVm/L4Zwnv+axTxYf0j0PZq7ezjlPTuHLxZu559yu/PuXxykJVBNKBCJxwswYfkwLPrv5ZIb1as6TXyzlnCe/ZtrKbQf9XFGR41+TlvGzZ74jIQHeur4/1wxoh5magqoLJQKRONMopSaPXXIML159Arn5RVz8zHfc/b+57Mo98J4H27LzuObFaTw8cRFndmvKhzcNpFerBlUftERURBOBmQ0xs8VmttTM7ihj/PVmNtfMZpnZFDPrGsl4RKTYyZ3S+eT3g7hmQDtenbqaMx+bzCfzN+4fP3X5Vs5+YjLfLNvKX87vzpjLe1OvVlKAEUukROxgsZklAj8BZwBrgWnAZc65BSHT1HPO7fJfDwN+65wbcrD56mCxSOWbtWYHd7w9h0UbdzO0RzM6NqnLU18soU3jFJ6+/Fi6Na8fdIhyhA52sDiSl5g4AVjqnFvuB/EaMBzYnwj2JQFfChBbXZhEqoljWjXg/RsHMHbycp74fAkT5m5k+DHNefCCHqQm60o01V0k13ALYE3I+7VA39ITmdkNwM1ATeC0smZkZtcB1wG0bt260gMVEe+eBzec2oGhPY5i5ZZsTumcrgPCcSLwg8XOuTHOufbA7cDd5Uwz1jnXxznXJz1dVzIUiaR2aSmc2qWJkkAciWQiWAe0Cnnf0h9WnteA8yMYj4iIlCGSiWAa0NHM2plZTeBSYHzoBGbWMeTtOcCSCMYjIiJliNgxAudcgZmNAj4GEoHnnXPzzex+YLpzbjwwyswGA/nAduDKSMUjIiJli2h3AOfcBGBCqWH3hrweHcnli4hIxQI/WCwiIsFSIhARiXNKBCIicU6JQEQkzikRiIjEOSUCEZE4p0QgIhLnlAhEROKcEoGISJxTIhARiXNKBCIicS6sRGBmo82snnmeM7OZZnZmpIMTEZHIC3eP4Gr/tpJnAg2BK4BHIhaViIhUmXATwb5bFQ0F/uucmx8yTEREYli4iWCGmX2Clwg+NrO6QFHkwhIRkaoS7v0IrgGOAZY753LMrBFwVcSiEhGRKhPuHsGJwGLn3A4z+wXeTeZ3Ri4sERGpKuEmgn8COWbWC7gFWAa8FLGoRESkyoSbCAqccw4YDjztnBsD1I1cWCIiUlXCPUaw28zuxOs2OtDMEoCkyIUlIiJVJdw9gkuAvXjnE2wEWgJ/jVhUIiJSZcJKBH7l/wpQ38zOBXKdczpGICJSDYR7iYmfAT8AFwM/A6aa2UWRDExERKpGuMcI/ggc75zbDGBm6cBnwFuRCkxERKpGuMcIEvYlAd/WQ/isiIhEsXD3CD4ys4+Bcf77S4AJkQlJRESqUliJwDn3BzMbAZzkDxrrnHs3cmGJiEhVCXePAOfc28DbEYxFREQCcNBEYGa7AVfWKMA55+pFJCoREakyB00EzjldRkJEpJpTzx8RkTinRCAiEueUCERE4pwSgYhInItoIjCzIWa22MyWmtkdZYy/2cwWmNkcM/vczNpEMh4RETlQxBKBmSUCY4Czga7AZWbWtdRkPwJ9nHM98a5b9H+RikdERMoWyT2CE4Clzrnlzrk84DW8O5zt55z70jmX47/9Hu8+ByIiUoUimQhaAGtC3q/1h5XnGmBiWSPM7Dozm25m0zMzMysxRBERiYqDxWb2C6AP5dz1zDk31jnXxznXJz09vWqDExGp5sK+1tBhWAe0Cnnf0h9WgpkNxrvfwcnOub0RjEdERMoQyT2CaUBHM2tnZjWBS4HxoROY2bHAv4Bhpe53ICIiVSRiicA5VwCMAj4GFgJvOOfmm9n9ZjbMn+yvQCrwppnNMrPx5cxOREQiJJJNQzjnJlDqBjbOuXtDXg+O5PJFRKRiUXGwWEREgqNEICIS55QIRETinBKBiEicUyIQEYlzSgQiInFOiUBEJM4pEYiIxDklAhGROKdEICIS55QIRETinBKBiEicUyIQEYlzSgQiInFOiUBEJM4pEYiIxDklAhGROKdEICIS55QIRETinBKBiEicUyIQEYlzSgQiInFOiUBEJM4pEYiIxDklAhGROKdEICIS55QIRESiXfZW+PEV2LY8IrOvEZG5iojIkdm6DBZPgEUTYM334IrgzAeg/42VviglAhGRaFBUBOtnwqIPvQSQucgb3rQHDPoDdB4KR/WKyKKVCEREgpKfCysmw+IPYfFHkLURLBHa9IfjRnqVf8M2EQ9DiUBEpCrlbIMln3qV/9LPIS8LaqZCh9Oh8znQ8Qyo06hKQ1IiEBGJtO2r/Pb+D2HVt+AKIbUp9LgYupwDbQdCUq3AwlMiEBGpbM7Bhlnegd7FE2DTPG94ehc4abRX+TfvDQnR0XFTiUBEpDIU5MHKr72Kf/FE2LUOLAFa9fN6+3QeCo3bBx1lmZQIREQOV+5Ov71/gve8dxfUqO2195/6R+h0FqSkBR1lhSKaCMxsCPAEkAg865x7pNT4QcDfgZ7Apc65tyIZj4jIEdu51tviX/QhrJwCRflQJw26DoMu50LGKZBUO+goD0nEEoGZJQJjgDOAtcA0MxvvnFsQMtlqYCRwa6TiEBE5Is55bfyLJng9fTbM9oY37gD9fuO197c8HhISg43zCERyj+AEYKlzbjmAmb0GDAf2JwLn3Ep/XFEE4xAROTSFBbD62+LKf8dqwLwKf/B9XjfP9E5BR1lpIpkIWgBrQt6vBfoezozM7DrgOoDWrVsfeWQiIqXt3e316188AX76GHJ3QGIytD8VBt4KnYZA3aZBRxkRMXGw2Dk3FhgL0KdPHxdwOCJSXezeWHw9nxWToDAPajeEzmd7vXzanwbJqUFHGXGRTATrgFYh71v6w0REguEcZC6GRR94CWDdDG94w7Zw/LXQZajX3TMxJraRK00kSzsN6Ghm7fASwKXA5RFcnojIgYoKYc3U4ou57buUc/PecNrdXnt/k6PBLNg4AxSxROCcKzCzUcDHeN1Hn3fOzTez+4HpzrnxZnY88C7QEDjPzP7snOsWqZhEJE7k5cCyL/z2/o8gZyskJEG7QXDiDV6zT73mQUcZNSK6/+OcmwBMKDXs3pDX0/CajEREjkxWJvw00WvvX/4lFORCcn3odKZX8XcYDLXqBR1lVIqvhjARqV62LPW6dy6a4DX/4KB+K+h9pdfe3+YkSEwKOsqop0QgIrGjqAjWTS9u79/ykze8WQ84+Xbv5K5mPeK6vf9wKBGISHTL3wPLJxXfvCV7MyTU8Lb2j/+V19Wzgc4vOhJKBCISfXK2eQd5F33oHfTNz4GadaHjYP/mLYO9/v5SKZQIRCQ6bFtRfHLX6m+9m7XXbQ69LvPa+9sOhBrJQUdZLSkRiEgwiopgw4/FN2/Z7F+GrElXGHCzf/OWY9XeXwWUCESk6hTshRVf++39E2H3Bu/mLa37w1kPed08G7ULOsq4o0QgIpG1Z0fxzdqXfAZ5uyGpTvHN2judVeU3a5eSlAhEpPLtWBNys/ZvoKgAUppA9wu8yj/j5Ji7eUt1pkQgIkfOOdg4p/j6/RvnesPTOsGJo7z2/hZ9ouZm7VKSEoGIHJ7CfG9rf5Hf3r9zDWDQqi+ccb+35Z/WIegoJQxKBCISvtxdsPQz/2btn3g3b69RCzJO9c7s7TQEUtODjlIOkRKBiBzcrvUhN2+Z7N+svbF3o/bOQ707eNVMCTpKOQJKBCJSknOweaHf5PMhrP/RG94oA/r+2mvvb9U3pm/WLiUpEYjEo6Ii2L3eu0lLicdK7zk/25uuRR84/V7/Zu2ddXJXNaVEIFJdFRZ4B3D3V/Iril9vXwmFe4unTazp3a6xUQa0GwjpXaDjmVDvqKCilyqkRCASywr2wo7VZWzZL/eGFxUUT1ujtlfRp3X0TuJq1M573ygD6rVQU08cUyIQiXZ5Od4W/AGV/Qq/y6YrnrZmXWicAUf1gm4XQMOQyr5uMzXtSJmUCESiQe4u2L7iwIp+23LvejyhajfyKvbW/aDR5SW37Os0VmUvh0yJQKQqOAd7tpdspw995GwpOX1qU69ib3+aV9Hv37Jvp+vwS6VTIhCpLM5B1mb/YGwZFX7uzpLT12vpVexdzimu5BtleJV+cmowZZC4pEQgcihKdLtccWAzzr5ul+BdXrlBa69y73FxcSXfKAMattFF1yRqKBGIlBba7XL7igMr/NBulwlJxd0u2w4ouWVfvxXUqBlYMUTCpUQg8akgD3asKruP/Y5VZXS7bAeNO0DHM0pu2ddvqW6XEvOUCKT6Kt3tMrTdfuda7564+9Ss61X2zXpA1+Elt+xTm+nyyVKtKRFIbDug22VIU87u9SWnrd3Qq9hb9fVuiB66ZZ+Spm6XErfiJxFsXQZblkBCDW/rLqGG97BE/3Wi/9g3PGSa0uMtZDptKUZezrYDu13uq/yzM0tOm9LEq9gzTgnZqve7X+p2iCJlip9EsPB9+OxPkZl3RUklIfHA5HHInykrEZX+TOkElljxvPbPL7HUZ45wXoeyde2cV6GX1b9+2wrI3VFy+notvEq+89nFJ1I19Cv85LqVumpF4kH8JIJel0G7QVBU6B0IdP5zUYHXJXD/6wKv7Xj/+8Li5xKfKfU5V1hy2jLntW9c6RgKvWvGhE5TYlnlfCb02RUG/Q2XZKUTROk9rITi4bs3Ql5WyGcTvB43jTKg+4hSfezbqtulSCWLn0RQt6n3qK6cKz+BHZBAQhNVqfflJr6Dfa68xBdmDB0Gl2yvb9Ba3S5FqlD8JILqzqx4K5zkoKMRkRiiI50iInFOiUBEJM4pEYiIxLmIJgIzG2Jmi81sqZndUcb4ZDN73R8/1czaRjIeERE5UMQSgZklAmOAs4GuwGVm1rXUZNcA251zHYDHgUcjFY+IiJQtknsEJwBLnXPLnXN5wGvA8FLTDAde9F+/BZxupvP8RUSqUiQTQQtgTcj7tf6wMqdxzhUAO4HGpWdkZteZ2XQzm56ZmVl6tIiIHIGYOFjsnBvrnOvjnOuTnp4edDgiItVKJE8oWwe0Cnnf0h9W1jRrzawGUB/YerCZzpgxY4uZrTrMmNKALRVOFRtUluhTXcoBKku0OpKytClvRCQTwTSgo5m1w6vwLwUuLzXNeOBK4DvgIuAL55w72Eydc4e9S2Bm051zfQ7389FEZYk+1aUcoLJEq0iVJWKJwDlXYGajgI+BROB559x8M7sfmO6cGw88B/zXzJYC2/CShYiIVKGIXmvIOTcBmFBq2L0hr3OBiyMZg4iIHFxMHCyuRGODDqASqSzRp7qUA1SWaBWRslgFTfIiIlLNxdsegYiIlKJEICIS56pdIjCz581ss5nNK2e8mdmT/oXu5phZ76qOMVxhlOUUM9tpZrP8x71lTRcNzKyVmX1pZgvMbL6ZjS5jmqhfN2GWIybWi5nVMrMfzGy2X5Y/lzFNTFwYMsyyjDSzzJD18qsgYg2HmSWa2Y9m9kEZ4yp/nTjnqtUDGAT0BuaVM34oMBEwoB8wNeiYj6AspwAfBB1nmGU5Cujtv64L/AR0jbV1E2Y5YmK9+N9zqv86CZgK9Cs1zW+BZ/zXlwKvBx33EZRlJPB00LGGWZ6bgVfL+h1FYp1Uuz0C59xkvHMSyjMceMl5vgcamNlRVRPdoQmjLDHDObfBOTfTf70bWMiB156K+nUTZjligv89Z/lvk/xH6d4jMXFhyDDLEhPMrCVwDvBsOZNU+jqpdokgDOFcDC+WnOjvDk80s25BBxMOf1f2WLyttlAxtW4OUg6IkfXiN0HMAjYDnzrnyl0n7iAXhowGYZQFYITf7PiWmbUqY3w0+DtwG1BUzvhKXyfxmAiqk5lAG+dcL+Ap4H/BhlMxM0sF3gZ+55zbFXQ8h6uCcsTMenHOFTrnjsG7FtgJZtY94JAOWxhleR9o65zrCXxK8VZ11DCzc4HNzrkZVbnceEwE4VwMLyY453bt2x123lncSWaWFnBY5TKzJLzK8xXn3DtlTBIT66aicsTaegFwzu0AvgSGlBq1f52Ee2HIoJVXFufcVufcXv/ts8BxVRxaOE4ChpnZSrx7uJxmZi+XmqbS10k8JoLxwC/9Hir9gJ3OuQ1BB3U4zKzZvrZBMzsBb31G5Z/Uj/M5YKFz7rFyJov6dRNOOWJlvZhZupk18F/XBs4AFpWabN+FISHMC0MGIZyylDreNAzv+E5Ucc7d6Zxr6Zxri3cg+Avn3C9KTVbp6ySi1xoKgpmNw+u1kWZma4E/4R04wjn3DN61j4YCS4Ec4KpgIq1YGGW5CPiNmRUAe4BLo/FP6jsJuAKY67fjAtwFtIaYWjfhlCNW1stRwIvm3VY2AXjDOfeBxeaFIcMpy01mNgwowCvLyMCiPUSRXie6xISISJyLx6YhEREJoUQgIhLnlAhEROKcEoGISJxTIhARiXNKBCJVyL8y6QFXlBQJkhKBiEicUyIQKYOZ/cK/vv0sM/uXf0GzLDN73L/e/edmlu5Pe4yZfe9fzOxdM2voD+9gZp/5F5+baWbt/dmn+hc9W2Rmr0Tj1TwlvigRiJRiZkcDlwAn+RcxKwR+DqTgnd3ZDZiEd6Y3wEvA7f7FzOaGDH8FGONffK4/sO9yGccCvwO6Ahl4ZyuLBKbaXWJCpBKcjndBsmn+xnptvEsbFwGv+9O8DLxjZvWBBs65Sf7wF4E3zawu0MI59y6Acy4XwJ/fD865tf77WUBbYErESyVSDiUCkQMZ8KJz7s4SA83uKTXd4V6fZW/I60L0P5SAqWlI5ECfAxeZWRMAM2tkZm3w/i8X+dNcDkxxzu0EtpvZQH/4FcAk/+5la83sfH8eyWZWpyoLIRIubYmIlOKcW2BmdwOfmFkCkA/cAGTj3fDkbrymokv8j1wJPONX9MspvmrqFcC//CtH5gMXV2ExRMKmq4+KhMnMspxzqUHHIVLZ1DQkIhLntEcgIhLntEcgIhLnlAhEROKcEoGISJxTIhARiXNKBCIice7/A0ggi0xWhAAdAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(natural_target)\n",
    "plt.plot(natural_trojan)\n",
    "plt.title(\"the natural losses of target layer and trojan layers\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "#plt.savefig(acc_trend_graph_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, nan, 0.39057692307692304, 0.5045192307692308, 0.4110576923076924]\n",
      "[nan, nan, 0.1455769230769231, 0.023846153846153875, 0.04807692307692309]\n"
     ]
    }
   ],
   "source": [
    "print(loss_sequential_1)\n",
    "print(loss_sequential_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrojanNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
